{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79cbab-b668-49cd-974a-18335f352f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f02c94-b718-459e-b5a2-7de41528c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction # Added for BLEU\n",
    "\n",
    "# # --- DATA ---\n",
    "\n",
    "# def load_menatqa_dataset(file_path='/content/MenatQA.json'):\n",
    "#     if not os.path.exists(file_path):\n",
    "#         import urllib.request\n",
    "#         print(\"Downloading MenatQA...\")\n",
    "#         urllib.request.urlretrieve(\n",
    "#             \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "#             file_path\n",
    "#         )\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "#     return data\n",
    "\n",
    "# def extract_reasoning_hops(example):\n",
    "#     question = example.get('question', '')\n",
    "#     answer = example.get('answer', '')\n",
    "#     q_type = example.get('type', '')\n",
    "#     time_scope = example.get('time_scope', '')\n",
    "#     sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "#     clauses = len([c for c in re.split(r'and|or|but|because|when|if', question.lower()) if c.strip()])\n",
    "#     capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "#     complexity_score = 1\n",
    "#     complexity_score += min(1, len(sentences) - 1)\n",
    "#     complexity_score += min(1, (clauses - 1) // 2)\n",
    "#     complexity_score += min(1, capitalized_words // 3)\n",
    "#     if time_scope:\n",
    "#         complexity_score += 1\n",
    "#     complexity_score = min(4, max(1, complexity_score))\n",
    "#     hops = []\n",
    "#     hops.append(f\"Understand the question: {question}\")\n",
    "#     if complexity_score >= 2:\n",
    "#         if time_scope:\n",
    "#             hops.append(f\"Identify time context: {time_scope}\")\n",
    "#         else:\n",
    "#             hops.append(f\"Recognize question type: {q_type}\")\n",
    "#     if complexity_score >= 3:\n",
    "#         hops.append(\"Retrieve relevant facts and information\")\n",
    "#     if complexity_score >= 4:\n",
    "#         hops.append(\"Analyze relationships between facts\")\n",
    "#     hops.append(f\"Formulate answer: {answer}\")\n",
    "#     return hops, complexity_score\n",
    "\n",
    "# def preprocess_dataset(data):\n",
    "#     processed_data = []\n",
    "#     hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "#     for item in data:\n",
    "#         gold_hops, complexity_score = extract_reasoning_hops(item)\n",
    "#         hop_counts[min(4, complexity_score)] += 1\n",
    "#         entry = {\n",
    "#             'ID': item.get('ID', ''),\n",
    "#             'question': item.get('question', ''),\n",
    "#             'answer': item.get('answer', ''),\n",
    "#             'type': item.get('type', ''),\n",
    "#             'time_scope': item.get('time_scope', ''),\n",
    "#             'gold_hops': gold_hops,\n",
    "#             'hop_count': complexity_score,\n",
    "#             'model_prediction': '',\n",
    "#             'model_reasoning_steps': []\n",
    "#         }\n",
    "#         processed_data.append(entry)\n",
    "#     df = pd.DataFrame(processed_data)\n",
    "#     df['hop_category'] = pd.cut(\n",
    "#         df['hop_count'],\n",
    "#         bins=[-1, 1, 2, 3, float('inf')],\n",
    "#         labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "#     )\n",
    "#     print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "#     for i in range(1, 5):\n",
    "#         print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "#     return df\n",
    "\n",
    "# # --- LLM ---\n",
    "\n",
    "# def run_qwen_cot_predictions(\n",
    "#         df,\n",
    "#         model_name=\"Qwen/Qwen3-0.6B\",\n",
    "#         device=None,\n",
    "#         max_new_tokens=256,\n",
    "#         temperature=0.0):\n",
    "\n",
    "#     if device is None:\n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     print(f\"Loading HuggingFace model {model_name} ({device}) ...\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto').to(device)\n",
    "#     pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if device == 'cuda' else -1,\n",
    "#                     max_new_tokens=max_new_tokens, temperature=temperature, do_sample=False)\n",
    "\n",
    "#     def format_cot_prompt(question):\n",
    "#         return (f\"Answer the following question step by step. \"\n",
    "#                 f\"Write each reasoning step on a new line, and finish with your answer.\\n\"\n",
    "#                 f\"Question: {question}\\nLet's think step by step:\")\n",
    "\n",
    "#     all_preds, all_steps = [], []\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating predictions\"):\n",
    "#         prompt = format_cot_prompt(row[\"question\"])\n",
    "#         try:\n",
    "#             generation = pipe(prompt)[0]['generated_text']\n",
    "#         except Exception as e:\n",
    "#             print(f\"Qwen error on idx {idx} (ID: {row.get('ID', 'N/A')}): {e}\")\n",
    "#             generation = \"\"\n",
    "#         output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "#         steps = [x.strip() for x in output.split('\\n') if x.strip()]\n",
    "#         if not steps:\n",
    "#             steps = [\"\"] # Ensure there's always at least one step (even if empty)\n",
    "#         model_answer = steps[-1]\n",
    "#         all_preds.append(model_answer)\n",
    "#         all_steps.append(steps)\n",
    "#     df = df.copy()\n",
    "#     df['model_prediction'] = all_preds\n",
    "#     df['model_reasoning_steps'] = all_steps\n",
    "#     return df\n",
    "\n",
    "# # --- METRICS ---\n",
    "\n",
    "# def compute_exact_match(prediction, gold):\n",
    "#     return int(str(prediction).strip().lower() == str(gold).strip().lower())\n",
    "\n",
    "# def compute_f1(prediction, gold):\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "#     common = set(pred_tokens) & set(gold_tokens)\n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "#     prec = len(common) / len(pred_tokens)\n",
    "#     rec = len(common) / len(gold_tokens)\n",
    "#     if prec + rec == 0:\n",
    "#         return 0.0\n",
    "#     f1 = 2 * prec * rec / (prec + rec)\n",
    "#     return f1\n",
    "\n",
    "# def compute_bleu(prediction, gold):\n",
    "#     \"\"\"\n",
    "#     Computes sentence BLEU score between prediction and gold answer.\n",
    "#     \"\"\"\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "\n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "\n",
    "#     reference = [gold_tokens]\n",
    "#     candidate = pred_tokens\n",
    "    \n",
    "#     weights = (0.25, 0.25, 0.25, 0.25) # BLEU-4\n",
    "#     chencherry = SmoothingFunction()\n",
    "#     try:\n",
    "#         bleu_score = sentence_bleu(reference, candidate, weights=weights, smoothing_function=chencherry.method1)\n",
    "#     except ZeroDivisionError:\n",
    "#         bleu_score = 0.0\n",
    "#     except ValueError: # Handles cases like candidate/reference too short\n",
    "#         bleu_score = 0.0\n",
    "#     return bleu_score\n",
    "\n",
    "# def evaluate_hop_wise_accuracy(df):\n",
    "#     categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0]) # Sort categories\n",
    "#     hop_results = {}\n",
    "#     overall_em, overall_f1, overall_bleu, overall_count = 0, 0, 0, 0\n",
    "    \n",
    "#     for cat in categories:\n",
    "#         subdf = df[df['hop_category'] == cat]\n",
    "#         if subdf.empty:\n",
    "#             hop_results[str(cat)] = {'count': 0, 'EM': 0, 'F1': 0, 'BLEU': 0}\n",
    "#             continue\n",
    "\n",
    "#         ems = [compute_exact_match(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "#         f1s = [compute_f1(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "#         bleus = [compute_bleu(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "        \n",
    "#         count = len(subdf)\n",
    "#         mean_em = np.mean(ems) if ems else 0\n",
    "#         mean_f1 = np.mean(f1s) if f1s else 0\n",
    "#         mean_bleu = np.mean(bleus) if bleus else 0\n",
    "        \n",
    "#         hop_results[str(cat)] = {'count': count, 'EM': mean_em, 'F1': mean_f1, 'BLEU': mean_bleu}\n",
    "        \n",
    "#         overall_em += sum(ems)\n",
    "#         overall_f1 += sum(f1s)\n",
    "#         overall_bleu += sum(bleus)\n",
    "#         overall_count += count\n",
    "        \n",
    "#     hop_results['overall'] = {\n",
    "#         'count': overall_count,\n",
    "#         'EM': overall_em / overall_count if overall_count else 0,\n",
    "#         'F1': overall_f1 / overall_count if overall_count else 0,\n",
    "#         'BLEU': overall_bleu / overall_count if overall_count else 0,\n",
    "#     }\n",
    "#     return hop_results\n",
    "\n",
    "# def evaluate_step_verification(df: pd.DataFrame) -> dict:\n",
    "#     all_step_matches = []\n",
    "#     complete_path_matches = 0\n",
    "#     partial_path_matches = 0\n",
    "#     total_questions = 0\n",
    "#     # Ensure categories are sorted for consistent output\n",
    "#     sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "#     hop_category_results = defaultdict(lambda: {'matches': 0, 'partial_matches': 0, 'total': 0})\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         gold_hops = row['gold_hops']\n",
    "#         model_steps = row['model_reasoning_steps']\n",
    "        \n",
    "#         if not gold_hops or not model_steps or not model_steps[0]: # Check if model_steps[0] is not empty\n",
    "#             continue\n",
    "            \n",
    "#         total_questions += 1\n",
    "#         hop_cat = row['hop_category']\n",
    "#         if hop_cat is None: continue # Skip if hop_category is somehow None\n",
    "\n",
    "#             hop_category_results[hop_cat]['total'] += 1            \n",
    "#             step_matches = []\n",
    "#             first_match = 0\n",
    "#             last_match = 0\n",
    "#             middle_matches = []\n",
    "\n",
    "#         # First step (question understanding)\n",
    "#         gold_q_terms = set(re.findall(r'\\w+', gold_hops[0].lower()))\n",
    "#         model_q_terms = set(re.findall(r'\\w+', model_steps[0].lower())) # model_steps[0] might be empty if parsing failed\n",
    "#         if gold_q_terms: # Avoid division by zero if gold_q_terms is empty (shouldn't happen with current logic)\n",
    "#             common_terms = gold_q_terms & model_q_terms\n",
    "#             first_match = len(common_terms) / len(gold_q_terms)\n",
    "#         step_matches.append(first_match)\n",
    "\n",
    "#         # Last step (answer formulation)\n",
    "#         if len(gold_hops) > 1 and len(model_steps) > 1:\n",
    "#             gold_ans_text = row['answer'].lower() # The actual gold answer\n",
    "#             # Gold last hop usually contains the answer\n",
    "#             # Model last hop is considered the model's answer\n",
    "#             model_last_hop_text = model_steps[-1].lower()\n",
    "            \n",
    "#             # Check if the gold answer text is present in the model's final step (which is its answer)\n",
    "#             last_match = 1.0 if gold_ans_text in model_last_hop_text else 0.0\n",
    "#             step_matches.append(last_match)\n",
    "\n",
    "#         # Intermediate steps\n",
    "#         if len(gold_hops) > 2 and len(model_steps) > 2: # At least one intermediate step in both\n",
    "#             # Compare internal reasoning steps (excluding first and last)\n",
    "#             # Number of intermediate steps to compare is min of available intermediate steps\n",
    "#             num_intermediate_to_compare = min(len(gold_hops) - 2, len(model_steps) - 2)\n",
    "#             for i in range(num_intermediate_to_compare):\n",
    "#                 gold_mid_idx = i + 1\n",
    "#                 model_mid_idx = i + 1\n",
    "                \n",
    "#                 gold_mid = gold_hops[gold_mid_idx].lower()\n",
    "#                 model_mid = model_steps[model_mid_idx].lower()\n",
    "                \n",
    "#                 gold_words = set(re.findall(r'\\w+', gold_mid))\n",
    "#                 model_words = set(re.findall(r'\\w+', model_mid))\n",
    "                \n",
    "#                 if gold_words and model_words:\n",
    "#                     common_words = gold_words & model_words\n",
    "#                     similarity = len(common_words) / max(len(gold_words), len(model_words)) # Jaccard-like on words\n",
    "#                     middle_matches.append(similarity)\n",
    "#                     step_matches.append(similarity)\n",
    "#                 elif gold_words or model_words: # One is empty, the other not\n",
    "#                     middle_matches.append(0.0)\n",
    "#                     step_matches.append(0.0)\n",
    "#                 # If both are empty, it could be a 0/0 case, or simply 0 similarity if one had content.\n",
    "#                 # The logic above handles if one is empty. If both are empty (e.g. just \".\"), similarity is low/0.\n",
    "\n",
    "#         all_step_matches.extend(step_matches)\n",
    "        \n",
    "#         if step_matches: # Only if we actually made comparisons\n",
    "#             # Define complete match: all compared steps must be >= 0.6\n",
    "#             is_complete_match = all(match >= 0.6 for match in step_matches)\n",
    "\n",
    "#             # Define partial match (if not a complete match)\n",
    "#             # First and last steps are critical\n",
    "#             is_partial_match = False\n",
    "#             if not is_complete_match:\n",
    "#                 avg_middle_match = sum(middle_matches) / len(middle_matches) if middle_matches else 1.0 # If no middle, condition passes\n",
    "#                 if first_match >= 0.7 and last_match >= 0.7 and avg_middle_match >= 0.4:\n",
    "#                     is_partial_match = True\n",
    "            \n",
    "#             if is_complete_match:\n",
    "#                 complete_path_matches += 1\n",
    "#                 hop_category_results[hop_cat]['matches'] += 1\n",
    "#             elif is_partial_match:\n",
    "#                 partial_path_matches += 1\n",
    "#                 hop_category_results[hop_cat]['partial_matches'] += 1\n",
    "\n",
    "#     cot_em_overall = complete_path_matches / total_questions if total_questions > 0 else 0\n",
    "#     cot_partial_overall = (complete_path_matches + partial_path_matches) / total_questions if total_questions > 0 else 0\n",
    "    \n",
    "#     cot_em_by_category = {}\n",
    "#     cot_partial_by_category = {}\n",
    "\n",
    "#     for hop_cat_key in sorted_categories: # Use sorted categories for results dict\n",
    "#         data = hop_category_results[hop_cat_key]\n",
    "#         cat_total = data['total']\n",
    "#         cot_em = data['matches'] / cat_total if cat_total > 0 else 0\n",
    "#         cot_partial = (data['matches'] + data['partial_matches']) / cat_total if cat_total > 0 else 0\n",
    "#         cot_em_by_category[str(hop_cat_key)] = cot_em\n",
    "#         cot_partial_by_category[str(hop_cat_key)] = cot_partial\n",
    "        \n",
    "#     results = {\n",
    "#         'CoT-EM': cot_em_overall,\n",
    "#         'CoT-Partial': cot_partial_overall,\n",
    "#         'Hop-Match-Rate': np.mean(all_step_matches) if all_step_matches else 0,\n",
    "#         'CoT-EM by Category': cot_em_by_category,\n",
    "#         'CoT-Partial by Category': cot_partial_by_category,\n",
    "#         'Total Questions Evaluated for Step Verification': total_questions\n",
    "#     }\n",
    "#     return results\n",
    "\n",
    "\n",
    "# def compute_cot_hop_em(df: pd.DataFrame) -> float: # Strict CoT EM\n",
    "#     exact_matches = 0\n",
    "#     total = 0\n",
    "#     for _, row in df.iterrows():\n",
    "#         gold_hops = row.get('gold_hops', [])\n",
    "#         pred_hops = row.get('model_reasoning_steps', [])\n",
    "        \n",
    "#         if not gold_hops or not pred_hops or not pred_hops[0]: # ensure pred_hops is not empty or has empty first step\n",
    "#             continue\n",
    "            \n",
    "#         total += 1\n",
    "#         # Compare only up to the length of the shorter list of hops,\n",
    "#         # but for strict EM, lengths must match.\n",
    "#         if len(pred_hops) == len(gold_hops) and \\\n",
    "#            all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "#             exact_matches += 1\n",
    "            \n",
    "#     cot_em = exact_matches / total if total > 0 else 0.0\n",
    "#     print(f\"Strict CoT-EM (exact hop sequence): {cot_em:.4f} ({exact_matches}/{total})\")\n",
    "#     return cot_em\n",
    "\n",
    "# def compute_cot_hop_em_by_category(df: pd.DataFrame) -> dict:\n",
    "#     results = {}\n",
    "#     # Ensure categories are sorted for consistent output\n",
    "#     sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "#     for cat in sorted_categories:\n",
    "#         subdf = df[df['hop_category'] == cat]\n",
    "#         if subdf.empty:\n",
    "#             results[str(cat)] = 0.0\n",
    "#             continue\n",
    "#         em = compute_cot_hop_em(subdf) # This will print for each category, which might be verbose.\n",
    "#                                       # For aggregated report, better to compute silently.\n",
    "#         # Silent computation for aggregation:\n",
    "#         exact_matches = 0\n",
    "#         total = 0\n",
    "#         for _, row in subdf.iterrows():\n",
    "#             gold_hops = row.get('gold_hops', [])\n",
    "#             pred_hops = row.get('model_reasoning_steps', [])\n",
    "#             if not gold_hops or not pred_hops or not pred_hops[0]: continue\n",
    "#             total += 1\n",
    "#             if len(pred_hops) == len(gold_hops) and \\\n",
    "#                all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "#                 exact_matches += 1\n",
    "#         results[str(cat)] = exact_matches / total if total > 0 else 0.0\n",
    "#     return results\n",
    "\n",
    "# # --- VISUALIZATION ---\n",
    "\n",
    "# def visualize_hop_wise_results(hop_results, step_results, strict_cot_em_overall, cot_em_per_cat_strict, save_path=None):\n",
    "#     plt.style.use('seaborn-v0_8-whitegrid') # Using a seaborn style\n",
    "#     plt.figure(figsize=(15, 9)) # Adjusted figure size\n",
    "    \n",
    "#     # Ensure categories are sorted as in hop_results (which should be from evaluate_hop_wise_accuracy)\n",
    "#     categories = [cat for cat in hop_results.keys() if cat != 'overall']\n",
    "    \n",
    "#     counts = [hop_results[cat]['count'] for cat in categories]\n",
    "#     em_scores = [hop_results[cat]['EM'] for cat in categories]\n",
    "#     f1_scores = [hop_results[cat]['F1'] for cat in categories]\n",
    "#     bleu_scores = [hop_results[cat]['BLEU'] for cat in categories] # Get BLEU scores\n",
    "    \n",
    "#     soft_cot_em_by_cat = step_results.get('CoT-EM by Category', {})\n",
    "#     soft_cot_em_scores = [soft_cot_em_by_cat.get(cat, 0) for cat in categories]\n",
    "    \n",
    "#     # cot_em_per_cat_strict is already the dict from compute_cot_hop_em_by_category\n",
    "#     strict_cot_em_scores = [cot_em_per_cat_strict.get(cat, 0) for cat in categories]\n",
    "    \n",
    "#     x = np.arange(len(categories))\n",
    "#     width = 0.15 # Width for each bar (5 bars per group)\n",
    "    \n",
    "#     plt.bar(x - width*2, em_scores, width, label='Exact Match (Answer)', color='#3498db', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "#     plt.bar(x - width, f1_scores, width, label='F1 Score (Answer)', color='#2ecc71', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "#     plt.bar(x, bleu_scores, width, label='BLEU Score (Answer)', color='#f39c12', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "#     plt.bar(x + width, soft_cot_em_scores, width, label='Soft CoT-EM (Path)', color='#e74c3c', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "#     plt.bar(x + width*2, strict_cot_em_scores, width, label='Strict CoT-EM (Path)', color='#8e44ad', alpha=0.85, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "#     plt.xlabel('Number of Reasoning Hops (Heuristic)', fontsize=13, fontweight='bold')\n",
    "#     plt.ylabel('Score', fontsize=13, fontweight='bold')\n",
    "#     plt.title('Qwen3-0.6B Performance on MenatQA by Reasoning Complexity', fontsize=16, fontweight='bold', pad=20)\n",
    "#     plt.xticks(x, categories, fontsize=11)\n",
    "#     plt.yticks(fontsize=10)\n",
    "#     plt.ylim(0, 1.05)\n",
    "    \n",
    "#     for i, count in enumerate(counts):\n",
    "#         if count > 0: # Only add text if there are samples\n",
    "#             plt.text(i, 0.015, f'n={count}', ha='center', va='bottom', fontsize=9,\n",
    "#                      bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "                     \n",
    "#     overall = hop_results['overall']\n",
    "#     overall_text = (f\"Overall Metrics (n={overall['count']}):\\n\"\n",
    "#                     f\"Answer EM: {overall['EM']:.3f}  |  Answer F1: {overall['F1']:.3f}  |  Answer BLEU: {overall['BLEU']:.3f}\\n\"\n",
    "#                     f\"Path Soft CoT-EM: {step_results['CoT-EM']:.3f}  |  Path Strict CoT-EM: {strict_cot_em_overall:.3f}\")\n",
    "                    \n",
    "#     plt.figtext(0.5, 0.005, overall_text, ha='center', fontsize=11, fontweight='bold',\n",
    "#                 bbox=dict(facecolor='whitesmoke', edgecolor='gray', boxstyle='round,pad=0.5'))\n",
    "                \n",
    "#     plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=3, fontsize=10, frameon=True, edgecolor='black')\n",
    "#     plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "#     plt.tight_layout(rect=[0, 0.06, 1, 0.95]) # Adjust layout to make space for figtext and title\n",
    "    \n",
    "#     if save_path:\n",
    "#         plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#         print(f\"Visualization saved to {save_path}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # --- MAIN ---\n",
    "# if __name__ == '__main__':\n",
    "#     # Ensure NLTK data is available if more advanced tokenization were used in compute_bleu\n",
    "#     # For current simple .split(), it's not strictly needed for BLEU but good for other NLTK uses.\n",
    "#     # import nltk\n",
    "#     # try:\n",
    "#     #     nltk.data.find('tokenizers/punkt')\n",
    "#     # except nltk.downloader.DownloadError:\n",
    "#     #     print(\"NLTK 'punkt' resource not found. Downloading...\")\n",
    "#     #     nltk.download('punkt', quiet=True)\n",
    "\n",
    "\n",
    "#     # --- Configuration ---\n",
    "#     DATASET_FILE = '/content/MenatQA.json'\n",
    "#     MODEL_NAME = \"Qwen/Qwen3-0.6B\" # \"Qwen/Qwen1.5-0.5B-Chat\" # or other small models\n",
    "#     OUTPUT_IMAGE_PATH = '/content/menatqa_qwen_performance.png'\n",
    "#     OUTPUT_RESULTS_PATH = '/content/menatqa_qwen_summary_results.json'\n",
    "#     # For faster testing, you can subset the data:\n",
    "#     # MAX_SAMPLES = 10 # Set to None to run on all data\n",
    "#     MAX_SAMPLES = None\n",
    "\n",
    "\n",
    "#     # --- Execution ---\n",
    "#     data = load_menatqa_dataset(DATASET_FILE)\n",
    "    \n",
    "#     if MAX_SAMPLES is not None:\n",
    "#         print(f\"Subsetting dataset to {MAX_SAMPLES} samples for quick testing.\")\n",
    "#         data = data[:MAX_SAMPLES]\n",
    "\n",
    "#     df = preprocess_dataset(data)\n",
    "\n",
    "#     print(f\"\\nRunning {MODEL_NAME} on {len(df)} questions. This may take time.\")\n",
    "#     df_results = run_qwen_cot_predictions(df, model_name=MODEL_NAME)\n",
    "\n",
    "#     print(\"\\nEvaluating results...\")\n",
    "#     hop_results = evaluate_hop_wise_accuracy(df_results)\n",
    "#     step_verification_results = evaluate_step_verification(df_results)\n",
    "#     strict_cot_em_overall = compute_cot_hop_em(df_results) # This will print the overall strict CoT-EM\n",
    "#     strict_cot_em_by_category = compute_cot_hop_em_by_category(df_results) # This will compute per category for the dict\n",
    "\n",
    "#     print(\"\\n--- Hop-wise Answer Accuracy (including BLEU) ---\")\n",
    "#     for category, metrics in hop_results.items():\n",
    "#         if category == 'overall': continue\n",
    "#         print(f\"Category: {category} (n={metrics['count']}) | EM: {metrics['EM']:.3f}, F1: {metrics['F1']:.3f}, BLEU: {metrics['BLEU']:.3f}\")\n",
    "#     print(f\"Overall (n={hop_results['overall']['count']}): EM: {hop_results['overall']['EM']:.3f}, F1: {hop_results['overall']['F1']:.3f}, BLEU: {hop_results['overall']['BLEU']:.3f}\")\n",
    "\n",
    "#     print(\"\\n--- Reasoning Path Verification ---\")\n",
    "#     print(f\"Overall Soft CoT-EM (Path): {step_verification_results['CoT-EM']:.3f}\")\n",
    "#     print(f\"Overall CoT-Partial (Path): {step_verification_results['CoT-Partial']:.3f}\")\n",
    "#     print(f\"Overall Hop-Match-Rate: {step_verification_results['Hop-Match-Rate']:.3f}\")\n",
    "#     print(f\"Overall Strict CoT-EM (Path): {strict_cot_em_overall:.3f}\")\n",
    "    \n",
    "#     print(\"\\nBy Category - Soft CoT-EM (Path):\")\n",
    "#     for cat, score in step_verification_results['CoT-EM by Category'].items():\n",
    "#         print(f\"  {cat}: {score:.3f}\")\n",
    "#     print(\"By Category - Strict CoT-EM (Path):\")\n",
    "#     for cat, score in strict_cot_em_by_category.items():\n",
    "#         print(f\"  {cat}: {score:.3f}\")\n",
    "\n",
    "\n",
    "#     visualize_hop_wise_results(\n",
    "#         hop_results,\n",
    "#         step_verification_results,\n",
    "#         strict_cot_em_overall,\n",
    "#         strict_cot_em_by_category,\n",
    "#         save_path=OUTPUT_IMAGE_PATH\n",
    "#     )\n",
    "\n",
    "#     summary_results_output = {\n",
    "#         'model_name': MODEL_NAME,\n",
    "#         'dataset_info': {\n",
    "#             'name': 'MenatQA',\n",
    "#             'total_questions_processed': len(df_results),\n",
    "#             'hop_distribution': df_results['hop_category'].value_counts().to_dict()\n",
    "#         },\n",
    "#         'answer_accuracy_metrics': hop_results,\n",
    "#         'reasoning_path_verification_metrics': step_verification_results,\n",
    "#         'strict_cot_em_overall': float(strict_cot_em_overall),\n",
    "#         'strict_cot_em_by_category': {k: float(v) for k, v in strict_cot_em_by_category.items()}\n",
    "#     }\n",
    "#     with open(OUTPUT_RESULTS_PATH, 'w') as f:\n",
    "#         json.dump(summary_results_output, f, indent=2)\n",
    "\n",
    "#     print(f\"\\nEvaluation complete! Results saved to {OUTPUT_RESULTS_PATH} and {OUTPUT_IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97df8a-a6a5-496e-b752-98d38e6035da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb88ea-e6c7-4573-a6bb-d4a2c4687d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070eed9-461d-408e-98c1-a04af861482b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc7398-c0b1-4899-9909-e58f9f85f360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eee47d-8f32-4c2b-9fa3-bd6d1ecc88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from pathlib import Path\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# # Download required NLTK data\n",
    "# import nltk\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     print(\"Downloading required NLTK data...\")\n",
    "#     nltk.download('punkt', quiet=True)\n",
    "\n",
    "# # --- CONFIGURATION ---\n",
    "# class EvaluationConfig:\n",
    "#     def __init__(self):\n",
    "#         # Create output directory in current working directory\n",
    "#         self.output_dir = Path.cwd() / \"menatqa_evaluation\"\n",
    "#         self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "#         self.dataset_file = self.output_dir / 'MenatQA.json'\n",
    "#         self.model_name = \"Qwen/Qwen3-0.6B\"  # Updated to available model\n",
    "#         self.max_samples = None  # Set to small number for testing\n",
    "#         self.max_new_tokens = 256\n",
    "#         self.temperature = 0.0\n",
    "#         self.batch_size = 1\n",
    "#         self.use_fp16 = True\n",
    "        \n",
    "#         # Output files\n",
    "#         self.output_image_path = self.output_dir / 'menatqa_performance.png'\n",
    "#         self.output_results_path = self.output_dir / 'menatqa_results.json'\n",
    "        \n",
    "#     def to_dict(self):\n",
    "#         # Convert Path objects to strings for JSON serialization\n",
    "#         config_dict = {}\n",
    "#         for key, value in self.__dict__.items():\n",
    "#             if isinstance(value, Path):\n",
    "#                 config_dict[key] = str(value)\n",
    "#             else:\n",
    "#                 config_dict[key] = value\n",
    "#         return config_dict\n",
    "\n",
    "# # --- DATA ---\n",
    "# def load_menatqa_dataset(file_path):\n",
    "#     \"\"\"Load MenatQA dataset with improved error handling\"\"\"\n",
    "#     file_path = Path(file_path)\n",
    "    \n",
    "#     # Create parent directory if it doesn't exist\n",
    "#     file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     if not file_path.exists():\n",
    "#         import urllib.request\n",
    "#         print(f\"Downloading MenatQA to {file_path}...\")\n",
    "#         try:\n",
    "#             urllib.request.urlretrieve(\n",
    "#                 \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "#                 str(file_path)\n",
    "#             )\n",
    "#             print(\"Download completed successfully!\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error downloading dataset: {e}\")\n",
    "#             # Fallback: create a small sample dataset for testing\n",
    "#             sample_data = [\n",
    "#                 {\n",
    "#                     \"ID\": \"test_1\",\n",
    "#                     \"question\": \"What is the capital of France?\",\n",
    "#                     \"answer\": \"Paris\",\n",
    "#                     \"type\": \"factual\",\n",
    "#                     \"time_scope\": \"\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"ID\": \"test_2\", \n",
    "#                     \"question\": \"Who was the first president of the United States and when did he serve?\",\n",
    "#                     \"answer\": \"George Washington served from 1789 to 1797\",\n",
    "#                     \"type\": \"historical\",\n",
    "#                     \"time_scope\": \"1789-1797\"\n",
    "#                 }\n",
    "#             ]\n",
    "#             with open(file_path, 'w') as f:\n",
    "#                 json.dump(sample_data, f, indent=2)\n",
    "#             print(f\"Created sample dataset at {file_path}\")\n",
    "    \n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "#     return data\n",
    "\n",
    "# def extract_reasoning_hops_advanced(example):\n",
    "#     \"\"\"Enhanced hop detection with better linguistic analysis\"\"\"\n",
    "#     question = example.get('question', '')\n",
    "#     answer = example.get('answer', '')\n",
    "#     q_type = example.get('type', '')\n",
    "#     time_scope = example.get('time_scope', '')\n",
    "    \n",
    "#     # More sophisticated analysis\n",
    "#     sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "    \n",
    "#     # Enhanced logical connector detection\n",
    "#     logical_connectors = ['and', 'or', 'but', 'because', 'since', 'although', \n",
    "#                          'however', 'therefore', 'moreover', 'furthermore', 'while']\n",
    "#     temporal_words = ['before', 'after', 'during', 'when', 'then', 'until', 'since']\n",
    "    \n",
    "#     connector_count = sum(1 for word in logical_connectors if word in question.lower())\n",
    "#     temporal_count = sum(1 for word in temporal_words if word in question.lower())\n",
    "    \n",
    "#     # Question complexity indicators\n",
    "#     question_words = ['what', 'where', 'when', 'who', 'why', 'how', 'which']\n",
    "#     q_word_count = sum(1 for word in question_words if word in question.lower())\n",
    "    \n",
    "#     capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "#     # Enhanced complexity scoring\n",
    "#     complexity_score = 1\n",
    "#     complexity_score += min(2, len(sentences) - 1)  # Multiple sentences\n",
    "#     complexity_score += min(1, connector_count)     # Logical connections\n",
    "#     complexity_score += min(1, temporal_count)      # Temporal reasoning\n",
    "#     complexity_score += min(1, q_word_count - 1)    # Multiple question aspects\n",
    "#     complexity_score += min(1, capitalized_words // 4)  # Named entities\n",
    "    \n",
    "#     if time_scope:\n",
    "#         complexity_score += 1\n",
    "    \n",
    "#     complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "#     # Generate reasoning hops\n",
    "#     hops = []\n",
    "#     hops.append(f\"Understand the question: {question}\")\n",
    "    \n",
    "#     if complexity_score >= 2:\n",
    "#         if time_scope:\n",
    "#             hops.append(f\"Identify temporal context: {time_scope}\")\n",
    "#         elif connector_count > 0:\n",
    "#             hops.append(\"Analyze logical relationships in the question\")\n",
    "#         else:\n",
    "#             hops.append(f\"Recognize question type: {q_type}\")\n",
    "    \n",
    "#     if complexity_score >= 3:\n",
    "#         hops.append(\"Retrieve and analyze relevant facts\")\n",
    "        \n",
    "#     if complexity_score >= 4:\n",
    "#         hops.append(\"Synthesize information and resolve dependencies\")\n",
    "    \n",
    "#     hops.append(f\"Formulate final answer: {answer}\")\n",
    "    \n",
    "#     return hops, complexity_score\n",
    "\n",
    "# def preprocess_dataset(data):\n",
    "#     \"\"\"Preprocess dataset with enhanced hop extraction\"\"\"\n",
    "#     processed_data = []\n",
    "#     hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "#     for item in data:\n",
    "#         gold_hops, complexity_score = extract_reasoning_hops_advanced(item)\n",
    "#         hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "#         entry = {\n",
    "#             'ID': item.get('ID', ''),\n",
    "#             'question': item.get('question', ''),\n",
    "#             'answer': item.get('answer', ''),\n",
    "#             'type': item.get('type', ''),\n",
    "#             'time_scope': item.get('time_scope', ''),\n",
    "#             'gold_hops': gold_hops,\n",
    "#             'hop_count': complexity_score,\n",
    "#             'model_prediction': '',\n",
    "#             'model_reasoning_steps': []\n",
    "#         }\n",
    "#         processed_data.append(entry)\n",
    "    \n",
    "#     df = pd.DataFrame(processed_data)\n",
    "#     df['hop_category'] = pd.cut(\n",
    "#         df['hop_count'],\n",
    "#         bins=[-1, 1, 2, 3, float('inf')],\n",
    "#         labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "#     for i in range(1, 5):\n",
    "#         print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # --- LLM ---\n",
    "# def run_qwen_cot_predictions(df, config):\n",
    "#     \"\"\"Enhanced model inference with better memory management\"\"\"\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     print(f\"Loading model {config.model_name} on {device}...\")\n",
    "    \n",
    "#     # Clear GPU memory\n",
    "#     if device == \"cuda\":\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "#     try:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        \n",
    "#         # Memory-efficient model loading\n",
    "#         model_kwargs = {\n",
    "#             'torch_dtype': torch.float16 if config.use_fp16 and device == \"cuda\" else 'auto',\n",
    "#             'device_map': \"auto\" if device == \"cuda\" else None\n",
    "#         }\n",
    "        \n",
    "#         model = AutoModelForCausalLM.from_pretrained(config.model_name, **model_kwargs)\n",
    "#         if not model_kwargs.get('device_map'):\n",
    "#             model = model.to(device)\n",
    "            \n",
    "#         # Create pipeline\n",
    "#         pipe = pipeline(\n",
    "#             \"text-generation\", \n",
    "#             model=model, \n",
    "#             tokenizer=tokenizer, \n",
    "#             device=0 if device == 'cuda' else -1,\n",
    "#             max_new_tokens=config.max_new_tokens,\n",
    "#             temperature=config.temperature,\n",
    "#             do_sample=False\n",
    "#         )\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading model: {e}\")\n",
    "#         print(\"Using dummy predictions for testing...\")\n",
    "#         return create_dummy_predictions(df)\n",
    "\n",
    "#     def format_cot_prompt(question):\n",
    "#         return (f\"Answer the following question step by step. \"\n",
    "#                 f\"Write each reasoning step on a new line, and finish with your answer.\\n\"\n",
    "#                 f\"Question: {question}\\nLet's think step by step:\")\n",
    "\n",
    "#     all_preds, all_steps = [], []\n",
    "    \n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating predictions\"):\n",
    "#         prompt = format_cot_prompt(row[\"question\"])\n",
    "        \n",
    "#         try:\n",
    "#             generation = pipe(prompt)[0]['generated_text']\n",
    "#             output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "#             steps = [x.strip() for x in output.split('\\n') if x.strip()]\n",
    "            \n",
    "#             if not steps:\n",
    "#                 steps = [\"Unable to generate reasoning steps\"]\n",
    "                \n",
    "#             model_answer = steps[-1] if steps else \"No answer generated\"\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error on question {idx}: {e}\")\n",
    "#             steps = [\"Error in generation\"]\n",
    "#             model_answer = \"Error\"\n",
    "        \n",
    "#         all_preds.append(model_answer)\n",
    "#         all_steps.append(steps)\n",
    "    \n",
    "#     df = df.copy()\n",
    "#     df['model_prediction'] = all_preds\n",
    "#     df['model_reasoning_steps'] = all_steps\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# def create_dummy_predictions(df):\n",
    "#     \"\"\"Create dummy predictions for testing when model loading fails\"\"\"\n",
    "#     dummy_preds = []\n",
    "#     dummy_steps = []\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         # Create plausible dummy reasoning\n",
    "#         steps = [\n",
    "#             f\"I need to understand: {row['question'][:50]}...\",\n",
    "#             \"Let me think about the relevant information.\",\n",
    "#             \"Based on my analysis, the answer is:\"\n",
    "#         ]\n",
    "#         pred = f\"Dummy answer for: {row['question'][:20]}...\"\n",
    "        \n",
    "#         dummy_preds.append(pred)\n",
    "#         dummy_steps.append(steps)\n",
    "    \n",
    "#     df = df.copy()\n",
    "#     df['model_prediction'] = dummy_preds\n",
    "#     df['model_reasoning_steps'] = dummy_steps\n",
    "    \n",
    "#     print(\"Created dummy predictions for testing\")\n",
    "#     return df\n",
    "\n",
    "# # --- ENHANCED METRICS ---\n",
    "# def compute_exact_match(prediction, gold):\n",
    "#     \"\"\"Enhanced exact match with better preprocessing\"\"\"\n",
    "#     pred_clean = str(prediction).strip().lower()\n",
    "#     gold_clean = str(gold).strip().lower()\n",
    "    \n",
    "#     # Remove common prefixes/suffixes\n",
    "#     prefixes = [\"the answer is\", \"answer:\", \"final answer:\"]\n",
    "#     for prefix in prefixes:\n",
    "#         if pred_clean.startswith(prefix):\n",
    "#             pred_clean = pred_clean[len(prefix):].strip()\n",
    "    \n",
    "#     return int(pred_clean == gold_clean)\n",
    "\n",
    "# def compute_f1(prediction, gold):\n",
    "#     \"\"\"Enhanced F1 with better tokenization\"\"\"\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "    \n",
    "#     # Remove stopwords for better matching\n",
    "#     stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "#     pred_tokens = [t for t in pred_tokens if t not in stopwords]\n",
    "#     gold_tokens = [t for t in gold_tokens if t not in stopwords]\n",
    "    \n",
    "#     common = set(pred_tokens) & set(gold_tokens)\n",
    "    \n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "    \n",
    "#     prec = len(common) / len(pred_tokens)\n",
    "#     rec = len(common) / len(gold_tokens)\n",
    "    \n",
    "#     if prec + rec == 0:\n",
    "#         return 0.0\n",
    "    \n",
    "#     return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "# def compute_bleu(prediction, gold):\n",
    "#     \"\"\"Enhanced BLEU computation\"\"\"\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "\n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "\n",
    "#     reference = [gold_tokens]\n",
    "#     candidate = pred_tokens\n",
    "    \n",
    "#     weights = (0.25, 0.25, 0.25, 0.25)\n",
    "#     chencherry = SmoothingFunction()\n",
    "    \n",
    "#     try:\n",
    "#         bleu_score = sentence_bleu(\n",
    "#             reference, candidate, \n",
    "#             weights=weights, \n",
    "#             smoothing_function=chencherry.method1\n",
    "#         )\n",
    "#     except (ZeroDivisionError, ValueError):\n",
    "#         bleu_score = 0.0\n",
    "    \n",
    "#     return bleu_score\n",
    "\n",
    "# # [Keep all the existing evaluation functions: evaluate_hop_wise_accuracy, evaluate_step_verification, etc.]\n",
    "# def evaluate_hop_wise_accuracy(df):\n",
    "#     categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "#     hop_results = {}\n",
    "#     overall_em, overall_f1, overall_bleu, overall_count = 0, 0, 0, 0\n",
    "    \n",
    "#     for cat in categories:\n",
    "#         subdf = df[df['hop_category'] == cat]\n",
    "#         if subdf.empty:\n",
    "#             hop_results[str(cat)] = {'count': 0, 'EM': 0, 'F1': 0, 'BLEU': 0}\n",
    "#             continue\n",
    "\n",
    "#         ems = [compute_exact_match(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "#         f1s = [compute_f1(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "#         bleus = [compute_bleu(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "        \n",
    "#         count = len(subdf)\n",
    "#         mean_em = np.mean(ems) if ems else 0\n",
    "#         mean_f1 = np.mean(f1s) if f1s else 0\n",
    "#         mean_bleu = np.mean(bleus) if bleus else 0\n",
    "        \n",
    "#         hop_results[str(cat)] = {'count': count, 'EM': mean_em, 'F1': mean_f1, 'BLEU': mean_bleu}\n",
    "        \n",
    "#         overall_em += sum(ems)\n",
    "#         overall_f1 += sum(f1s)\n",
    "#         overall_bleu += sum(bleus)\n",
    "#         overall_count += count\n",
    "        \n",
    "#     hop_results['overall'] = {\n",
    "#         'count': overall_count,\n",
    "#         'EM': overall_em / overall_count if overall_count else 0,\n",
    "#         'F1': overall_f1 / overall_count if overall_count else 0,\n",
    "#         'BLEU': overall_bleu / overall_count if overall_count else 0,\n",
    "#     }\n",
    "#     return hop_results\n",
    "\n",
    "# def evaluate_step_verification(df: pd.DataFrame) -> dict:\n",
    "#     all_step_matches = []\n",
    "#     complete_path_matches = 0\n",
    "#     partial_path_matches = 0\n",
    "#     total_questions = 0\n",
    "#     sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "#     hop_category_results = defaultdict(lambda: {'matches': 0, 'partial_matches': 0, 'total': 0})\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         gold_hops = row['gold_hops']\n",
    "#         model_steps = row['model_reasoning_steps']\n",
    "        \n",
    "#         if not gold_hops or not model_steps or not model_steps[0]:\n",
    "#             continue\n",
    "            \n",
    "#         total_questions += 1\n",
    "#         hop_cat = row['hop_category']\n",
    "#         if hop_cat is None: continue\n",
    "\n",
    "#         hop_category_results[hop_cat]['total'] += 1\n",
    "        \n",
    "#         step_matches = []\n",
    "#         first_match = 0\n",
    "#         last_match = 0\n",
    "#         middle_matches = []\n",
    "\n",
    "#         # First step (question understanding)\n",
    "#         gold_q_terms = set(re.findall(r'\\w+', gold_hops[0].lower()))\n",
    "#         model_q_terms = set(re.findall(r'\\w+', model_steps[0].lower()))\n",
    "#         if gold_q_terms:\n",
    "#             common_terms = gold_q_terms & model_q_terms\n",
    "#             first_match = len(common_terms) / len(gold_q_terms)\n",
    "#         step_matches.append(first_match)\n",
    "\n",
    "#         # Last step (answer formulation)\n",
    "#         if len(gold_hops) > 1 and len(model_steps) > 1:\n",
    "#             gold_ans_text = row['answer'].lower()\n",
    "#             model_last_hop_text = model_steps[-1].lower()\n",
    "            \n",
    "#             # Check if the gold answer appears in the model's final step\n",
    "#             last_match = 1.0 if gold_ans_text in model_last_hop_text else 0.0\n",
    "#             step_matches.append(last_match)\n",
    "\n",
    "#         # Intermediate steps (middle reasoning)\n",
    "#         if len(gold_hops) > 2 and len(model_steps) > 2:\n",
    "#             # Compare intermediate steps (excluding first and last)\n",
    "#             num_intermediate_to_compare = min(len(gold_hops) - 2, len(model_steps) - 2)\n",
    "            \n",
    "#             for i in range(num_intermediate_to_compare):\n",
    "#                 gold_mid_idx = i + 1  # Skip first step\n",
    "#                 model_mid_idx = i + 1  # Skip first step\n",
    "                \n",
    "#                 gold_mid = gold_hops[gold_mid_idx].lower()\n",
    "#                 model_mid = model_steps[model_mid_idx].lower()\n",
    "                \n",
    "#                 # Extract meaningful words from both steps\n",
    "#                 gold_words = set(re.findall(r'\\w+', gold_mid))\n",
    "#                 model_words = set(re.findall(r'\\w+', model_mid))\n",
    "                \n",
    "#                 # Calculate word overlap similarity\n",
    "#                 if gold_words and model_words:\n",
    "#                     common_words = gold_words & model_words\n",
    "#                     # Use Jaccard similarity\n",
    "#                     similarity = len(common_words) / max(len(gold_words), len(model_words))\n",
    "#                     middle_matches.append(similarity)\n",
    "#                     step_matches.append(similarity)\n",
    "#                 elif gold_words or model_words:\n",
    "#                     # One is empty, the other is not\n",
    "#                     middle_matches.append(0.0)\n",
    "#                     step_matches.append(0.0)\n",
    "#                 # If both are empty, skip this comparison\n",
    "\n",
    "#         # Add all step matches to overall tracking\n",
    "#         all_step_matches.extend(step_matches)\n",
    "        \n",
    "#         # Determine match quality\n",
    "#         if step_matches:\n",
    "#             # Complete match: all compared steps must meet high threshold\n",
    "#             is_complete_match = all(match >= 0.6 for match in step_matches)\n",
    "\n",
    "#             # Partial match: critical steps (first and last) are good, middle is reasonable\n",
    "#             is_partial_match = False\n",
    "#             if not is_complete_match:\n",
    "#                 # Calculate average middle step performance\n",
    "#                 avg_middle_match = sum(middle_matches) / len(middle_matches) if middle_matches else 1.0\n",
    "                \n",
    "#                 # Partial match criteria: good first/last steps, reasonable middle steps\n",
    "#                 if first_match >= 0.7 and last_match >= 0.7 and avg_middle_match >= 0.4:\n",
    "#                     is_partial_match = True\n",
    "            \n",
    "#             # Update counters\n",
    "#             if is_complete_match:\n",
    "#                 complete_path_matches += 1\n",
    "#                 hop_category_results[hop_cat]['matches'] += 1\n",
    "#             elif is_partial_match:\n",
    "#                 partial_path_matches += 1\n",
    "#                 hop_category_results[hop_cat]['partial_matches'] += 1\n",
    "\n",
    "#     # Calculate overall metrics\n",
    "#     cot_em_overall = complete_path_matches / total_questions if total_questions > 0 else 0\n",
    "#     cot_partial_overall = (complete_path_matches + partial_path_matches) / total_questions if total_questions > 0 else 0\n",
    "    \n",
    "#     # Calculate by-category metrics\n",
    "#     cot_em_by_category = {}\n",
    "#     cot_partial_by_category = {}\n",
    "\n",
    "#     for hop_cat_key in sorted_categories:\n",
    "#         data = hop_category_results[hop_cat_key]\n",
    "#         cat_total = data['total']\n",
    "#         cot_em = data['matches'] / cat_total if cat_total > 0 else 0\n",
    "#         cot_partial = (data['matches'] + data['partial_matches']) / cat_total if cat_total > 0 else 0\n",
    "#         cot_em_by_category[str(hop_cat_key)] = cot_em\n",
    "#         cot_partial_by_category[str(hop_cat_key)] = cot_partial\n",
    "        \n",
    "#     # Return comprehensive results\n",
    "#     results = {\n",
    "#         'CoT-EM': cot_em_overall,\n",
    "#         'CoT-Partial': cot_partial_overall,\n",
    "#         'Hop-Match-Rate': np.mean(all_step_matches) if all_step_matches else 0,\n",
    "#         'CoT-EM by Category': cot_em_by_category,\n",
    "#         'CoT-Partial by Category': cot_partial_by_category,\n",
    "#         'Total Questions Evaluated for Step Verification': total_questions,\n",
    "#         'Complete Path Matches': complete_path_matches,\n",
    "#         'Partial Path Matches': partial_path_matches\n",
    "#     }\n",
    "#     return results\n",
    "\n",
    "\n",
    "# def compute_cot_hop_em(df: pd.DataFrame) -> float:\n",
    "#     \"\"\"Strict CoT EM - exact hop sequence matching\"\"\"\n",
    "#     exact_matches = 0\n",
    "#     total = 0\n",
    "#     for _, row in df.iterrows():\n",
    "#         gold_hops = row.get('gold_hops', [])\n",
    "#         pred_hops = row.get('model_reasoning_steps', [])\n",
    "        \n",
    "#         if not gold_hops or not pred_hops or not pred_hops[0]:\n",
    "#             continue\n",
    "            \n",
    "#         total += 1\n",
    "#         if len(pred_hops) == len(gold_hops) and \\\n",
    "#            all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "#             exact_matches += 1\n",
    "            \n",
    "#     cot_em = exact_matches / total if total > 0 else 0.0\n",
    "#     return cot_em\n",
    "\n",
    "# def compute_cot_hop_em_by_category(df: pd.DataFrame) -> dict:\n",
    "#     \"\"\"Compute strict CoT-EM by hop category\"\"\"\n",
    "#     results = {}\n",
    "#     sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "    \n",
    "#     for cat in sorted_categories:\n",
    "#         subdf = df[df['hop_category'] == cat]\n",
    "#         if subdf.empty:\n",
    "#             results[str(cat)] = 0.0\n",
    "#             continue\n",
    "            \n",
    "#         exact_matches = 0\n",
    "#         total = 0\n",
    "#         for _, row in subdf.iterrows():\n",
    "#             gold_hops = row.get('gold_hops', [])\n",
    "#             pred_hops = row.get('model_reasoning_steps', [])\n",
    "#             if not gold_hops or not pred_hops or not pred_hops[0]: \n",
    "#                 continue\n",
    "#             total += 1\n",
    "#             if len(pred_hops) == len(gold_hops) and \\\n",
    "#                all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "#                 exact_matches += 1\n",
    "                \n",
    "#         results[str(cat)] = exact_matches / total if total > 0 else 0.0\n",
    "#     return results\n",
    "\n",
    "# # --- ENHANCED VISUALIZATION ---\n",
    "# def visualize_hop_wise_results(hop_results, step_results, strict_cot_em_overall, cot_em_per_cat_strict, config):\n",
    "#     \"\"\"Enhanced visualization with better styling\"\"\"\n",
    "#     plt.style.use('default')  # Use default style for compatibility\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "#     # Main performance chart\n",
    "#     categories = [cat for cat in hop_results.keys() if cat != 'overall']\n",
    "#     counts = [hop_results[cat]['count'] for cat in categories]\n",
    "#     em_scores = [hop_results[cat]['EM'] for cat in categories]\n",
    "#     f1_scores = [hop_results[cat]['F1'] for cat in categories]\n",
    "#     bleu_scores = [hop_results[cat]['BLEU'] for cat in categories]\n",
    "    \n",
    "#     soft_cot_em_by_cat = step_results.get('CoT-EM by Category', {})\n",
    "#     soft_cot_em_scores = [soft_cot_em_by_cat.get(cat, 0) for cat in categories]\n",
    "#     strict_cot_em_scores = [cot_em_per_cat_strict.get(cat, 0) for cat in categories]\n",
    "    \n",
    "#     x = np.arange(len(categories))\n",
    "#     width = 0.15\n",
    "    \n",
    "#     ax1.bar(x - width*2, em_scores, width, label='Exact Match', color='#3498db', alpha=0.8)\n",
    "#     ax1.bar(x - width, f1_scores, width, label='F1 Score', color='#2ecc71', alpha=0.8)\n",
    "#     ax1.bar(x, bleu_scores, width, label='BLEU Score', color='#f39c12', alpha=0.8)\n",
    "#     ax1.bar(x + width, soft_cot_em_scores, width, label='Soft CoT-EM', color='#e74c3c', alpha=0.8)\n",
    "#     ax1.bar(x + width*2, strict_cot_em_scores, width, label='Strict CoT-EM', color='#8e44ad', alpha=0.8)\n",
    "    \n",
    "#     ax1.set_xlabel('Reasoning Complexity', fontweight='bold')\n",
    "#     ax1.set_ylabel('Score', fontweight='bold')\n",
    "#     ax1.set_title(f'{config.model_name} Performance by Complexity', fontweight='bold', pad=20)\n",
    "#     ax1.set_xticks(x)\n",
    "#     ax1.set_xticklabels(categories)\n",
    "#     ax1.set_ylim(0, 1.05)\n",
    "#     ax1.legend(loc='upper right')\n",
    "#     ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "#     # Add sample counts\n",
    "#     for i, count in enumerate(counts):\n",
    "#         if count > 0:\n",
    "#             ax1.text(i, 0.02, f'n={count}', ha='center', va='bottom', fontsize=9,\n",
    "#                     bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.2'))\n",
    "    \n",
    "#     # Distribution pie chart\n",
    "#     if counts and sum(counts) > 0:\n",
    "#         ax2.pie(counts, labels=categories, autopct='%1.1f%%', startangle=90)\n",
    "#         ax2.set_title('Question Distribution by Complexity', fontweight='bold')\n",
    "#     else:\n",
    "#         ax2.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax2.transAxes)\n",
    "#         ax2.set_title('No Distribution Data', fontweight='bold')\n",
    "    \n",
    "#     # Overall metrics text\n",
    "#     overall = hop_results['overall']\n",
    "#     overall_text = (f\"Overall Results (n={overall['count']}):\\n\"\n",
    "#                    f\"EM: {overall['EM']:.3f} | F1: {overall['F1']:.3f} | BLEU: {overall['BLEU']:.3f}\\n\"\n",
    "#                    f\"Soft CoT-EM: {step_results['CoT-EM']:.3f} | Strict CoT-EM: {strict_cot_em_overall:.3f}\")\n",
    "    \n",
    "#     fig.text(0.5, 0.02, overall_text, ha='center', fontsize=11, fontweight='bold',\n",
    "#              bbox=dict(facecolor='lightgray', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "#     plt.tight_layout(rect=[0, 0.1, 1, 0.96])\n",
    "    \n",
    "#     # Save plot\n",
    "#     plt.savefig(config.output_image_path, dpi=300, bbox_inches='tight')\n",
    "#     print(f\"Visualization saved to {config.output_image_path}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # --- MAIN EXECUTION ---\n",
    "# def main():\n",
    "#     \"\"\"Main execution function with comprehensive error handling\"\"\"\n",
    "#     print(\"=== MenatQA Evaluation Pipeline ===\\n\")\n",
    "    \n",
    "#     # Initialize configuration\n",
    "#     config = EvaluationConfig()\n",
    "#     print(f\"Output directory: {config.output_dir}\")\n",
    "#     print(f\"Model: {config.model_name}\")\n",
    "#     print(f\"Max samples: {config.max_samples or 'All'}\\n\")\n",
    "    \n",
    "#     try:\n",
    "#         # Load and preprocess data\n",
    "#         print(\" Loading dataset...\")\n",
    "#         data = load_menatqa_dataset(config.dataset_file)\n",
    "        \n",
    "#         if config.max_samples is not None:\n",
    "#             print(f\"Limiting to {config.max_samples} samples for testing\")\n",
    "#             data = data[:config.max_samples]\n",
    "        \n",
    "#         print(\" Preprocessing dataset...\")\n",
    "#         df = preprocess_dataset(data)\n",
    "        \n",
    "#         # Run model inference\n",
    "#         print(f\"\\n Running {config.model_name} inference...\")\n",
    "#         df_results = run_qwen_cot_predictions(df, config)\n",
    "        \n",
    "#         # Evaluate results\n",
    "#         print(\"\\n Evaluating results...\")\n",
    "#         hop_results = evaluate_hop_wise_accuracy(df_results)\n",
    "#         step_verification_results = evaluate_step_verification(df_results)\n",
    "#         strict_cot_em_overall = compute_cot_hop_em(df_results)\n",
    "#         strict_cot_em_by_category = compute_cot_hop_em_by_category(df_results)\n",
    "        \n",
    "#         # Display results\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\" RESULTS SUMMARY\")\n",
    "#         print(\"=\"*60)\n",
    "        \n",
    "#         print(\"\\n--- Answer Accuracy by Complexity ---\")\n",
    "#         for category, metrics in hop_results.items():\n",
    "#             if category == 'overall': continue\n",
    "#             print(f\"{category:>10} (n={metrics['count']:>3}): EM={metrics['EM']:.3f}, F1={metrics['F1']:.3f}, BLEU={metrics['BLEU']:.3f}\")\n",
    "        \n",
    "#         overall = hop_results['overall']\n",
    "#         print(f\"{'Overall':>10} (n={overall['count']:>3}): EM={overall['EM']:.3f}, F1={overall['F1']:.3f}, BLEU={overall['BLEU']:.3f}\")\n",
    "        \n",
    "#         print(\"\\n--- Reasoning Path Verification ---\")\n",
    "#         print(f\"Soft CoT-EM (Path):   {step_verification_results['CoT-EM']:.3f}\")\n",
    "#         print(f\"CoT-Partial (Path):   {step_verification_results['CoT-Partial']:.3f}\")\n",
    "#         print(f\"Hop-Match-Rate:       {step_verification_results['Hop-Match-Rate']:.3f}\")\n",
    "#         print(f\"Strict CoT-EM (Path): {strict_cot_em_overall:.3f}\")\n",
    "        \n",
    "#         print(\"\\nBy Category - Soft CoT-EM:\")\n",
    "#         for cat, score in step_verification_results['CoT-EM by Category'].items():\n",
    "#             print(f\"  {cat}: {score:.3f}\")\n",
    "        \n",
    "#         # Generate visualization\n",
    "#         print(\"\\n Generating visualization...\")\n",
    "#         visualize_hop_wise_results(\n",
    "#             hop_results, step_verification_results, \n",
    "#             strict_cot_em_overall, strict_cot_em_by_category, config\n",
    "#         )\n",
    "        \n",
    "#         # Save comprehensive results\n",
    "#         summary_results = {\n",
    "#             'config': config.to_dict(),\n",
    "#             'dataset_info': {\n",
    "#                 'name': 'MenatQA',\n",
    "#                 'total_questions_processed': len(df_results),\n",
    "#                 'hop_distribution': df_results['hop_category'].value_counts().to_dict()\n",
    "#             },\n",
    "#             'answer_accuracy_metrics': hop_results,\n",
    "#             'reasoning_path_verification_metrics': step_verification_results,\n",
    "#             'strict_cot_em_overall': float(strict_cot_em_overall),\n",
    "#             'strict_cot_em_by_category': {k: float(v) for k, v in strict_cot_em_by_category.items()}\n",
    "#         }\n",
    "        \n",
    "#         with open(config.output_results_path, 'w') as f:\n",
    "#             json.dump(summary_results, f, indent=2, default=str)\n",
    "        \n",
    "#         print(f\"\\n Evaluation complete!\")\n",
    "#         print(f\" Results saved to: {config.output_results_path}\")\n",
    "#         print(f\" Visualization saved to: {config.output_image_path}\")\n",
    "        \n",
    "#         return df_results, summary_results\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n Error during evaluation: {e}\")\n",
    "#         print(\"This might be due to model loading issues. Check your GPU/CPU setup.\")\n",
    "#         return None, None\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Set test configuration for initial run\n",
    "#     config = EvaluationConfig()\n",
    "#     config.max_samples = 10  # Small test run\n",
    "#     config.model_name = \"Qwen/Qwen3-0.6B\"  # Try smaller model first\n",
    "    \n",
    "#     print(\" Running test evaluation with limited samples...\")\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adfec6-28a5-4e58-b822-745ada25349a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9829c4c-831b-4909-bb8a-efc715521913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed73fe5-0367-4410-8b0a-dbe60514dac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa51703-6fef-45f1-841f-258e7d699a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cda196-4f54-4f58-ad7d-e126b99a8afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cb566-1fa6-4a77-abd5-97c3a9cbc9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397bb850-9482-4dec-b8fe-f2e9d8ee773e",
   "metadata": {},
   "source": [
    "**NEW CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01526a60-66f5-48a2-afc2-360eff58fabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84480c8-8fd9-4b2e-8d9f-786d9ce3256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from pathlib import Path\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# # Download required NLTK data\n",
    "# import nltk\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     print(\"Downloading required NLTK data...\")\n",
    "#     nltk.download('punkt', quiet=True)\n",
    "\n",
    "# # --- DATA ---\n",
    "# def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "#     \"\"\"Load MenatQA dataset\"\"\"\n",
    "#     file_path = Path(file_path)\n",
    "    \n",
    "#     if not file_path.exists():\n",
    "#         import urllib.request\n",
    "#         print(f\"Downloading MenatQA to {file_path}...\")\n",
    "#         urllib.request.urlretrieve(\n",
    "#             \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "#             str(file_path)\n",
    "#         )\n",
    "    \n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "#     return data\n",
    "\n",
    "# def extract_reasoning_hops_advanced(example):\n",
    "#     \"\"\"Enhanced hop detection with better linguistic analysis\"\"\"\n",
    "#     question = example.get('question', '')\n",
    "#     answer = example.get('answer', '')\n",
    "#     q_type = example.get('type', '')\n",
    "#     time_scope = example.get('time_scope', '')\n",
    "    \n",
    "#     # More sophisticated analysis\n",
    "#     sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "    \n",
    "#     # Enhanced logical connector detection\n",
    "#     logical_connectors = ['and', 'or', 'but', 'because', 'since', 'although', \n",
    "#                          'however', 'therefore', 'moreover', 'furthermore', 'while']\n",
    "#     temporal_words = ['before', 'after', 'during', 'when', 'then', 'until', 'since']\n",
    "    \n",
    "#     connector_count = sum(1 for word in logical_connectors if word in question.lower())\n",
    "#     temporal_count = sum(1 for word in temporal_words if word in question.lower())\n",
    "    \n",
    "#     # Question complexity indicators\n",
    "#     question_words = ['what', 'where', 'when', 'who', 'why', 'how', 'which']\n",
    "#     q_word_count = sum(1 for word in question_words if word in question.lower())\n",
    "    \n",
    "#     capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "#     # Enhanced complexity scoring\n",
    "#     complexity_score = 1\n",
    "#     complexity_score += min(2, len(sentences) - 1)  # Multiple sentences\n",
    "#     complexity_score += min(1, connector_count)     # Logical connections\n",
    "#     complexity_score += min(1, temporal_count)      # Temporal reasoning\n",
    "#     complexity_score += min(1, q_word_count - 1)    # Multiple question aspects\n",
    "#     complexity_score += min(1, capitalized_words // 4)  # Named entities\n",
    "    \n",
    "#     if time_scope:\n",
    "#         complexity_score += 1\n",
    "    \n",
    "#     complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "#     # Generate reasoning hops\n",
    "#     hops = []\n",
    "#     hops.append(f\"Understand the question: {question}\")\n",
    "    \n",
    "#     if complexity_score >= 2:\n",
    "#         if time_scope:\n",
    "#             hops.append(f\"Identify temporal context: {time_scope}\")\n",
    "#         elif connector_count > 0:\n",
    "#             hops.append(\"Analyze logical relationships in the question\")\n",
    "#         else:\n",
    "#             hops.append(f\"Recognize question type: {q_type}\")\n",
    "    \n",
    "#     if complexity_score >= 3:\n",
    "#         hops.append(\"Retrieve and analyze relevant facts\")\n",
    "        \n",
    "#     if complexity_score >= 4:\n",
    "#         hops.append(\"Synthesize information and resolve dependencies\")\n",
    "    \n",
    "#     hops.append(f\"Formulate final answer: {answer}\")\n",
    "    \n",
    "#     return hops, complexity_score\n",
    "\n",
    "# def preprocess_dataset(data):\n",
    "#     \"\"\"Preprocess dataset with enhanced hop extraction\"\"\"\n",
    "#     processed_data = []\n",
    "#     hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "#     for item in data:\n",
    "#         gold_hops, complexity_score = extract_reasoning_hops_advanced(item)\n",
    "#         hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "#         entry = {\n",
    "#             'ID': item.get('ID', ''),\n",
    "#             'question': item.get('question', ''),\n",
    "#             'answer': item.get('answer', ''),\n",
    "#             'type': item.get('type', ''),\n",
    "#             'time_scope': item.get('time_scope', ''),\n",
    "#             'gold_hops': gold_hops,\n",
    "#             'hop_count': complexity_score,\n",
    "#             'model_prediction': '',\n",
    "#             'model_reasoning_steps': []\n",
    "#         }\n",
    "#         processed_data.append(entry)\n",
    "    \n",
    "#     df = pd.DataFrame(processed_data)\n",
    "#     df['hop_category'] = pd.cut(\n",
    "#         df['hop_count'],\n",
    "#         bins=[-1, 1, 2, 3, float('inf')],\n",
    "#         labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "#     for i in range(1, 5):\n",
    "#         print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # --- MODEL ---\n",
    "# def run_qwen_predictions(df, model_name=\"Qwen/Qwen3-0.6B\"):\n",
    "#     \"\"\"Run Qwen model inference\"\"\"\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     print(f\"Loading model {model_name} on {device}...\")\n",
    "    \n",
    "#     # Clear GPU memory\n",
    "#     if device == \"cuda\":\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    \n",
    "#     # Create pipeline\n",
    "#     pipe = pipeline(\n",
    "#         \"text-generation\", \n",
    "#         model=model, \n",
    "#         tokenizer=tokenizer, \n",
    "#         device=0 if device == 'cuda' else -1,\n",
    "#         max_new_tokens=256,\n",
    "#         do_sample=False\n",
    "#     )\n",
    "\n",
    "#     def format_cot_prompt(question):\n",
    "#         return (f\"Answer the following question step by step. \"\n",
    "#                 f\"Write each reasoning step on a new line, and finish with your answer.\\n\"\n",
    "#                 f\"Question: {question}\\nLet's think step by step:\")\n",
    "\n",
    "#     all_preds, all_steps = [], []\n",
    "    \n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating predictions\"):\n",
    "#         prompt = format_cot_prompt(row[\"question\"])\n",
    "#         generation = pipe(prompt)[0]['generated_text']\n",
    "#         output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "#         steps = [x.strip() for x in output.split('\\n') if x.strip()]\n",
    "        \n",
    "#         if not steps:\n",
    "#             steps = [\"\"]\n",
    "            \n",
    "#         model_answer = steps[-1]\n",
    "#         all_preds.append(model_answer)\n",
    "#         all_steps.append(steps)\n",
    "    \n",
    "#     df = df.copy()\n",
    "#     df['model_prediction'] = all_preds\n",
    "#     df['model_reasoning_steps'] = all_steps\n",
    "#     return df\n",
    "\n",
    "# # --- ENHANCED METRICS ---\n",
    "# def compute_exact_match(prediction, gold):\n",
    "#     \"\"\"Enhanced exact match with better preprocessing\"\"\"\n",
    "#     pred_clean = str(prediction).strip().lower()\n",
    "#     gold_clean = str(gold).strip().lower()\n",
    "    \n",
    "#     # Remove common prefixes/suffixes\n",
    "#     prefixes = [\"the answer is\", \"answer:\", \"final answer:\"]\n",
    "#     for prefix in prefixes:\n",
    "#         if pred_clean.startswith(prefix):\n",
    "#             pred_clean = pred_clean[len(prefix):].strip()\n",
    "    \n",
    "#     return int(pred_clean == gold_clean)\n",
    "\n",
    "# def compute_f1(prediction, gold):\n",
    "#     \"\"\"Enhanced F1 with better tokenization\"\"\"\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "    \n",
    "#     # Remove stopwords for better matching\n",
    "#     stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}\n",
    "#     pred_tokens = [t for t in pred_tokens if t not in stopwords]\n",
    "#     gold_tokens = [t for t in gold_tokens if t not in stopwords]\n",
    "    \n",
    "#     common = set(pred_tokens) & set(gold_tokens)\n",
    "    \n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "    \n",
    "#     prec = len(common) / len(pred_tokens)\n",
    "#     rec = len(common) / len(gold_tokens)\n",
    "    \n",
    "#     if prec + rec == 0:\n",
    "#         return 0.0\n",
    "    \n",
    "#     return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "# def compute_bleu(prediction, gold):\n",
    "#     \"\"\"Enhanced BLEU computation\"\"\"\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "\n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "\n",
    "#     reference = [gold_tokens]\n",
    "#     candidate = pred_tokens\n",
    "    \n",
    "#     weights = (0.25, 0.25, 0.25, 0.25)\n",
    "#     chencherry = SmoothingFunction()\n",
    "    \n",
    "#     try:\n",
    "#         bleu_score = sentence_bleu(\n",
    "#             reference, candidate, \n",
    "#             weights=weights, \n",
    "#             smoothing_function=chencherry.method1\n",
    "#         )\n",
    "#     except (ZeroDivisionError, ValueError):\n",
    "#         bleu_score = 0.0\n",
    "    \n",
    "#     return bleu_score\n",
    "\n",
    "# def evaluate_hop_wise_accuracy(df):\n",
    "#     categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "#     hop_results = {}\n",
    "#     overall_em, overall_f1, overall_bleu, overall_count = 0, 0, 0, 0\n",
    "    \n",
    "#     for cat in categories:\n",
    "#         subdf = df[df['hop_category'] == cat]\n",
    "#         if subdf.empty:\n",
    "#             hop_results[str(cat)] = {'count': 0, 'EM': 0, 'F1': 0, 'BLEU': 0}\n",
    "#             continue\n",
    "\n",
    "#         ems = [compute_exact_match(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "#         f1s = [compute_f1(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "#         bleus = [compute_bleu(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "        \n",
    "#         count = len(subdf)\n",
    "#         mean_em = np.mean(ems) if ems else 0\n",
    "#         mean_f1 = np.mean(f1s) if f1s else 0\n",
    "#         mean_bleu = np.mean(bleus) if bleus else 0\n",
    "        \n",
    "#         hop_results[str(cat)] = {'count': count, 'EM': mean_em, 'F1': mean_f1, 'BLEU': mean_bleu}\n",
    "        \n",
    "#         overall_em += sum(ems)\n",
    "#         overall_f1 += sum(f1s)\n",
    "#         overall_bleu += sum(bleus)\n",
    "#         overall_count += count\n",
    "        \n",
    "#     hop_results['overall'] = {\n",
    "#         'count': overall_count,\n",
    "#         'EM': overall_em / overall_count if overall_count else 0,\n",
    "#         'F1': overall_f1 / overall_count if overall_count else 0,\n",
    "#         'BLEU': overall_bleu / overall_count if overall_count else 0,\n",
    "#     }\n",
    "#     return hop_results\n",
    "\n",
    "# def evaluate_step_verification(df):\n",
    "#     all_step_matches = []\n",
    "#     complete_path_matches = 0\n",
    "#     partial_path_matches = 0\n",
    "#     total_questions = 0\n",
    "#     sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "#     hop_category_results = defaultdict(lambda: {'matches': 0, 'partial_matches': 0, 'total': 0})\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         gold_hops = row['gold_hops']\n",
    "#         model_steps = row['model_reasoning_steps']\n",
    "        \n",
    "#         if not gold_hops or not model_steps or not model_steps[0]:\n",
    "#             continue\n",
    "            \n",
    "#         total_questions += 1\n",
    "#         hop_cat = row['hop_category']\n",
    "#         if hop_cat is None: continue\n",
    "\n",
    "#         hop_category_results[hop_cat]['total'] += 1\n",
    "        \n",
    "#         step_matches = []\n",
    "#         first_match = 0\n",
    "#         last_match = 0\n",
    "#         middle_matches = []\n",
    "\n",
    "#         # First step (question understanding)\n",
    "#         gold_q_terms = set(re.findall(r'\\w+', gold_hops[0].lower()))\n",
    "#         model_q_terms = set(re.findall(r'\\w+', model_steps[0].lower()))\n",
    "#         if gold_q_terms:\n",
    "#             common_terms = gold_q_terms & model_q_terms\n",
    "#             first_match = len(common_terms) / len(gold_q_terms)\n",
    "#         step_matches.append(first_match)\n",
    "\n",
    "#         # Last step (answer formulation)\n",
    "#         if len(gold_hops) > 1 and len(model_steps) > 1:\n",
    "#             gold_ans_text = row['answer'].lower()\n",
    "#             model_last_hop_text = model_steps[-1].lower()\n",
    "#             last_match = 1.0 if gold_ans_text in model_last_hop_text else 0.0\n",
    "#             step_matches.append(last_match)\n",
    "\n",
    "#         # Intermediate steps (middle reasoning)\n",
    "#         if len(gold_hops) > 2 and len(model_steps) > 2:\n",
    "#             num_intermediate_to_compare = min(len(gold_hops) - 2, len(model_steps) - 2)\n",
    "            \n",
    "#             for i in range(num_intermediate_to_compare):\n",
    "#                 gold_mid_idx = i + 1\n",
    "#                 model_mid_idx = i + 1\n",
    "                \n",
    "#                 gold_mid = gold_hops[gold_mid_idx].lower()\n",
    "#                 model_mid = model_steps[model_mid_idx].lower()\n",
    "                \n",
    "#                 gold_words = set(re.findall(r'\\w+', gold_mid))\n",
    "#                 model_words = set(re.findall(r'\\w+', model_mid))\n",
    "                \n",
    "#                 if gold_words and model_words:\n",
    "#                     common_words = gold_words & model_words\n",
    "#                     similarity = len(common_words) / max(len(gold_words), len(model_words))\n",
    "#                     middle_matches.append(similarity)\n",
    "#                     step_matches.append(similarity)\n",
    "#                 elif gold_words or model_words:\n",
    "#                     middle_matches.append(0.0)\n",
    "#                     step_matches.append(0.0)\n",
    "\n",
    "#         all_step_matches.extend(step_matches)\n",
    "        \n",
    "#                 if step_matches:\n",
    "#             is_complete_match = all(match >= 0.6 for match in step_matches)\n",
    "#             is_partial_match = False\n",
    "#             if not is_complete_match:\n",
    "#                 avg_middle_match = sum(middle_matches) / len(middle_matches) if middle_matches else 1.0\n",
    "#                 if first_match >= 0.7 and last_match >= 0.7 and avg_middle_match >= 0.4:\n",
    "#                     is_partial_match = True\n",
    "            \n",
    "#             if is_complete_match:\n",
    "#                 complete_path_matches += 1\n",
    "#                 hop_category_results[hop_cat]['matches'] += 1\n",
    "#             elif is_partial_match:\n",
    "#                 partial_path_matches += 1\n",
    "#                 hop_category_results[hop_cat]['partial_matches'] += 1\n",
    "\n",
    "#     cot_em_overall = complete_path_matches / total_questions if total_questions > 0 else 0\n",
    "#     cot_partial_overall = (complete_path_matches + partial_path_matches) / total_questions if total_questions > 0 else 0\n",
    "    \n",
    "#     cot_em_by_category = {}\n",
    "#     cot_partial_by_category = {}\n",
    "\n",
    "#     for hop_cat_key in sorted_categories:\n",
    "#         data = hop_category_results[hop_cat_key]\n",
    "#         cat_total = data['total']\n",
    "#         cot_em = data['matches'] / cat_total if cat_total > 0 else 0\n",
    "#         cot_partial = (data['matches'] + data['partial_matches']) / cat_total if cat_total > 0 else 0\n",
    "#         cot_em_by_category[str(hop_cat_key)] = cot_em\n",
    "#         cot_partial_by_category[str(hop_cat_key)] = cot_partial\n",
    "        \n",
    "#     results = {\n",
    "#         'CoT-EM': cot_em_overall,\n",
    "#         'CoT-Partial': cot_partial_overall,\n",
    "#         'Hop-Match-Rate': np.mean(all_step_matches) if all_step_matches else 0,\n",
    "#         'CoT-EM by Category': cot_em_by_category,\n",
    "#         'CoT-Partial by Category': cot_partial_by_category,\n",
    "#         'Total Questions Evaluated for Step Verification': total_questions,\n",
    "#         'Complete Path Matches': complete_path_matches,\n",
    "#         'Partial Path Matches': partial_path_matches\n",
    "#     }\n",
    "#     return results\n",
    "\n",
    "# def compute_cot_hop_em(df):\n",
    "#     \"\"\"Strict CoT EM - exact hop sequence matching\"\"\"\n",
    "#     exact_matches = 0\n",
    "#     total = 0\n",
    "#     for _, row in df.iterrows():\n",
    "#         gold_hops = row.get('gold_hops', [])\n",
    "#         pred_hops = row.get('model_reasoning_steps', [])\n",
    "        \n",
    "#         if not gold_hops or not pred_hops or not pred_hops[0]:\n",
    "#             continue\n",
    "            \n",
    "#         total += 1\n",
    "#         if len(pred_hops) == len(gold_hops) and \\\n",
    "#            all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "#             exact_matches += 1\n",
    "            \n",
    "#     return exact_matches / total if total > 0 else 0.0\n",
    "\n",
    "# def compute_cot_hop_em_by_category(df):\n",
    "#     \"\"\"Compute strict CoT-EM by hop category\"\"\"\n",
    "#     results = {}\n",
    "#     sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "    \n",
    "#     for cat in sorted_categories:\n",
    "#         subdf = df[df['hop_category'] == cat]\n",
    "#         if subdf.empty:\n",
    "#             results[str(cat)] = 0.0\n",
    "#             continue\n",
    "            \n",
    "#         exact_matches = 0\n",
    "#         total = 0\n",
    "#         for _, row in subdf.iterrows():\n",
    "#             gold_hops = row.get('gold_hops', [])\n",
    "#             pred_hops = row.get('model_reasoning_steps', [])\n",
    "#             if not gold_hops or not pred_hops or not pred_hops[0]: \n",
    "#                 continue\n",
    "#             total += 1\n",
    "#             if len(pred_hops) == len(gold_hops) and \\\n",
    "#                all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "#                 exact_matches += 1\n",
    "                \n",
    "#         results[str(cat)] = exact_matches / total if total > 0 else 0.0\n",
    "#     return results\n",
    "\n",
    "# # --- ENHANCED VISUALIZATION ---\n",
    "# def visualize_hop_wise_results(hop_results, step_results, strict_cot_em_overall, cot_em_per_cat_strict, save_path=None):\n",
    "#     \"\"\"Enhanced visualization with better styling\"\"\"\n",
    "#     plt.style.use('default')\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "#     # Main performance chart\n",
    "#     categories = [cat for cat in hop_results.keys() if cat != 'overall']\n",
    "#     counts = [hop_results[cat]['count'] for cat in categories]\n",
    "#     em_scores = [hop_results[cat]['EM'] for cat in categories]\n",
    "#     f1_scores = [hop_results[cat]['F1'] for cat in categories]\n",
    "#     bleu_scores = [hop_results[cat]['BLEU'] for cat in categories]\n",
    "    \n",
    "#     soft_cot_em_by_cat = step_results.get('CoT-EM by Category', {})\n",
    "#     soft_cot_em_scores = [soft_cot_em_by_cat.get(cat, 0) for cat in categories]\n",
    "#     strict_cot_em_scores = [cot_em_per_cat_strict.get(cat, 0) for cat in categories]\n",
    "    \n",
    "#     x = np.arange(len(categories))\n",
    "#     width = 0.15\n",
    "    \n",
    "#     ax1.bar(x - width*2, em_scores, width, label='Exact Match', color='#3498db', alpha=0.8)\n",
    "#     ax1.bar(x - width, f1_scores, width, label='F1 Score', color='#2ecc71', alpha=0.8)\n",
    "#     ax1.bar(x, bleu_scores, width, label='BLEU Score', color='#f39c12', alpha=0.8)\n",
    "#     ax1.bar(x + width, soft_cot_em_scores, width, label='Soft CoT-EM', color='#e74c3c', alpha=0.8)\n",
    "#     ax1.bar(x + width*2, strict_cot_em_scores, width, label='Strict CoT-EM', color='#8e44ad', alpha=0.8)\n",
    "    \n",
    "#     ax1.set_xlabel('Reasoning Complexity', fontweight='bold')\n",
    "#     ax1.set_ylabel('Score', fontweight='bold')\n",
    "#     ax1.set_title('Qwen3-0.6B Performance by Complexity', fontweight='bold', pad=20)\n",
    "#     ax1.set_xticks(x)\n",
    "#     ax1.set_xticklabels(categories)\n",
    "#     ax1.set_ylim(0, 1.05)\n",
    "#     ax1.legend(loc='upper right')\n",
    "#     ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "#     # Add sample counts\n",
    "#     for i, count in enumerate(counts):\n",
    "#         if count > 0:\n",
    "#             ax1.text(i, 0.02, f'n={count}', ha='center', va='bottom', fontsize=9,\n",
    "#                     bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.2'))\n",
    "    \n",
    "#     # Distribution pie chart\n",
    "#     if counts and sum(counts) > 0:\n",
    "#         ax2.pie(counts, labels=categories, autopct='%1.1f%%', startangle=90)\n",
    "#         ax2.set_title('Question Distribution by Complexity', fontweight='bold')\n",
    "#     else:\n",
    "#         ax2.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax2.transAxes)\n",
    "#         ax2.set_title('No Distribution Data', fontweight='bold')\n",
    "    \n",
    "#     # Overall metrics text\n",
    "#     overall = hop_results['overall']\n",
    "#     overall_text = (f\"Overall Results (n={overall['count']}):\\n\"\n",
    "#                    f\"EM: {overall['EM']:.3f} | F1: {overall['F1']:.3f} | BLEU: {overall['BLEU']:.3f}\\n\"\n",
    "#                    f\"Soft CoT-EM: {step_results['CoT-EM']:.3f} | Strict CoT-EM: {strict_cot_em_overall:.3f}\")\n",
    "    \n",
    "#     fig.text(0.5, 0.02, overall_text, ha='center', fontsize=11, fontweight='bold',\n",
    "#              bbox=dict(facecolor='lightgray', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "#     plt.tight_layout(rect=[0, 0.1, 1, 0.96])\n",
    "    \n",
    "#     # Save plot\n",
    "#     if save_path:\n",
    "#         plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#         print(f\"Visualization saved to {save_path}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # --- MAIN EXECUTION ---\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     print(\"=== MenatQA Evaluation Pipeline ===\\n\")\n",
    "    \n",
    "#     # Configuration\n",
    "#     OUTPUT_DIR = Path.cwd() / \"menatqa_evaluation\"\n",
    "#     OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "#     DATASET_FILE = OUTPUT_DIR / 'MenatQA.json'\n",
    "#     MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "#     OUTPUT_IMAGE_PATH = OUTPUT_DIR / 'menatqa_qwen3_performance.png'\n",
    "#     OUTPUT_RESULTS_PATH = OUTPUT_DIR / 'menatqa_qwen3_results.json'\n",
    "    \n",
    "#     print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "#     print(f\"Model: {MODEL_NAME}\\n\")\n",
    "    \n",
    "#     # Load and preprocess data\n",
    "#     print(\" Loading dataset...\")\n",
    "#     data = load_menatqa_dataset(DATASET_FILE)\n",
    "    \n",
    "#     print(\" Preprocessing dataset...\")\n",
    "#     df = preprocess_dataset(data)\n",
    "    \n",
    "#     # Run model inference\n",
    "#     print(f\"\\n Running {MODEL_NAME} inference...\")\n",
    "#     df_results = run_qwen_predictions(df, model_name=MODEL_NAME)\n",
    "    \n",
    "#     # Evaluate results\n",
    "#     print(\"\\n Evaluating results...\")\n",
    "#     hop_results = evaluate_hop_wise_accuracy(df_results)\n",
    "#     step_verification_results = evaluate_step_verification(df_results)\n",
    "#     strict_cot_em_overall = compute_cot_hop_em(df_results)\n",
    "#     strict_cot_em_by_category = compute_cot_hop_em_by_category(df_results)\n",
    "    \n",
    "#     # Display results\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\" RESULTS SUMMARY\")\n",
    "#     print(\"=\"*60)\n",
    "    \n",
    "#     print(\"\\n--- Answer Accuracy by Complexity ---\")\n",
    "#     for category, metrics in hop_results.items():\n",
    "#         if category == 'overall': continue\n",
    "#         print(f\"{category:>10} (n={metrics['count']:>3}): EM={metrics['EM']:.3f}, F1={metrics['F1']:.3f}, BLEU={metrics['BLEU']:.3f}\")\n",
    "    \n",
    "#     overall = hop_results['overall']\n",
    "#     print(f\"{'Overall':>10} (n={overall['count']:>3}): EM={overall['EM']:.3f}, F1={overall['F1']:.3f}, BLEU={overall['BLEU']:.3f}\")\n",
    "    \n",
    "#     print(\"\\n--- Reasoning Path Verification ---\")\n",
    "#     print(f\"Soft CoT-EM (Path):   {step_verification_results['CoT-EM']:.3f}\")\n",
    "#     print(f\"CoT-Partial (Path):   {step_verification_results['CoT-Partial']:.3f}\")\n",
    "#     print(f\"Hop-Match-Rate:       {step_verification_results['Hop-Match-Rate']:.3f}\")\n",
    "#     print(f\"Strict CoT-EM (Path): {strict_cot_em_overall:.3f}\")\n",
    "    \n",
    "#     print(\"\\nBy Category - Soft CoT-EM:\")\n",
    "#     for cat, score in step_verification_results['CoT-EM by Category'].items():\n",
    "#         print(f\"  {cat}: {score:.3f}\")\n",
    "    \n",
    "#     # Generate visualization\n",
    "#     print(\"\\n Generating visualization...\")\n",
    "#     visualize_hop_wise_results(\n",
    "#         hop_results, step_verification_results, \n",
    "#         strict_cot_em_overall, strict_cot_em_by_category, \n",
    "#         save_path=OUTPUT_IMAGE_PATH\n",
    "#     )\n",
    "    \n",
    "#     # Save comprehensive results\n",
    "#     summary_results = {\n",
    "#         'model_name': MODEL_NAME,\n",
    "#         'dataset_info': {\n",
    "#             'name': 'MenatQA',\n",
    "#             'total_questions_processed': len(df_results),\n",
    "#             'hop_distribution': df_results['hop_category'].value_counts().to_dict()\n",
    "#         },\n",
    "#         'answer_accuracy_metrics': hop_results,\n",
    "#         'reasoning_path_verification_metrics': step_verification_results,\n",
    "#         'strict_cot_em_overall': float(strict_cot_em_overall),\n",
    "#         'strict_cot_em_by_category': {k: float(v) for k, v in strict_cot_em_by_category.items()}\n",
    "#     }\n",
    "    \n",
    "#     with open(OUTPUT_RESULTS_PATH, 'w') as f:\n",
    "#         json.dump(summary_results, f, indent=2, default=str)\n",
    "    \n",
    "#     print(f\"\\n Evaluation complete!\")\n",
    "#     print(f\" Results saved to: {OUTPUT_RESULTS_PATH}\")\n",
    "#     print(f\" Visualization saved to: {OUTPUT_IMAGE_PATH}\")\n",
    "    \n",
    "#     return df_results, summary_results\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print(\" Running full MenatQA evaluation on Qwen3-0.6B...\")\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcbae4-d957-4aeb-9803-82e6f6da930d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52abf3-828c-46e8-bd49-f6359e322a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220bbc9e-defd-48c8-8786-56883cd90cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d93237-626d-4eba-a5fa-d33445b8b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83fc50-a158-40b4-8b47-d50a37375b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MenatQA Evaluation on Qwen ===\n",
      "\n",
      "Loading MenatQA dataset...\n",
      "Loaded MenatQA dataset with 999 examples\n",
      "Preprocessing dataset...\n",
      "Dataset preprocessed: 999 questions\n",
      "1-hop questions: 0 (0.0%)\n",
      "2-hop questions: 135 (13.5%)\n",
      "3-hop questions: 698 (69.9%)\n",
      "4-hop questions: 166 (16.6%)\n",
      "Running Qwen/Qwen3-0.6B on 999 questions...\n",
      "Loading Qwen/Qwen3-0.6B on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Generating predictions:  11%|         | 106/999 [21:59<3:04:04, 12.37s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# --- DATA ---\n",
    "def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        import urllib.request\n",
    "        print(f\"Downloading MenatQA to {file_path}...\")\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "            str(file_path)\n",
    "        )\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "    return data\n",
    "\n",
    "def extract_reasoning_hops(example):\n",
    "    question = example.get('question', '')\n",
    "    answer = example.get('answer', '')\n",
    "    q_type = example.get('type', '')\n",
    "    time_scope = example.get('time_scope', '')\n",
    "    \n",
    "    sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "    clauses = len([c for c in re.split(r'and|or|but|because|when|if', question.lower()) if c.strip()])\n",
    "    capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "    complexity_score = 1\n",
    "    complexity_score += min(1, len(sentences) - 1)\n",
    "    complexity_score += min(1, (clauses - 1) // 2)\n",
    "    complexity_score += min(1, capitalized_words // 3)\n",
    "    if time_scope:\n",
    "        complexity_score += 1\n",
    "    complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "    hops = []\n",
    "    hops.append(f\"Understand the question: {question}\")\n",
    "    if complexity_score >= 2:\n",
    "        if time_scope:\n",
    "            hops.append(f\"Identify time context: {time_scope}\")\n",
    "        else:\n",
    "            hops.append(f\"Recognize question type: {q_type}\")\n",
    "    if complexity_score >= 3:\n",
    "        hops.append(\"Retrieve relevant facts and information\")\n",
    "    if complexity_score >= 4:\n",
    "        hops.append(\"Analyze relationships between facts\")\n",
    "    hops.append(f\"Formulate answer: {answer}\")\n",
    "    \n",
    "    return hops, complexity_score\n",
    "\n",
    "def preprocess_dataset(data):\n",
    "    processed_data = []\n",
    "    hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "    for item in data:\n",
    "        gold_hops, complexity_score = extract_reasoning_hops(item)\n",
    "        hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "        entry = {\n",
    "            'ID': item.get('ID', ''),\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'type': item.get('type', ''),\n",
    "            'time_scope': item.get('time_scope', ''),\n",
    "            'gold_hops': gold_hops,\n",
    "            'hop_count': complexity_score,\n",
    "            'model_prediction': '',\n",
    "            'model_reasoning_steps': []\n",
    "        }\n",
    "        processed_data.append(entry)\n",
    "    \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df['hop_category'] = pd.cut(\n",
    "        df['hop_count'],\n",
    "        bins=[-1, 1, 2, 3, float('inf')],\n",
    "        labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "    for i in range(1, 5):\n",
    "        print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- MODEL ---\n",
    "def run_qwen_predictions(df, model_name=\"Qwen/Qwen2-0.5B\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading {model_name} on {device}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    \n",
    "    pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=0 if device == 'cuda' else -1,\n",
    "    max_new_tokens=400,  # Increased from 256\n",
    "    temperature=0.3,     # Increased from 0.01\n",
    "    do_sample=True,\n",
    "    top_p=0.9,          # Added for better generation\n",
    "    repetition_penalty=1.1  # Added to avoid repetition\n",
    ")\n",
    "\n",
    "    def format_cot_prompt(question):\n",
    "        return f\"\"\"You are an expert at answering questions step by step. Follow this format exactly:\n",
    "\n",
    "Step 1: [Understand what the question is asking]\n",
    "Step 2: [Identify key information needed]\n",
    "Step 3: [Reason through the problem]\n",
    "Final Answer: [Give the direct answer]\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Let me solve this step by step:\"\"\"\n",
    "\n",
    "    all_preds, all_steps = [], []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating predictions\"):\n",
    "        prompt = format_cot_prompt(row[\"question\"])\n",
    "        generation = pipe(prompt)[0]['generated_text']\n",
    "        output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "        \n",
    "        # Better parsing\n",
    "        lines = [x.strip() for x in output.split('\\n') if x.strip()]\n",
    "        steps = []\n",
    "        model_answer = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.lower().startswith('final answer:') or line.lower().startswith('answer:'):\n",
    "                model_answer = line.split(':', 1)[-1].strip()\n",
    "                steps.append(line)\n",
    "            elif line.lower().startswith('step') or line.lower().startswith('1.') or line.lower().startswith('2.'):\n",
    "                steps.append(line)\n",
    "            elif not model_answer and line:  # If no explicit answer found, use last meaningful line\n",
    "                model_answer = line\n",
    "                \n",
    "        if not steps:\n",
    "            steps = [output.strip()] if output.strip() else [\"No reasoning provided\"]\n",
    "        if not model_answer:\n",
    "            model_answer = steps[-1] if steps else \"No answer\"\n",
    "            \n",
    "        all_preds.append(model_answer)\n",
    "        all_steps.append(steps)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['model_prediction'] = all_preds\n",
    "    df['model_reasoning_steps'] = all_steps\n",
    "    return df\n",
    "\n",
    "# --- METRICS ---\n",
    "def compute_exact_match(prediction, gold):\n",
    "    pred_clean = re.sub(r'[^\\w\\s]', '', str(prediction).lower().strip())\n",
    "    gold_clean = re.sub(r'[^\\w\\s]', '', str(gold).lower().strip())\n",
    "    \n",
    "    # Direct match\n",
    "    if pred_clean == gold_clean:\n",
    "        return 1\n",
    "    \n",
    "    # Check if gold is contained in prediction\n",
    "    if gold_clean in pred_clean:\n",
    "        return 1\n",
    "        \n",
    "    # Check if prediction is contained in gold\n",
    "    if pred_clean in gold_clean:\n",
    "        return 1\n",
    "        \n",
    "    # Token-level similarity for partial credit\n",
    "    pred_tokens = set(pred_clean.split())\n",
    "    gold_tokens = set(gold_clean.split())\n",
    "    if pred_tokens and gold_tokens:\n",
    "        jaccard = len(pred_tokens & gold_tokens) / len(pred_tokens | gold_tokens)\n",
    "        return 1 if jaccard > 0.8 else 0\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def compute_f1(prediction, gold):\n",
    "    pred_tokens = str(prediction).lower().split()\n",
    "    gold_tokens = str(gold).lower().split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    prec = len(common) / len(pred_tokens)\n",
    "    rec = len(common) / len(gold_tokens)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "def compute_bleu(prediction, gold):\n",
    "    pred_tokens = str(prediction).lower().split()\n",
    "    gold_tokens = str(gold).lower().split()\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    reference = [gold_tokens]\n",
    "    candidate = pred_tokens\n",
    "    weights = (0.25, 0.25, 0.25, 0.25)\n",
    "    chencherry = SmoothingFunction()\n",
    "    try:\n",
    "        bleu_score = sentence_bleu(reference, candidate, weights=weights, smoothing_function=chencherry.method1)\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    return bleu_score\n",
    "\n",
    "def evaluate_hop_wise_accuracy(df):\n",
    "    categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "    hop_results = {}\n",
    "    overall_em, overall_f1, overall_bleu, overall_count = 0, 0, 0, 0\n",
    "    \n",
    "    for cat in categories:\n",
    "        subdf = df[df['hop_category'] == cat]\n",
    "        if subdf.empty:\n",
    "            hop_results[str(cat)] = {'count': 0, 'EM': 0, 'F1': 0, 'BLEU': 0}\n",
    "            continue\n",
    "\n",
    "        ems = [compute_exact_match(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "        f1s = [compute_f1(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "        bleus = [compute_bleu(row['model_prediction'], row['answer']) for _, row in subdf.iterrows()]\n",
    "        \n",
    "        count = len(subdf)\n",
    "        mean_em = np.mean(ems) if ems else 0\n",
    "        mean_f1 = np.mean(f1s) if f1s else 0\n",
    "        mean_bleu = np.mean(bleus) if bleus else 0\n",
    "        \n",
    "        hop_results[str(cat)] = {'count': count, 'EM': mean_em, 'F1': mean_f1, 'BLEU': mean_bleu}\n",
    "        \n",
    "        overall_em += sum(ems)\n",
    "        overall_f1 += sum(f1s)\n",
    "        overall_bleu += sum(bleus)\n",
    "        overall_count += count\n",
    "        \n",
    "    hop_results['overall'] = {\n",
    "        'count': overall_count,\n",
    "        'EM': overall_em / overall_count if overall_count else 0,\n",
    "        'F1': overall_f1 / overall_count if overall_count else 0,\n",
    "        'BLEU': overall_bleu / overall_count if overall_count else 0,\n",
    "    }\n",
    "    return hop_results\n",
    "\n",
    "def evaluate_step_verification(df):\n",
    "    all_step_matches = []\n",
    "    complete_path_matches = 0\n",
    "    partial_path_matches = 0\n",
    "    total_questions = 0\n",
    "    sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "    hop_category_results = defaultdict(lambda: {'matches': 0, 'partial_matches': 0, 'total': 0})\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        gold_hops = row['gold_hops']\n",
    "        model_steps = row['model_reasoning_steps']\n",
    "        \n",
    "        if not gold_hops or not model_steps or not model_steps[0]:\n",
    "            continue\n",
    "            \n",
    "        total_questions += 1\n",
    "        hop_cat = row['hop_category']\n",
    "        if hop_cat is None: continue\n",
    "\n",
    "        hop_category_results[hop_cat]['total'] += 1\n",
    "        \n",
    "        step_matches = []\n",
    "        first_match = 0\n",
    "        last_match = 0\n",
    "        middle_matches = []\n",
    "\n",
    "        # First step\n",
    "        gold_q_terms = set(re.findall(r'\\w+', gold_hops[0].lower()))\n",
    "        model_q_terms = set(re.findall(r'\\w+', model_steps[0].lower()))\n",
    "        if gold_q_terms:\n",
    "            common_terms = gold_q_terms & model_q_terms\n",
    "            first_match = len(common_terms) / len(gold_q_terms)\n",
    "        step_matches.append(first_match)\n",
    "\n",
    "        # Last step\n",
    "        if len(gold_hops) > 1 and len(model_steps) > 1:\n",
    "            gold_ans_text = row['answer'].lower()\n",
    "            model_last_hop_text = model_steps[-1].lower()\n",
    "            last_match = 1.0 if gold_ans_text in model_last_hop_text else 0.0\n",
    "            step_matches.append(last_match)\n",
    "\n",
    "        # Intermediate steps\n",
    "        if len(gold_hops) > 2 and len(model_steps) > 2:\n",
    "            num_intermediate_to_compare = min(len(gold_hops) - 2, len(model_steps) - 2)\n",
    "            for i in range(num_intermediate_to_compare):\n",
    "                gold_mid_idx = i + 1\n",
    "                model_mid_idx = i + 1\n",
    "                \n",
    "                gold_mid = gold_hops[gold_mid_idx].lower()\n",
    "                model_mid = model_steps[model_mid_idx].lower()\n",
    "                \n",
    "                gold_words = set(re.findall(r'\\w+', gold_mid))\n",
    "                model_words = set(re.findall(r'\\w+', model_mid))\n",
    "                \n",
    "                if gold_words and model_words:\n",
    "                    common_words = gold_words & model_words\n",
    "                    similarity = len(common_words) / max(len(gold_words), len(model_words))\n",
    "                    middle_matches.append(similarity)\n",
    "                    step_matches.append(similarity)\n",
    "                elif gold_words or model_words:\n",
    "                    middle_matches.append(0.0)\n",
    "                    step_matches.append(0.0)\n",
    "\n",
    "        all_step_matches.extend(step_matches)\n",
    "        \n",
    "        if step_matches:\n",
    "            is_complete_match = all(match >= 0.6 for match in step_matches)\n",
    "            is_partial_match = False\n",
    "            if not is_complete_match:\n",
    "                avg_middle_match = sum(middle_matches) / len(middle_matches) if middle_matches else 1.0\n",
    "                if first_match >= 0.7 and last_match >= 0.7 and avg_middle_match >= 0.4:\n",
    "                    is_partial_match = True\n",
    "            \n",
    "            if is_complete_match:\n",
    "                complete_path_matches += 1\n",
    "                hop_category_results[hop_cat]['matches'] += 1\n",
    "            elif is_partial_match:\n",
    "                partial_path_matches += 1\n",
    "                hop_category_results[hop_cat]['partial_matches'] += 1\n",
    "\n",
    "    cot_em_overall = complete_path_matches / total_questions if total_questions > 0 else 0\n",
    "    cot_partial_overall = (complete_path_matches + partial_path_matches) / total_questions if total_questions > 0 else 0\n",
    "    \n",
    "    cot_em_by_category = {}\n",
    "    cot_partial_by_category = {}\n",
    "\n",
    "    for hop_cat_key in sorted_categories:\n",
    "        data = hop_category_results[hop_cat_key]\n",
    "        cat_total = data['total']\n",
    "        cot_em = data['matches'] / cat_total if cat_total > 0 else 0\n",
    "        cot_partial = (data['matches'] + data['partial_matches']) / cat_total if cat_total > 0 else 0\n",
    "        cot_em_by_category[str(hop_cat_key)] = cot_em\n",
    "        cot_partial_by_category[str(hop_cat_key)] = cot_partial\n",
    "        \n",
    "    results = {\n",
    "        'CoT-EM': cot_em_overall,\n",
    "        'CoT-Partial': cot_partial_overall,\n",
    "        'Hop-Match-Rate': np.mean(all_step_matches) if all_step_matches else 0,\n",
    "        'CoT-EM by Category': cot_em_by_category,\n",
    "        'CoT-Partial by Category': cot_partial_by_category,\n",
    "        'Total Questions Evaluated for Step Verification': total_questions\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def compute_cot_hop_em(df):\n",
    "    exact_matches = 0\n",
    "    total = 0\n",
    "    for _, row in df.iterrows():\n",
    "        gold_hops = row.get('gold_hops', [])\n",
    "        pred_hops = row.get('model_reasoning_steps', [])\n",
    "        \n",
    "        if not gold_hops or not pred_hops or not pred_hops[0]:\n",
    "            continue\n",
    "            \n",
    "        total += 1\n",
    "        if len(pred_hops) == len(gold_hops) and \\\n",
    "           all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "            exact_matches += 1\n",
    "            \n",
    "    return exact_matches / total if total > 0 else 0.0\n",
    "\n",
    "def compute_cot_hop_em_by_category(df):\n",
    "    results = {}\n",
    "    sorted_categories = sorted([cat for cat in df['hop_category'].unique() if cat is not None], key=lambda x: x[0])\n",
    "    \n",
    "    for cat in sorted_categories:\n",
    "        subdf = df[df['hop_category'] == cat]\n",
    "        if subdf.empty:\n",
    "            results[str(cat)] = 0.0\n",
    "            continue\n",
    "            \n",
    "        exact_matches = 0\n",
    "        total = 0\n",
    "        for _, row in subdf.iterrows():\n",
    "            gold_hops = row.get('gold_hops', [])\n",
    "            pred_hops = row.get('model_reasoning_steps', [])\n",
    "            if not gold_hops or not pred_hops or not pred_hops[0]: \n",
    "                continue\n",
    "            total += 1\n",
    "            if len(pred_hops) == len(gold_hops) and \\\n",
    "               all(g.strip().lower() == p.strip().lower() for g, p in zip(gold_hops, pred_hops)):\n",
    "                exact_matches += 1\n",
    "                \n",
    "        results[str(cat)] = exact_matches / total if total > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "def visualize_hop_wise_results(hop_results, step_results, strict_cot_em_overall, cot_em_per_cat_strict, save_path=None):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    categories = [cat for cat in hop_results.keys() if cat != 'overall']\n",
    "    counts = [hop_results[cat]['count'] for cat in categories]\n",
    "    em_scores = [hop_results[cat]['EM'] for cat in categories]\n",
    "    f1_scores = [hop_results[cat]['F1'] for cat in categories]\n",
    "    bleu_scores = [hop_results[cat]['BLEU'] for cat in categories]\n",
    "    \n",
    "    soft_cot_em_by_cat = step_results.get('CoT-EM by Category', {})\n",
    "    soft_cot_em_scores = [soft_cot_em_by_cat.get(cat, 0) for cat in categories]\n",
    "    strict_cot_em_scores = [cot_em_per_cat_strict.get(cat, 0) for cat in categories]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.15\n",
    "    \n",
    "    plt.bar(x - width*2, em_scores, width, label='Exact Match', color='#3498db', alpha=0.8)\n",
    "    plt.bar(x - width, f1_scores, width, label='F1 Score', color='#2ecc71', alpha=0.8)\n",
    "    plt.bar(x, bleu_scores, width, label='BLEU Score', color='#f39c12', alpha=0.8)\n",
    "    plt.bar(x + width, soft_cot_em_scores, width, label='Soft CoT-EM', color='#e74c3c', alpha=0.8)\n",
    "    plt.bar(x + width*2, strict_cot_em_scores, width, label='Strict CoT-EM', color='#8e44ad', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Reasoning Complexity', fontweight='bold')\n",
    "    plt.ylabel('Score', fontweight='bold')\n",
    "    plt.title('Qwen Performance on MenatQA by Reasoning Complexity', fontweight='bold', pad=20)\n",
    "    plt.xticks(x, categories)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, count in enumerate(counts):\n",
    "        if count > 0:\n",
    "            plt.text(i, 0.02, f'n={count}', ha='center', va='bottom', fontsize=9,\n",
    "                    bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.2'))\n",
    "    \n",
    "    overall = hop_results['overall']\n",
    "    overall_text = (f\"Overall Results (n={overall['count']}):\\n\"\n",
    "                   f\"EM: {overall['EM']:.3f} | F1: {overall['F1']:.3f} | BLEU: {overall['BLEU']:.3f}\\n\"\n",
    "                   f\"Soft CoT-EM: {step_results['CoT-EM']:.3f} | Strict CoT-EM: {strict_cot_em_overall:.3f}\")\n",
    "    \n",
    "    plt.figtext(0.5, 0.02, overall_text, ha='center', fontsize=11, fontweight='bold',\n",
    "                bbox=dict(facecolor='lightgray', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.96])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    DATASET_FILE = './MenatQA.json'\n",
    "    MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "    OUTPUT_IMAGE_PATH = './menatqa_qwen_performance.png'\n",
    "    OUTPUT_RESULTS_PATH = './menatqa_qwen_results.json'\n",
    "\n",
    "    print(\"=== MenatQA Evaluation on Qwen ===\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading MenatQA dataset...\")\n",
    "    data = load_menatqa_dataset(DATASET_FILE)\n",
    "    \n",
    "    # Preprocess\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    df = preprocess_dataset(data)\n",
    "    \n",
    "    # Run model\n",
    "    print(f\"Running {MODEL_NAME} on {len(df)} questions...\")\n",
    "    df_results = run_qwen_predictions(df, model_name=MODEL_NAME)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"Evaluating results...\")\n",
    "    hop_results = evaluate_hop_wise_accuracy(df_results)\n",
    "    step_verification_results = evaluate_step_verification(df_results)\n",
    "    strict_cot_em_overall = compute_cot_hop_em(df_results)\n",
    "    strict_cot_em_by_category = compute_cot_hop_em_by_category(df_results)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n--- Answer Accuracy by Complexity ---\")\n",
    "    for category, metrics in hop_results.items():\n",
    "        if category == 'overall': continue\n",
    "        print(f\"{category}: EM={metrics['EM']:.3f}, F1={metrics['F1']:.3f}, BLEU={metrics['BLEU']:.3f} (n={metrics['count']})\")\n",
    "    \n",
    "    overall = hop_results['overall']\n",
    "    print(f\"Overall: EM={overall['EM']:.3f}, F1={overall['F1']:.3f}, BLEU={overall['BLEU']:.3f} (n={overall['count']})\")\n",
    "    \n",
    "    print(\"\\n--- Reasoning Path Verification ---\")\n",
    "    print(f\"Soft CoT-EM: {step_verification_results['CoT-EM']:.3f}\")\n",
    "    print(f\"CoT-Partial: {step_verification_results['CoT-Partial']:.3f}\")\n",
    "    print(f\"Hop-Match-Rate: {step_verification_results['Hop-Match-Rate']:.3f}\")\n",
    "    print(f\"Strict CoT-EM: {strict_cot_em_overall:.3f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\nGenerating visualization...\")\n",
    "    visualize_hop_wise_results(\n",
    "        hop_results, \n",
    "        step_verification_results, \n",
    "        strict_cot_em_overall, \n",
    "        strict_cot_em_by_category, \n",
    "        save_path=OUTPUT_IMAGE_PATH\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    summary_results = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dataset_info': {\n",
    "            'name': 'MenatQA',\n",
    "            'total_questions_processed': len(df_results),\n",
    "            'hop_distribution': df_results['hop_category'].value_counts().to_dict()\n",
    "        },\n",
    "        'answer_accuracy_metrics': hop_results,\n",
    "        'reasoning_path_verification_metrics': step_verification_results,\n",
    "        'strict_cot_em_overall': float(strict_cot_em_overall),\n",
    "        'strict_cot_em_by_category': {k: float(v) for k, v in strict_cot_em_by_category.items()}\n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_RESULTS_PATH, 'w') as f:\n",
    "        json.dump(summary_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nEvaluation complete!\")\n",
    "    print(f\"Results saved to: {OUTPUT_RESULTS_PATH}\")\n",
    "    print(f\"Visualization saved to: {OUTPUT_IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8bcce-f746-4229-95a6-9f3a056a3c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e47fcd-a014-4c37-9491-29242f4695e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cbb128-a13d-43fa-8e5a-3cb5ca99e50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f12e3-a8e5-40ac-accf-919bfc9b57f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1baa454-39af-4438-9709-6f2539cba824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b93ddd-6225-41d8-bb07-737f4fa8e825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
