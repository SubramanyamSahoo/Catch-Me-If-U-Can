{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242436b6-7341-4242-88a9-a9a447454180",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3313/2371374877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import random\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Tuple, Any, Set\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# --- DATASET LOADING ---\n",
    "def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "    \"\"\"Load MenatQA dataset with automatic download.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        import urllib.request\n",
    "        print(f\"Downloading MenatQA to {file_path}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "                str(file_path)\n",
    "            )\n",
    "            print(\"Download completed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "    return data\n",
    "\n",
    "def extract_reasoning_hops(example):\n",
    "    \"\"\"Extract reasoning complexity for hop-wise analysis.\"\"\"\n",
    "    question = example.get('question', '')\n",
    "    answer = example.get('answer', '')\n",
    "    q_type = example.get('type', '')\n",
    "    time_scope = example.get('time_scope', '')\n",
    "    \n",
    "    sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "    clauses = len([c for c in re.split(r'and|or|but|because|when|if', question.lower()) if c.strip()])\n",
    "    capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "    complexity_score = 1\n",
    "    complexity_score += min(1, len(sentences) - 1)\n",
    "    complexity_score += min(1, (clauses - 1) // 2)\n",
    "    complexity_score += min(1, capitalized_words // 3)\n",
    "    if time_scope:\n",
    "        complexity_score += 1\n",
    "    complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "    hops = []\n",
    "    hops.append(f\"Understand the question: {question}\")\n",
    "    if complexity_score >= 2:\n",
    "        if time_scope:\n",
    "            hops.append(f\"Identify time context: {time_scope}\")\n",
    "        else:\n",
    "            hops.append(f\"Recognize question type: {q_type}\")\n",
    "    if complexity_score >= 3:\n",
    "        hops.append(\"Retrieve relevant facts and information\")\n",
    "    if complexity_score >= 4:\n",
    "        hops.append(\"Analyze relationships between facts\")\n",
    "    hops.append(f\"Formulate answer: {answer}\")\n",
    "    \n",
    "    return hops, complexity_score\n",
    "\n",
    "def preprocess_dataset(data):\n",
    "    \"\"\"Preprocess MenatQA dataset for comprehensive evaluation.\"\"\"\n",
    "    processed_data = []\n",
    "    hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "    for item in data:\n",
    "        gold_hops, complexity_score = extract_reasoning_hops(item)\n",
    "        hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "        entry = {\n",
    "            'ID': item.get('ID', ''),\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'type': item.get('type', ''),\n",
    "            'time_scope': item.get('time_scope', ''),\n",
    "            'gold_hops': gold_hops,\n",
    "            'hop_count': complexity_score,\n",
    "            'model_prediction': '',\n",
    "            'model_explanation': '',\n",
    "            'model_reasoning_steps': [],\n",
    "            'faithfulness_score': 0.0,\n",
    "            'plausibility_score': 0.0,\n",
    "            'hallucination_indicator': False\n",
    "        }\n",
    "        processed_data.append(entry)\n",
    "    \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df['hop_category'] = pd.cut(\n",
    "        df['hop_count'],\n",
    "        bins=[-1, 1, 2, 3, float('inf')],\n",
    "        labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "    for i in range(1, 5):\n",
    "        print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- QWEN MODEL SETUP ---\n",
    "class QwenExplanationModel:\n",
    "    \"\"\"Qwen model for explanation generation and evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Qwen/Qwen2-0.5B\", device=\"auto\"):\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Try Qwen3-0.6B first, fallback to available models\n",
    "        models_to_try = [\n",
    "            \"Qwen/Qwen3-0.6B\",\n",
    "            \"Qwen/Qwen2-0.5B\", \n",
    "            \"Qwen/Qwen2-1.5B\",\n",
    "            model_name\n",
    "        ]\n",
    "        \n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        for model_to_try in models_to_try:\n",
    "            try:\n",
    "                print(f\"Attempting to load {model_to_try}...\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_to_try, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_to_try,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                    device_map=device\n",
    "                )\n",
    "                self.model_name = model_to_try\n",
    "                print(f\"Successfully loaded {model_to_try}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {model_to_try}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise Exception(\"Failed to load any Qwen model\")\n",
    "        \n",
    "        # Add padding token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=device,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Load semantic similarity model for faithfulness evaluation\n",
    "        print(\"Loading semantic similarity model...\")\n",
    "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def generate_explanation(self, question: str, max_length: int = 512) -> Tuple[str, str, List[str]]:\n",
    "        \"\"\"Generate answer with detailed explanation.\"\"\"\n",
    "        prompt = f\"\"\"Answer the following question and provide a detailed explanation of your reasoning process.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please structure your response as:\n",
    "1. Step-by-step reasoning (show each logical step)\n",
    "2. Final answer\n",
    "\n",
    "Your explanation should be clear and show exactly how you arrived at the answer.\n",
    "\n",
    "Reasoning steps:\"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.generator(\n",
    "                prompt,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated_text = result[0]['generated_text']\n",
    "            response = generated_text[len(prompt):].strip()\n",
    "            \n",
    "            # Parse reasoning steps and final answer\n",
    "            lines = response.split('\\n')\n",
    "            reasoning_steps = []\n",
    "            final_answer = \"\"\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if re.match(r'^\\d+\\.', line) or line.startswith('-') or line.startswith('‚Ä¢'):\n",
    "                    reasoning_steps.append(re.sub(r'^\\d+\\.|-|‚Ä¢', '', line).strip())\n",
    "                elif line.startswith('Answer:') or line.startswith('Final answer:'):\n",
    "                    final_answer = re.sub(r'^(Answer|Final answer):', '', line, flags=re.IGNORECASE).strip()\n",
    "                elif line and not final_answer and reasoning_steps:\n",
    "                    # Likely the final answer without explicit label\n",
    "                    final_answer = line\n",
    "            \n",
    "            if not final_answer and reasoning_steps:\n",
    "                final_answer = reasoning_steps[-1]\n",
    "            \n",
    "            if not final_answer:\n",
    "                final_answer = response.split('\\n')[-1] if response else \"No answer generated\"\n",
    "            \n",
    "            full_explanation = '\\n'.join(reasoning_steps) if reasoning_steps else response\n",
    "            \n",
    "            return final_answer, full_explanation, reasoning_steps\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating explanation: {e}\")\n",
    "            return \"Error in generation\", \"Error in explanation\", []\n",
    "\n",
    "# --- FAITHFULNESS EVALUATION ---\n",
    "def evaluate_faithfulness(explanation: str, reasoning_steps: List[str], \n",
    "                         question: str, answer: str, semantic_model: SentenceTransformer) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate faithfulness: Does the explanation truly reflect the computation that produced the answer?\n",
    "    Score: 1-5 (1=completely unfaithful, 5=perfectly faithful)\n",
    "    \"\"\"\n",
    "    if not explanation or not reasoning_steps:\n",
    "        return 1.0\n",
    "    \n",
    "    faithfulness_score = 0.0\n",
    "    criteria_count = 0\n",
    "    \n",
    "    # Criterion 1: Logical flow consistency (25%)\n",
    "    if len(reasoning_steps) > 1:\n",
    "        flow_consistency = 0.0\n",
    "        for i in range(len(reasoning_steps) - 1):\n",
    "            current_step = reasoning_steps[i]\n",
    "            next_step = reasoning_steps[i + 1]\n",
    "            \n",
    "            # Check if steps logically connect\n",
    "            similarity = cosine_similarity(\n",
    "                semantic_model.encode([current_step]).reshape(1, -1),\n",
    "                semantic_model.encode([next_step]).reshape(1, -1)\n",
    "            )[0][0]\n",
    "            \n",
    "            # Check for logical connectors\n",
    "            logical_words = ['therefore', 'thus', 'because', 'since', 'so', 'hence', 'as a result']\n",
    "            has_connectors = any(word in next_step.lower() for word in logical_words)\n",
    "            \n",
    "            step_score = (similarity * 0.7) + (0.3 if has_connectors else 0)\n",
    "            flow_consistency += step_score\n",
    "        \n",
    "        flow_consistency /= (len(reasoning_steps) - 1)\n",
    "        faithfulness_score += flow_consistency * 1.25  # 25% weight\n",
    "        criteria_count += 1\n",
    "    \n",
    "    # Criterion 2: Question-answer alignment (25%)\n",
    "    question_answer_alignment = 0.0\n",
    "    \n",
    "    # Check if explanation addresses the question\n",
    "    question_similarity = cosine_similarity(\n",
    "        semantic_model.encode([explanation]).reshape(1, -1),\n",
    "        semantic_model.encode([question]).reshape(1, -1)\n",
    "    )[0][0]\n",
    "    \n",
    "    # Check if explanation leads to the answer\n",
    "    answer_similarity = cosine_similarity(\n",
    "        semantic_model.encode([explanation]).reshape(1, -1),\n",
    "        semantic_model.encode([answer]).reshape(1, -1)\n",
    "    )[0][0]\n",
    "    \n",
    "    question_answer_alignment = (question_similarity + answer_similarity) / 2\n",
    "    faithfulness_score += question_answer_alignment * 1.25  # 25% weight\n",
    "    criteria_count += 1\n",
    "    \n",
    "    # Criterion 3: Step necessity (25%)\n",
    "    step_necessity = 0.0\n",
    "    if reasoning_steps:\n",
    "        necessary_steps = 0\n",
    "        for step in reasoning_steps:\n",
    "            # Check if step is relevant to question or contributes to answer\n",
    "            step_relevance = max(\n",
    "                cosine_similarity(\n",
    "                    semantic_model.encode([step]).reshape(1, -1),\n",
    "                    semantic_model.encode([question]).reshape(1, -1)\n",
    "                )[0][0],\n",
    "                cosine_similarity(\n",
    "                    semantic_model.encode([step]).reshape(1, -1),\n",
    "                    semantic_model.encode([answer]).reshape(1, -1)\n",
    "                )[0][0]\n",
    "            )\n",
    "            \n",
    "            if step_relevance > 0.3:  # Threshold for relevance\n",
    "                necessary_steps += 1\n",
    "        \n",
    "        step_necessity = necessary_steps / len(reasoning_steps) if reasoning_steps else 0\n",
    "        faithfulness_score += step_necessity * 1.25  # 25% weight\n",
    "        criteria_count += 1\n",
    "    \n",
    "    # Criterion 4: Factual consistency (25%)\n",
    "    factual_consistency = 0.0\n",
    "    \n",
    "    # Check for contradictions within explanation\n",
    "    if len(reasoning_steps) > 1:\n",
    "        contradiction_penalty = 0\n",
    "        for i, step1 in enumerate(reasoning_steps):\n",
    "            for j, step2 in enumerate(reasoning_steps[i+1:], i+1):\n",
    "                # Simple contradiction detection using negation patterns\n",
    "                negation_words = ['not', 'no', 'never', 'none', 'neither', 'nor']\n",
    "                step1_words = set(step1.lower().split())\n",
    "                step2_words = set(step2.lower().split())\n",
    "                \n",
    "                # Check for direct contradictions\n",
    "                if any(word in step1_words and f\"not {word}\" in step2.lower() for word in step1_words):\n",
    "                    contradiction_penalty += 0.1\n",
    "                elif any(word in step2_words and f\"not {word}\" in step1.lower() for word in step2_words):\n",
    "                    contradiction_penalty += 0.1\n",
    "        \n",
    "        factual_consistency = max(0, 1.0 - contradiction_penalty)\n",
    "    else:\n",
    "        factual_consistency = 1.0\n",
    "    \n",
    "    faithfulness_score += factual_consistency * 1.25  # 25% weight\n",
    "    criteria_count += 1\n",
    "    \n",
    "    # Normalize to 1-5 scale\n",
    "    if criteria_count > 0:\n",
    "        normalized_score = (faithfulness_score / criteria_count) * 4 + 1  # Convert to 1-5 scale\n",
    "        return min(5.0, max(1.0, normalized_score))\n",
    "    \n",
    "    return 1.0\n",
    "\n",
    "def evaluate_plausibility(explanation: str, reasoning_steps: List[str], \n",
    "                         question: str, semantic_model: SentenceTransformer) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate plausibility: Is the explanation convincing, regardless of correctness?\n",
    "    Score: 1-5 (1=completely implausible, 5=very convincing)\n",
    "    \"\"\"\n",
    "    if not explanation or not reasoning_steps:\n",
    "        return 1.0\n",
    "    \n",
    "    plausibility_score = 0.0\n",
    "    criteria_count = 0\n",
    "    \n",
    "    # Criterion 1: Clarity and coherence (30%)\n",
    "    clarity_score = 0.0\n",
    "    \n",
    "    # Check explanation length (not too short, not too verbose)\n",
    "    explanation_length = len(explanation.split())\n",
    "    length_score = 1.0\n",
    "    if explanation_length < 10:\n",
    "        length_score = 0.5  # Too short\n",
    "    elif explanation_length > 200:\n",
    "        length_score = 0.7  # Too verbose\n",
    "    \n",
    "    # Check for clear structure\n",
    "    structure_indicators = ['first', 'second', 'then', 'next', 'finally', 'therefore']\n",
    "    structure_score = min(1.0, sum(1 for indicator in structure_indicators \n",
    "                                 if indicator in explanation.lower()) * 0.2)\n",
    "    \n",
    "    clarity_score = (length_score + structure_score) / 2\n",
    "    plausibility_score += clarity_score * 1.5  # 30% weight\n",
    "    criteria_count += 1\n",
    "    \n",
    "        # Criterion 2: Logical flow and transitions (25%)\n",
    "    flow_score = 0.0\n",
    "    if len(reasoning_steps) > 1:\n",
    "        # Check for smooth transitions between steps\n",
    "        transition_quality = 0.0\n",
    "        transition_words = ['because', 'therefore', 'thus', 'since', 'so', 'hence', \n",
    "                          'as a result', 'consequently', 'furthermore', 'moreover', 'however']\n",
    "        \n",
    "        for i in range(len(reasoning_steps) - 1):\n",
    "            current_step = reasoning_steps[i]\n",
    "            next_step = reasoning_steps[i + 1]\n",
    "            \n",
    "            # Check for explicit transition words\n",
    "            has_transitions = any(word in next_step.lower() for word in transition_words)\n",
    "            \n",
    "            # Check semantic continuity\n",
    "            semantic_continuity = cosine_similarity(\n",
    "                semantic_model.encode([current_step]).reshape(1, -1),\n",
    "                semantic_model.encode([next_step]).reshape(1, -1)\n",
    "            )[0][0]\n",
    "            \n",
    "            step_flow = (0.4 if has_transitions else 0) + (semantic_continuity * 0.6)\n",
    "            transition_quality += step_flow\n",
    "        \n",
    "        flow_score = transition_quality / (len(reasoning_steps) - 1)\n",
    "    else:\n",
    "        flow_score = 0.8  # Single step gets moderate score\n",
    "    \n",
    "    plausibility_score += flow_score * 1.25  # 25% weight\n",
    "    criteria_count += 1\n",
    "    \n",
    "    # Criterion 3: Domain knowledge appropriateness (25%)\n",
    "    domain_score = 0.0\n",
    "    \n",
    "    # Check for domain-specific terms (Middle Eastern/North African context)\n",
    "    domain_terms = ['middle east', 'north africa', 'mena', 'arab', 'islamic', 'ottoman', \n",
    "                   'persian', 'mediterranean', 'gulf', 'levant', 'maghreb', 'arabia']\n",
    "    \n",
    "    explanation_lower = explanation.lower()\n",
    "    domain_relevance = sum(1 for term in domain_terms if term in explanation_lower)\n",
    "    domain_score = min(1.0, domain_relevance * 0.3)\n",
    "    \n",
    "    # Check for geographical/historical accuracy patterns\n",
    "    geo_patterns = ['capital', 'country', 'city', 'region', 'border', 'located', 'empire', 'dynasty']\n",
    "    pattern_usage = sum(1 for pattern in geo_patterns if pattern in explanation_lower)\n",
    "    pattern_score = min(1.0, pattern_usage * 0.2)\n",
    "    \n",
    "    domain_score = (domain_score + pattern_score) / 2\n",
    "    plausibility_score += domain_score * 1.25  # 25% weight\n",
    "    criteria_count += 1\n",
    "    \n",
    "    # Criterion 4: Confidence and assertiveness (20%)\n",
    "    confidence_score = 0.0\n",
    "    \n",
    "    # Check for confidence indicators\n",
    "    confident_phrases = ['clearly', 'obviously', 'definitely', 'certainly', 'undoubtedly']\n",
    "    uncertain_phrases = ['maybe', 'perhaps', 'possibly', 'might', 'could be', 'unclear']\n",
    "    \n",
    "    confident_count = sum(1 for phrase in confident_phrases if phrase in explanation_lower)\n",
    "    uncertain_count = sum(1 for phrase in uncertain_phrases if phrase in explanation_lower)\n",
    "    \n",
    "    # Balanced confidence is good (not overconfident, not too uncertain)\n",
    "    if confident_count > 0 and uncertain_count == 0:\n",
    "        confidence_score = 0.9\n",
    "    elif confident_count == 0 and uncertain_count > 2:\n",
    "        confidence_score = 0.3\n",
    "    elif confident_count <= 2 and uncertain_count <= 1:\n",
    "        confidence_score = 0.8\n",
    "    else:\n",
    "        confidence_score = 0.6\n",
    "    \n",
    "    plausibility_score += confidence_score * 1.0  # 20% weight\n",
    "    criteria_count += 1\n",
    "    \n",
    "    # Normalize to 1-5 scale\n",
    "    if criteria_count > 0:\n",
    "        normalized_score = (plausibility_score / criteria_count) * 4 + 1\n",
    "        return min(5.0, max(1.0, normalized_score))\n",
    "    \n",
    "    return 1.0\n",
    "\n",
    "def detect_hallucination(faithfulness_score: float, plausibility_score: float) -> bool:\n",
    "    \"\"\"\n",
    "    Detect hallucinated reasoning: High plausibility + Low faithfulness\n",
    "    \"\"\"\n",
    "    # Hallucination threshold: plausible but not faithful\n",
    "    plausibility_threshold = 3.5  # Reasonably convincing\n",
    "    faithfulness_threshold = 2.5  # Poor faithfulness\n",
    "    \n",
    "    return (plausibility_score >= plausibility_threshold and \n",
    "            faithfulness_score <= faithfulness_threshold)\n",
    "\n",
    "# --- COMPREHENSIVE EVALUATION ---\n",
    "def run_comprehensive_evaluation(df: pd.DataFrame, model: QwenExplanationModel) -> pd.DataFrame:\n",
    "    \"\"\"Run comprehensive evaluation with explanation generation and assessment.\"\"\"\n",
    "    \n",
    "    print(\"Generating explanations and evaluating faithfulness/plausibility...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating explanations\"):\n",
    "        try:\n",
    "            question = row['question']\n",
    "            gold_answer = row['answer']\n",
    "            \n",
    "            # Generate explanation\n",
    "            pred_answer, explanation, reasoning_steps = model.generate_explanation(question)\n",
    "            \n",
    "            # Evaluate faithfulness\n",
    "            faithfulness = evaluate_faithfulness(\n",
    "                explanation, reasoning_steps, question, pred_answer, model.semantic_model\n",
    "            )\n",
    "            \n",
    "            # Evaluate plausibility\n",
    "            plausibility = evaluate_plausibility(\n",
    "                explanation, reasoning_steps, question, model.semantic_model\n",
    "            )\n",
    "            \n",
    "            # Detect hallucination\n",
    "            hallucination = detect_hallucination(faithfulness, plausibility)\n",
    "            \n",
    "            # Update dataframe\n",
    "            df.at[idx, 'model_prediction'] = pred_answer\n",
    "            df.at[idx, 'model_explanation'] = explanation\n",
    "            df.at[idx, 'model_reasoning_steps'] = reasoning_steps\n",
    "            df.at[idx, 'faithfulness_score'] = faithfulness\n",
    "            df.at[idx, 'plausibility_score'] = plausibility\n",
    "            df.at[idx, 'hallucination_indicator'] = hallucination\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            # Set default values for failed cases\n",
    "            df.at[idx, 'model_prediction'] = \"Error\"\n",
    "            df.at[idx, 'model_explanation'] = \"Error in generation\"\n",
    "            df.at[idx, 'model_reasoning_steps'] = []\n",
    "            df.at[idx, 'faithfulness_score'] = 1.0\n",
    "            df.at[idx, 'plausibility_score'] = 1.0\n",
    "            df.at[idx, 'hallucination_indicator'] = False\n",
    "            continue\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- ACCURACY METRICS ---\n",
    "def compute_exact_match(prediction: str, gold: str) -> int:\n",
    "    \"\"\"Compute exact match score.\"\"\"\n",
    "    return int(str(prediction).strip().lower() == str(gold).strip().lower())\n",
    "\n",
    "def compute_f1(prediction: str, gold: str) -> float:\n",
    "    \"\"\"Compute F1 score.\"\"\"\n",
    "    pred_tokens = str(prediction).lower().split()\n",
    "    gold_tokens = str(gold).lower().split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    prec = len(common) / len(pred_tokens)\n",
    "    rec = len(common) / len(gold_tokens)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "def compute_bleu(prediction: str, gold: str) -> float:\n",
    "    \"\"\"Compute BLEU score.\"\"\"\n",
    "    pred_tokens = str(prediction).lower().split()\n",
    "    gold_tokens = str(gold).lower().split()\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return 0.0\n",
    "    reference = [gold_tokens]\n",
    "    candidate = pred_tokens\n",
    "    weights = (0.25, 0.25, 0.25, 0.25)\n",
    "    chencherry = SmoothingFunction()\n",
    "    try:\n",
    "        bleu_score = sentence_bleu(reference, candidate, weights=weights, smoothing_function=chencherry.method1)\n",
    "    except:\n",
    "        bleu_score = 0.0\n",
    "    return bleu_score\n",
    "\n",
    "def compute_comprehensive_metrics(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
    "    \n",
    "    # Basic accuracy metrics\n",
    "    em_scores = [compute_exact_match(row['model_prediction'], row['answer']) \n",
    "                for _, row in df.iterrows()]\n",
    "    f1_scores = [compute_f1(row['model_prediction'], row['answer']) \n",
    "                for _, row in df.iterrows()]\n",
    "    bleu_scores = [compute_bleu(row['model_prediction'], row['answer']) \n",
    "                  for _, row in df.iterrows()]\n",
    "    \n",
    "    # Explanation quality metrics\n",
    "    faithfulness_scores = df['faithfulness_score'].tolist()\n",
    "    plausibility_scores = df['plausibility_score'].tolist()\n",
    "    hallucination_rates = df['hallucination_indicator'].tolist()\n",
    "    \n",
    "    # Hop-wise analysis\n",
    "    hop_wise_results = {}\n",
    "    for hop_cat in df['hop_category'].unique():\n",
    "        if pd.isna(hop_cat):\n",
    "            continue\n",
    "        \n",
    "        subset = df[df['hop_category'] == hop_cat]\n",
    "        hop_wise_results[str(hop_cat)] = {\n",
    "            'count': len(subset),\n",
    "            'EM': np.mean([compute_exact_match(row['model_prediction'], row['answer']) \n",
    "                          for _, row in subset.iterrows()]),\n",
    "            'F1': np.mean([compute_f1(row['model_prediction'], row['answer']) \n",
    "                          for _, row in subset.iterrows()]),\n",
    "            'BLEU': np.mean([compute_bleu(row['model_prediction'], row['answer']) \n",
    "                            for _, row in subset.iterrows()]),\n",
    "            'Faithfulness': np.mean(subset['faithfulness_score']),\n",
    "            'Plausibility': np.mean(subset['plausibility_score']),\n",
    "            'Hallucination_Rate': np.mean(subset['hallucination_indicator'])\n",
    "        }\n",
    "    \n",
    "    # Correlation analysis\n",
    "    faithfulness_plausibility_corr = pearsonr(faithfulness_scores, plausibility_scores)[0]\n",
    "    faithfulness_accuracy_corr = pearsonr(faithfulness_scores, f1_scores)[0]\n",
    "    plausibility_accuracy_corr = pearsonr(plausibility_scores, f1_scores)[0]\n",
    "    \n",
    "    return {\n",
    "        'overall_metrics': {\n",
    "            'Exact_Match': np.mean(em_scores),\n",
    "            'F1_Score': np.mean(f1_scores),\n",
    "            'BLEU_Score': np.mean(bleu_scores),\n",
    "            'Faithfulness': np.mean(faithfulness_scores),\n",
    "            'Plausibility': np.mean(plausibility_scores),\n",
    "            'Hallucination_Rate': np.mean(hallucination_rates),\n",
    "            'Total_Questions': len(df)\n",
    "        },\n",
    "        'hop_wise_metrics': hop_wise_results,\n",
    "        'correlations': {\n",
    "            'Faithfulness_Plausibility': faithfulness_plausibility_corr,\n",
    "            'Faithfulness_Accuracy': faithfulness_accuracy_corr,\n",
    "            'Plausibility_Accuracy': plausibility_accuracy_corr\n",
    "        },\n",
    "        'detailed_scores': {\n",
    "            'faithfulness_scores': faithfulness_scores,\n",
    "            'plausibility_scores': plausibility_scores,\n",
    "            'hallucination_indicators': hallucination_rates,\n",
    "            'accuracy_scores': f1_scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "# --- ADVANCED VISUALIZATIONS ---\n",
    "def create_comprehensive_visualizations(results: Dict, save_dir: str = './results'):\n",
    "    \"\"\"Create comprehensive visualizations for all metrics.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    \n",
    "    # 1. Overall Performance Dashboard\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Overall metrics bar chart\n",
    "    overall = results['overall_metrics']\n",
    "    metrics = ['Exact_Match', 'F1_Score', 'BLEU_Score', 'Faithfulness', 'Plausibility']\n",
    "    values = [overall[metric] for metric in metrics]\n",
    "    colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "    \n",
    "    bars = ax1.bar(metrics, values, color=colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Score', fontweight='bold')\n",
    "    ax1.set_title('Overall Performance Metrics', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylim(0, 5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Faithfulness vs Plausibility scatter plot\n",
    "    faith_scores = results['detailed_scores']['faithfulness_scores']\n",
    "    plaus_scores = results['detailed_scores']['plausibility_scores']\n",
    "    halluc_indicators = results['detailed_scores']['hallucination_indicators']\n",
    "    \n",
    "    # Color points by hallucination status\n",
    "    colors_scatter = ['red' if h else 'blue' for h in halluc_indicators]\n",
    "    scatter = ax2.scatter(faith_scores, plaus_scores, c=colors_scatter, alpha=0.6)\n",
    "    ax2.set_xlabel('Faithfulness Score', fontweight='bold')\n",
    "    ax2.set_ylabel('Plausibility Score', fontweight='bold')\n",
    "    ax2.set_title('Faithfulness vs Plausibility\\n(Red = Hallucination)', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal line and quadrant labels\n",
    "    ax2.plot([1, 5], [1, 5], 'k--', alpha=0.5)\n",
    "    ax2.text(4.5, 2, 'High Plausibility\\nLow Faithfulness\\n(Hallucination Zone)', \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"red\", alpha=0.3))\n",
    "    \n",
    "    # Hop-wise performance\n",
    "    hop_data = results['hop_wise_metrics']\n",
    "    hop_categories = list(hop_data.keys())\n",
    "    hop_f1 = [hop_data[cat]['F1'] for cat in hop_categories]\n",
    "    hop_faith = [hop_data[cat]['Faithfulness'] for cat in hop_categories]\n",
    "    hop_plaus = [hop_data[cat]['Plausibility'] for cat in hop_categories]\n",
    "    \n",
    "    x = np.arange(len(hop_categories))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax3.bar(x - width, hop_f1, width, label='F1 Score', color='#3498db', alpha=0.8)\n",
    "    ax3.bar(x, hop_faith, width, label='Faithfulness', color='#e74c3c', alpha=0.8)\n",
    "    ax3.bar(x + width, hop_plaus, width, label='Plausibility', color='#9b59b6', alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('Reasoning Complexity', fontweight='bold')\n",
    "    ax3.set_ylabel('Score', fontweight='bold')\n",
    "    ax3.set_title('Performance by Reasoning Complexity', fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(hop_categories)\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Hallucination rate by complexity\n",
    "    hop_halluc = [hop_data[cat]['Hallucination_Rate'] * 100 for cat in hop_categories]\n",
    "    \n",
    "    bars_halluc = ax4.bar(hop_categories, hop_halluc, color='#e74c3c', alpha=0.7)\n",
    "    ax4.set_ylabel('Hallucination Rate (%)', fontweight='bold')\n",
    "    ax4.set_xlabel('Reasoning Complexity', fontweight='bold')\n",
    "    ax4.set_title('Hallucination Rate by Complexity', fontweight='bold')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, rate in zip(bars_halluc, hop_halluc):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/comprehensive_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Correlation Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_data = {\n",
    "        'Faithfulness': faith_scores,\n",
    "        'Plausibility': plaus_scores,\n",
    "        'F1_Score': results['detailed_scores']['accuracy_scores'],\n",
    "        'Hallucination': [int(h) for h in halluc_indicators]\n",
    "    }\n",
    "    \n",
    "    corr_df = pd.DataFrame(corr_data)\n",
    "    correlation_matrix = corr_df.corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', \n",
    "                center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Correlation Matrix: Explanation Quality vs Performance', \n",
    "              fontweight='bold', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Distribution Analysis\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "        # Faithfulness distribution\n",
    "    ax1.hist(faith_scores, bins=20, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(np.mean(faith_scores), color='blue', linestyle='--', \n",
    "                label=f'Mean: {np.mean(faith_scores):.2f}')\n",
    "    ax1.set_xlabel('Faithfulness Score')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Faithfulness Scores', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plausibility distribution\n",
    "    ax2.hist(plaus_scores, bins=20, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "    ax2.axvline(np.mean(plaus_scores), color='blue', linestyle='--', \n",
    "                label=f'Mean: {np.mean(plaus_scores):.2f}')\n",
    "    ax2.set_xlabel('Plausibility Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Plausibility Scores', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # F1 Score distribution\n",
    "    f1_scores = results['detailed_scores']['accuracy_scores']\n",
    "    ax3.hist(f1_scores, bins=20, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(np.mean(f1_scores), color='blue', linestyle='--', \n",
    "                label=f'Mean: {np.mean(f1_scores):.2f}')\n",
    "    ax3.set_xlabel('F1 Score')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Distribution of F1 Scores', fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Hallucination analysis\n",
    "    halluc_counts = [sum(halluc_indicators), len(halluc_indicators) - sum(halluc_indicators)]\n",
    "    labels = ['Hallucinated', 'Non-Hallucinated']\n",
    "    colors_pie = ['#e74c3c', '#2ecc71']\n",
    "    \n",
    "    wedges, texts, autotexts = ax4.pie(halluc_counts, labels=labels, colors=colors_pie, \n",
    "                                      autopct='%1.1f%%', startangle=90)\n",
    "    ax4.set_title('Hallucination Distribution', fontweight='bold')\n",
    "    \n",
    "    # Make percentage text bold\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Detailed Performance Analysis by Question Type\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Group by question types if available\n",
    "    if 'type' in results.get('question_types', {}):\n",
    "        # This would need to be implemented based on question types in MenatQA\n",
    "        pass\n",
    "    else:\n",
    "        # Create analysis by complexity and hallucination status\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Performance comparison: Hallucinated vs Non-Hallucinated\n",
    "        halluc_faith = [f for f, h in zip(faith_scores, halluc_indicators) if h]\n",
    "        non_halluc_faith = [f for f, h in zip(faith_scores, halluc_indicators) if not h]\n",
    "        halluc_f1 = [f for f, h in zip(f1_scores, halluc_indicators) if h]\n",
    "        non_halluc_f1 = [f for f, h in zip(f1_scores, halluc_indicators) if not h]\n",
    "        \n",
    "        # Box plots for comparison\n",
    "        faith_data = [non_halluc_faith, halluc_faith]\n",
    "        f1_data = [non_halluc_f1, halluc_f1]\n",
    "        \n",
    "        bp1 = ax1.boxplot(faith_data, labels=['Non-Hallucinated', 'Hallucinated'], \n",
    "                         patch_artist=True)\n",
    "        bp1['boxes'][0].set_facecolor('#2ecc71')\n",
    "        bp1['boxes'][1].set_facecolor('#e74c3c')\n",
    "        ax1.set_ylabel('Faithfulness Score')\n",
    "        ax1.set_title('Faithfulness: Hallucinated vs Non-Hallucinated', fontweight='bold')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        bp2 = ax2.boxplot(f1_data, labels=['Non-Hallucinated', 'Hallucinated'], \n",
    "                         patch_artist=True)\n",
    "        bp2['boxes'][0].set_facecolor('#2ecc71')\n",
    "        bp2['boxes'][1].set_facecolor('#e74c3c')\n",
    "        ax2.set_ylabel('F1 Score')\n",
    "        ax2.set_title('Accuracy: Hallucinated vs Non-Hallucinated', fontweight='bold')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_dir}/hallucination_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def print_comprehensive_results(results: Dict, model_name: str):\n",
    "    \"\"\"Print detailed results summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"COMPREHENSIVE EVALUATION RESULTS - {model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    overall = results['overall_metrics']\n",
    "    print(f\"\\nüìä OVERALL PERFORMANCE:\")\n",
    "    print(f\"   ‚Ä¢ Total Questions Evaluated: {overall['Total_Questions']}\")\n",
    "    print(f\"   ‚Ä¢ Exact Match: {overall['Exact_Match']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Score: {overall['F1_Score']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ BLEU Score: {overall['BLEU_Score']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüîç EXPLANATION QUALITY:\")\n",
    "    print(f\"   ‚Ä¢ Average Faithfulness: {overall['Faithfulness']:.3f}/5.0\")\n",
    "    print(f\"   ‚Ä¢ Average Plausibility: {overall['Plausibility']:.3f}/5.0\")\n",
    "    print(f\"   ‚Ä¢ Hallucination Rate: {overall['Hallucination_Rate']:.1%}\")\n",
    "    \n",
    "    print(f\"\\nüìà CORRELATIONS:\")\n",
    "    corr = results['correlations']\n",
    "    print(f\"   ‚Ä¢ Faithfulness ‚Üî Plausibility: {corr['Faithfulness_Plausibility']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Faithfulness ‚Üî Accuracy: {corr['Faithfulness_Accuracy']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Plausibility ‚Üî Accuracy: {corr['Plausibility_Accuracy']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ PERFORMANCE BY REASONING COMPLEXITY:\")\n",
    "    hop_metrics = results['hop_wise_metrics']\n",
    "    for hop_cat, metrics in hop_metrics.items():\n",
    "        print(f\"\\n   {hop_cat} Questions (n={metrics['count']}):\")\n",
    "        print(f\"      ‚îú‚îÄ F1 Score: {metrics['F1']:.3f}\")\n",
    "        print(f\"      ‚îú‚îÄ Faithfulness: {metrics['Faithfulness']:.3f}\")\n",
    "        print(f\"      ‚îú‚îÄ Plausibility: {metrics['Plausibility']:.3f}\")\n",
    "        print(f\"      ‚îî‚îÄ Hallucination Rate: {metrics['Hallucination_Rate']:.1%}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "    if overall['Hallucination_Rate'] > 0.2:\n",
    "        print(f\"   ‚ö†Ô∏è  High hallucination rate detected ({overall['Hallucination_Rate']:.1%})\")\n",
    "    if corr['Faithfulness_Plausibility'] < 0.5:\n",
    "        print(f\"   ‚ö†Ô∏è  Low correlation between faithfulness and plausibility\")\n",
    "    if overall['Faithfulness'] < 3.0:\n",
    "        print(f\"   ‚ö†Ô∏è  Low faithfulness scores indicate reasoning issues\")\n",
    "    if overall['Plausibility'] > 3.5 and overall['Faithfulness'] < 3.0:\n",
    "        print(f\"   üö® High plausibility with low faithfulness suggests systematic hallucination\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "def main():\n",
    "    \"\"\"Run the complete evaluation pipeline.\"\"\"\n",
    "    print(\"üöÄ MenatQA Comprehensive Evaluation with Explanation Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\n1Ô∏è‚É£ Loading MenatQA dataset...\")\n",
    "    try:\n",
    "        data = load_menatqa_dataset('./MenatQA.json')\n",
    "        df = preprocess_dataset(data)\n",
    "        print(f\"‚úÖ Successfully loaded and preprocessed {len(df)} questions\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        print(\"üìù Creating sample dataset for demonstration...\")\n",
    "        sample_data = [\n",
    "            {'ID': 1, 'question': 'What is the capital of Egypt?', 'answer': 'Cairo', 'type': 'geography'},\n",
    "            {'ID': 2, 'question': 'Which empire controlled most of the Middle East in the 16th century?', 'answer': 'Ottoman Empire', 'type': 'history'},\n",
    "            {'ID': 3, 'question': 'What language is primarily spoken in Morocco?', 'answer': 'Arabic', 'type': 'culture'},\n",
    "            {'ID': 4, 'question': 'Which country borders both Iraq and Iran?', 'answer': 'Turkey', 'type': 'geography'},\n",
    "            {'ID': 5, 'question': 'When did the Suez Canal open?', 'answer': '1869', 'type': 'history'}\n",
    "        ]\n",
    "        df = preprocess_dataset(sample_data)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\n2Ô∏è‚É£ Initializing Qwen model...\")\n",
    "    try:\n",
    "        model = QwenExplanationModel(\"Qwen/Qwen2-0.5B\")  # Will try Qwen3-0.6B first\n",
    "        print(f\"‚úÖ Successfully loaded {model.model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Run evaluation (limit for demo)\n",
    "    print(\"\\n3Ô∏è‚É£ Running comprehensive evaluation...\")\n",
    "    eval_sample_size = min(50, len(df))  # Limit for demonstration\n",
    "    df_sample = df.sample(n=eval_sample_size, random_state=42).reset_index(drop=True)\n",
    "    print(f\"üìä Evaluating on {len(df_sample)} questions...\")\n",
    "    \n",
    "    df_results = run_comprehensive_evaluation(df_sample, model)\n",
    "    \n",
    "    # Compute metrics\n",
    "    print(\"\\n4Ô∏è‚É£ Computing comprehensive metrics...\")\n",
    "    results = compute_comprehensive_metrics(df_results)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"\\n5Ô∏è‚É£ Generating visualizations...\")\n",
    "    results_dir = './menatqa_comprehensive_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    create_comprehensive_visualizations(results, results_dir)\n",
    "    \n",
    "    # Print results\n",
    "    print_comprehensive_results(results, model.model_name)\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(f\"\\n6Ô∏è‚É£ Saving detailed results...\")\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(f'{results_dir}/comprehensive_metrics.json', 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        json_results = {}\n",
    "        for key, value in results.items():\n",
    "            if key == 'detailed_scores':\n",
    "                json_results[key] = {k: [float(x) for x in v] for k, v in value.items()}\n",
    "            elif key == 'overall_metrics':\n",
    "                json_results[key] = {k: float(v) for k, v in value.items()}\n",
    "            elif key == 'hop_wise_metrics':\n",
    "                json_results[key] = {k: {k2: float(v2) for k2, v2 in v.items()} for k, v in value.items()}\n",
    "            elif key == 'correlations':\n",
    "                json_results[key] = {k: float(v) for k, v in value.items()}\n",
    "            else:\n",
    "                json_results[key] = value\n",
    "        \n",
    "        json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    # Save detailed examples\n",
    "    examples_for_inspection = []\n",
    "    for idx, row in df_results.head(10).iterrows():\n",
    "        example = {\n",
    "            'question': row['question'],\n",
    "            'gold_answer': row['answer'],\n",
    "            'predicted_answer': row['model_prediction'],\n",
    "            'explanation': row['model_explanation'],\n",
    "            'reasoning_steps': row['model_reasoning_steps'],\n",
    "            'faithfulness_score': float(row['faithfulness_score']),\n",
    "            'plausibility_score': float(row['plausibility_score']),\n",
    "            'hallucination_indicator': bool(row['hallucination_indicator']),\n",
    "            'hop_category': str(row['hop_category'])\n",
    "        }\n",
    "        examples_for_inspection.append(example)\n",
    "    \n",
    "    with open(f'{results_dir}/example_evaluations.json', 'w') as f:\n",
    "        json.dump(examples_for_inspection, f, indent=2)\n",
    "    \n",
    "    # Save evaluation DataFrame\n",
    "    df_results.to_csv(f'{results_dir}/full_evaluation_results.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation complete! Results saved to: {results_dir}\")\n",
    "    print(f\"üìÅ Files generated:\")\n",
    "    print(f\"   ‚Ä¢ comprehensive_dashboard.png\")\n",
    "    print(f\"   ‚Ä¢ correlation_heatmap.png\") \n",
    "    print(f\"   ‚Ä¢ distribution_analysis.png\")\n",
    "    print(f\"   ‚Ä¢ hallucination_comparison.png\")\n",
    "    print(f\"   ‚Ä¢ comprehensive_metrics.json\")\n",
    "    print(f\"   ‚Ä¢ example_evaluations.json\")\n",
    "    print(f\"   ‚Ä¢ full_evaluation_results.csv\")\n",
    "    \n",
    "    return results, df_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, df_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606376d-f7d2-4d59-9c56-1d5a2b02aa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d2f37-483c-4ddf-b658-329e433c6316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4cdab2-e2c9-4b1d-880a-b7efe8b1639a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f91a8-24d2-4df8-837d-9ca98b283e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738ebdc-74fb-4de6-b92c-4a4315d3d8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98491bcf-997d-4eb0-ad05-9fcebbf4209a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (984619119.py, line 418)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_71714/984619119.py\"\u001b[0;36m, line \u001b[0;32m418\u001b[0m\n\u001b[0;31m    criteria_count += 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from collections import defaultdict, Counter\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from pathlib import Path\n",
    "# import random\n",
    "# import networkx as nx\n",
    "# from typing import List, Dict, Tuple, Any, Set\n",
    "# import warnings\n",
    "# import time\n",
    "# import gc\n",
    "# from datetime import datetime\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# # Download NLTK data\n",
    "# import nltk\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt', quiet=True)\n",
    "\n",
    "# # Set style for better visualizations\n",
    "# plt.style.use('seaborn-v0_8')\n",
    "# sns.set_palette(\"husl\")\n",
    "\n",
    "# # --- DATASET LOADING ---\n",
    "# def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "#     \"\"\"Load MenatQA dataset with automatic download.\"\"\"\n",
    "#     file_path = Path(file_path)\n",
    "    \n",
    "#     if not file_path.exists():\n",
    "#         import urllib.request\n",
    "#         print(f\"Downloading MenatQA to {file_path}...\")\n",
    "#         try:\n",
    "#             urllib.request.urlretrieve(\n",
    "#                 \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "#                 str(file_path)\n",
    "#             )\n",
    "#             print(\"Download completed successfully!\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Download failed: {e}\")\n",
    "#             raise\n",
    "    \n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "#     print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "#     return data\n",
    "\n",
    "# def extract_reasoning_hops(example):\n",
    "#     \"\"\"Extract reasoning complexity for hop-wise analysis.\"\"\"\n",
    "#     question = example.get('question', '')\n",
    "#     answer = example.get('answer', '')\n",
    "#     q_type = example.get('type', '')\n",
    "#     time_scope = example.get('time_scope', '')\n",
    "    \n",
    "#     sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "#     clauses = len([c for c in re.split(r'and|or|but|because|when|if', question.lower()) if c.strip()])\n",
    "#     capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "#     complexity_score = 1\n",
    "#     complexity_score += min(1, len(sentences) - 1)\n",
    "#     complexity_score += min(1, (clauses - 1) // 2)\n",
    "#     complexity_score += min(1, capitalized_words // 3)\n",
    "#     if time_scope:\n",
    "#         complexity_score += 1\n",
    "#     complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "#     hops = []\n",
    "#     hops.append(f\"Understand the question: {question}\")\n",
    "#     if complexity_score >= 2:\n",
    "#         if time_scope:\n",
    "#             hops.append(f\"Identify time context: {time_scope}\")\n",
    "#         else:\n",
    "#             hops.append(f\"Recognize question type: {q_type}\")\n",
    "#     if complexity_score >= 3:\n",
    "#         hops.append(\"Retrieve relevant facts and information\")\n",
    "#     if complexity_score >= 4:\n",
    "#         hops.append(\"Analyze relationships between facts\")\n",
    "#     hops.append(f\"Formulate answer: {answer}\")\n",
    "    \n",
    "#     return hops, complexity_score\n",
    "\n",
    "# def preprocess_dataset(data):\n",
    "#     \"\"\"Preprocess MenatQA dataset for comprehensive evaluation.\"\"\"\n",
    "#     processed_data = []\n",
    "#     hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "#     for item in data:\n",
    "#         gold_hops, complexity_score = extract_reasoning_hops(item)\n",
    "#         hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "#         entry = {\n",
    "#             'ID': item.get('ID', ''),\n",
    "#             'question': item.get('question', ''),\n",
    "#             'answer': item.get('answer', ''),\n",
    "#             'type': item.get('type', ''),\n",
    "#             'time_scope': item.get('time_scope', ''),\n",
    "#             'gold_hops': gold_hops,\n",
    "#             'hop_count': complexity_score,\n",
    "#             'model_prediction': '',\n",
    "#             'model_explanation': '',\n",
    "#             'model_reasoning_steps': [],\n",
    "#             'faithfulness_score': 0.0,\n",
    "#             'plausibility_score': 0.0,\n",
    "#             'hallucination_indicator': False\n",
    "#         }\n",
    "#         processed_data.append(entry)\n",
    "    \n",
    "#     df = pd.DataFrame(processed_data)\n",
    "#     df['hop_category'] = pd.cut(\n",
    "#         df['hop_count'],\n",
    "#         bins=[-1, 1, 2, 3, float('inf')],\n",
    "#         labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "#     for i in range(1, 5):\n",
    "#         print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # --- QWEN MODEL SETUP ---\n",
    "# class QwenExplanationModel:\n",
    "#     \"\"\"Qwen model for explanation generation and evaluation.\"\"\"\n",
    "    \n",
    "#     def __init__(self, model_name=\"Qwen/Qwen2-0.5B\", device=\"auto\", batch_size=8):\n",
    "#         print(f\"Loading {model_name}...\")\n",
    "#         self.model_name = model_name\n",
    "#         self.batch_size = batch_size\n",
    "        \n",
    "#         # Try Qwen models in order of preference\n",
    "#         models_to_try = [\n",
    "#             \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "#             \"Qwen/Qwen2-0.5B-Instruct\", \n",
    "#             \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "#             \"Qwen/Qwen2-0.5B\",\n",
    "#             model_name\n",
    "#         ]\n",
    "        \n",
    "#         self.tokenizer = None\n",
    "#         self.model = None\n",
    "        \n",
    "#         for model_to_try in models_to_try:\n",
    "#             try:\n",
    "#                 print(f\"Attempting to load {model_to_try}...\")\n",
    "#                 self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "#                     model_to_try, \n",
    "#                     trust_remote_code=True,\n",
    "#                     padding_side='left'\n",
    "#                 )\n",
    "#                 self.model = AutoModelForCausalLM.from_pretrained(\n",
    "#                     model_to_try,\n",
    "#                     trust_remote_code=True,\n",
    "#                     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "#                     device_map=device,\n",
    "#                     low_cpu_mem_usage=True\n",
    "#                 )\n",
    "#                 self.model_name = model_to_try\n",
    "#                 print(f\"Successfully loaded {model_to_try}\")\n",
    "#                 break\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to load {model_to_try}: {e}\")\n",
    "#                 continue\n",
    "        \n",
    "#         if self.model is None:\n",
    "#             raise Exception(\"Failed to load any Qwen model\")\n",
    "        \n",
    "#         # Add padding token if not present\n",
    "#         if self.tokenizer.pad_token is None:\n",
    "#             self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "#         # Load semantic similarity model for faithfulness evaluation\n",
    "#         print(\"Loading semantic similarity model...\")\n",
    "#         self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "#     def generate_explanation_batch(self, questions: List[str], max_length: int = 512) -> List[Tuple[str, str, List[str]]]:\n",
    "#         \"\"\"Generate explanations for a batch of questions.\"\"\"\n",
    "#         prompts = []\n",
    "#         for question in questions:\n",
    "#             prompt = f\"\"\"Answer the following question and provide a detailed explanation of your reasoning process.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Please structure your response as:\n",
    "# 1. Step-by-step reasoning (show each logical step)\n",
    "# 2. Final answer\n",
    "\n",
    "# Your explanation should be clear and show exactly how you arrived at the answer.\n",
    "\n",
    "# Reasoning steps:\"\"\"\n",
    "#             prompts.append(prompt)\n",
    "        \n",
    "#         try:\n",
    "#             # Tokenize batch\n",
    "#             inputs = self.tokenizer(\n",
    "#                 prompts,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 padding=True,\n",
    "#                 truncation=True,\n",
    "#                 max_length=1024\n",
    "#             )\n",
    "            \n",
    "#             # Move to device\n",
    "#             if torch.cuda.is_available():\n",
    "#                 inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "#             # Generate\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=max_length,\n",
    "#                     do_sample=True,\n",
    "#                     temperature=0.1,\n",
    "#                     pad_token_id=self.tokenizer.eos_token_id,\n",
    "#                     eos_token_id=self.tokenizer.eos_token_id\n",
    "#                 )\n",
    "            \n",
    "#             # Decode responses\n",
    "#             responses = []\n",
    "#             for i, output in enumerate(outputs):\n",
    "#                 # Remove input prompt from output\n",
    "#                 input_length = inputs['input_ids'][i].shape[0]\n",
    "#                 generated_tokens = output[input_length:]\n",
    "#                 response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "                \n",
    "#                 # Parse reasoning steps and final answer\n",
    "#                 lines = response.split('\\n')\n",
    "#                 reasoning_steps = []\n",
    "#                 final_answer = \"\"\n",
    "                \n",
    "#                 for line in lines:\n",
    "#                     line = line.strip()\n",
    "#                     if re.match(r'^\\d+\\.', line) or line.startswith('-') or line.startswith('‚Ä¢'):\n",
    "#                         reasoning_steps.append(re.sub(r'^\\d+\\.|-|‚Ä¢', '', line).strip())\n",
    "#                     elif line.startswith('Answer:') or line.startswith('Final answer:'):\n",
    "#                         final_answer = re.sub(r'^(Answer|Final answer):', '', line, flags=re.IGNORECASE).strip()\n",
    "#                     elif line and not final_answer and reasoning_steps:\n",
    "#                         # Likely the final answer without explicit label\n",
    "#                         final_answer = line\n",
    "                \n",
    "#                 if not final_answer and reasoning_steps:\n",
    "#                     final_answer = reasoning_steps[-1]\n",
    "                \n",
    "#                 if not final_answer:\n",
    "#                     final_answer = response.split('\\n')[-1] if response else \"No answer generated\"\n",
    "                \n",
    "#                 full_explanation = '\\n'.join(reasoning_steps) if reasoning_steps else response\n",
    "                \n",
    "#                 responses.append((final_answer, full_explanation, reasoning_steps))\n",
    "            \n",
    "#             return responses\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in batch generation: {e}\")\n",
    "#             # Fallback to individual generation\n",
    "#             return [self.generate_explanation_single(q, max_length) for q in questions]\n",
    "    \n",
    "#     def generate_explanation_single(self, question: str, max_length: int = 512) -> Tuple[str, str, List[str]]:\n",
    "#         \"\"\"Generate explanation for a single question (fallback method).\"\"\"\n",
    "#         prompt = f\"\"\"Answer the following question and provide a detailed explanation of your reasoning process.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Please structure your response as:\n",
    "# 1. Step-by-step reasoning (show each logical step)\n",
    "# 2. Final answer\n",
    "\n",
    "# Your explanation should be clear and show exactly how you arrived at the answer.\n",
    "\n",
    "# Reasoning steps:\"\"\"\n",
    "\n",
    "#         try:\n",
    "#             inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "#             if torch.cuda.is_available():\n",
    "#                 inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=max_length,\n",
    "#                     do_sample=True,\n",
    "#                     temperature=0.1,\n",
    "#                     pad_token_id=self.tokenizer.eos_token_id\n",
    "#                 )\n",
    "            \n",
    "#             generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#             response = generated_text[len(prompt):].strip()\n",
    "            \n",
    "#             # Parse reasoning steps and final answer\n",
    "#             lines = response.split('\\n')\n",
    "#             reasoning_steps = []\n",
    "#             final_answer = \"\"\n",
    "            \n",
    "#             for line in lines:\n",
    "#                 line = line.strip()\n",
    "#                 if re.match(r'^\\d+\\.', line) or line.startswith('-') or line.startswith('‚Ä¢'):\n",
    "#                     reasoning_steps.append(re.sub(r'^\\d+\\.|-|‚Ä¢', '', line).strip())\n",
    "#                 elif line.startswith('Answer:') or line.startswith('Final answer:'):\n",
    "#                     final_answer = re.sub(r'^(Answer|Final answer):', '', line, flags=re.IGNORECASE).strip()\n",
    "#                 elif line and not final_answer and reasoning_steps:\n",
    "#                     final_answer = line\n",
    "            \n",
    "#             if not final_answer and reasoning_steps:\n",
    "#                 final_answer = reasoning_steps[-1]\n",
    "            \n",
    "#             if not final_answer:\n",
    "#                 final_answer = response.split('\\n')[-1] if response else \"No answer generated\"\n",
    "            \n",
    "#             full_explanation = '\\n'.join(reasoning_steps) if reasoning_steps else response\n",
    "            \n",
    "#             return final_answer, full_explanation, reasoning_steps\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating explanation: {e}\")\n",
    "#             return \"Error in generation\", \"Error in explanation\", []\n",
    "\n",
    "# # --- FAITHFULNESS EVALUATION ---\n",
    "# def evaluate_faithfulness(explanation: str, reasoning_steps: List[str], \n",
    "#                          question: str, answer: str, semantic_model: SentenceTransformer) -> float:\n",
    "#     \"\"\"\n",
    "#     Evaluate faithfulness: Does the explanation truly reflect the computation that produced the answer?\n",
    "#     Score: 1-5 (1=completely unfaithful, 5=perfectly faithful)\n",
    "#     \"\"\"\n",
    "#     if not explanation or not reasoning_steps:\n",
    "#         return 1.0\n",
    "    \n",
    "#     faithfulness_score = 0.0\n",
    "#     criteria_count = 0\n",
    "    \n",
    "#     # Criterion 1: Logical flow consistency (25%)\n",
    "#     if len(reasoning_steps) > 1:\n",
    "#         flow_consistency = 0.0\n",
    "#         for i in range(len(reasoning_steps) - 1):\n",
    "#             current_step = reasoning_steps[i]\n",
    "#             next_step = reasoning_steps[i + 1]\n",
    "            \n",
    "#             # Check if steps logically connect\n",
    "#             try:\n",
    "#                 similarity = cosine_similarity(\n",
    "#                     semantic_model.encode([current_step]).reshape(1, -1),\n",
    "#                     semantic_model.encode([next_step]).reshape(1, -1)\n",
    "#                 )[0][0]\n",
    "#             except:\n",
    "#                 similarity = 0.5  # Default similarity\n",
    "            \n",
    "#             # Check for logical connectors\n",
    "#             logical_words = ['therefore', 'thus', 'because', 'since', 'so', 'hence', 'as a result']\n",
    "#             has_connectors = any(word in next_step.lower() for word in logical_words)\n",
    "            \n",
    "#             step_score = (similarity * 0.7) + (0.3 if has_connectors else 0)\n",
    "#             flow_consistency += step_score\n",
    "        \n",
    "#         flow_consistency /= (len(reasoning_steps) - 1)\n",
    "#         faithfulness_score += flow_consistency * 1.25  # 25% weight\n",
    "#         criteria_count += 1\n",
    "    \n",
    "#     # Criterion 2: Question-answer alignment (25%)\n",
    "#     question_answer_alignment = 0.0\n",
    "    \n",
    "#     try:\n",
    "#         # Check if explanation addresses the question\n",
    "#         question_similarity = cosine_similarity(\n",
    "#             semantic_model.encode([explanation]).reshape(1, -1),\n",
    "#             semantic_model.encode([question]).reshape(1, -1)\n",
    "#         )[0][0]\n",
    "        \n",
    "#         # Check if explanation leads to the answer\n",
    "#         answer_similarity = cosine_similarity(\n",
    "#             semantic_model.encode([explanation]).reshape(1, -1),\n",
    "#             semantic_model.encode([answer]).reshape(1, -1)\n",
    "#         )[0][0]\n",
    "        \n",
    "#         question_answer_alignment = (question_similarity + answer_similarity) / 2\n",
    "#     except:\n",
    "#         question_answer_alignment = 0.5  # Default alignment\n",
    "    \n",
    "#     faithfulness_score += question_answer_alignment * 1.25  # 25% weight\n",
    "#     criteria_count += 1\n",
    "    \n",
    "#     # Criterion 3: Step necessity (25%)\n",
    "#     step_necessity = 0.0\n",
    "#     if reasoning_steps:\n",
    "#         necessary_steps = 0\n",
    "#         for step in reasoning_steps:\n",
    "#             try:\n",
    "#                 # Check if step is relevant to question or contributes to answer\n",
    "#                 step_relevance = max(\n",
    "#                     cosine_similarity(\n",
    "#                         semantic_model.encode([step]).reshape(1, -1),\n",
    "#                         semantic_model.encode([question]).reshape(1, -1)\n",
    "#                     )[0][0],\n",
    "#                     cosine_similarity(\n",
    "#                         semantic_model.encode([step]).reshape(1, -1),\n",
    "#                         semantic_model.encode([answer]).reshape(1, -1)\n",
    "#                     )[0][0]\n",
    "#                 )\n",
    "                \n",
    "#                 if step_relevance > 0.3:  # Threshold for relevance\n",
    "#                     necessary_steps += 1\n",
    "#             except:\n",
    "#                 necessary_steps += 0.5  # Default relevance\n",
    "        \n",
    "#         step_necessity = necessary_steps / len(reasoning_steps) if reasoning_steps else 0\n",
    "#         faithfulness_score += step_necessity * 1.25  # 25% weight\n",
    "#                 criteria_count += 1\n",
    "    \n",
    "#     # Criterion 4: Factual consistency (25%)\n",
    "#     factual_consistency = 0.0\n",
    "    \n",
    "#     # Check for contradictions within explanation\n",
    "#     if len(reasoning_steps) > 1:\n",
    "#         contradiction_penalty = 0\n",
    "#         for i, step1 in enumerate(reasoning_steps):\n",
    "#             for j, step2 in enumerate(reasoning_steps[i+1:], i+1):\n",
    "#                 # Simple contradiction detection using negation patterns\n",
    "#                 negation_words = ['not', 'no', 'never', 'none', 'neither', 'nor']\n",
    "#                 step1_words = set(step1.lower().split())\n",
    "#                 step2_words = set(step2.lower().split())\n",
    "                \n",
    "#                 # Check for direct contradictions\n",
    "#                 if any(word in step1_words and f\"not {word}\" in step2.lower() for word in step1_words):\n",
    "#                     contradiction_penalty += 0.1\n",
    "#                 elif any(word in step2_words and f\"not {word}\" in step1.lower() for word in step2_words):\n",
    "#                     contradiction_penalty += 0.1\n",
    "        \n",
    "#         factual_consistency = max(0, 1.0 - contradiction_penalty)\n",
    "#     else:\n",
    "#         factual_consistency = 1.0\n",
    "    \n",
    "#     faithfulness_score += factual_consistency * 1.25  # 25% weight\n",
    "#     criteria_count += 1\n",
    "    \n",
    "#     # Normalize to 1-5 scale\n",
    "#     if criteria_count > 0:\n",
    "#         normalized_score = (faithfulness_score / criteria_count) * 4 + 1  # Convert to 1-5 scale\n",
    "#         return min(5.0, max(1.0, normalized_score))\n",
    "    \n",
    "#     return 1.0\n",
    "\n",
    "# def evaluate_plausibility(explanation: str, reasoning_steps: List[str], \n",
    "#                          question: str, semantic_model: SentenceTransformer) -> float:\n",
    "#     \"\"\"\n",
    "#     Evaluate plausibility: Is the explanation convincing, regardless of correctness?\n",
    "#     Score: 1-5 (1=completely implausible, 5=very convincing)\n",
    "#     \"\"\"\n",
    "#     if not explanation or not reasoning_steps:\n",
    "#         return 1.0\n",
    "    \n",
    "#     plausibility_score = 0.0\n",
    "#     criteria_count = 0\n",
    "    \n",
    "#     # Criterion 1: Clarity and coherence (30%)\n",
    "#     clarity_score = 0.0\n",
    "    \n",
    "#     # Check explanation length (not too short, not too verbose)\n",
    "#     explanation_length = len(explanation.split())\n",
    "#     length_score = 1.0\n",
    "#     if explanation_length < 10:\n",
    "#         length_score = 0.5  # Too short\n",
    "#     elif explanation_length > 200:\n",
    "#         length_score = 0.7  # Too verbose\n",
    "    \n",
    "#     # Check for clear structure\n",
    "#     structure_indicators = ['first', 'second', 'then', 'next', 'finally', 'therefore']\n",
    "#     structure_score = min(1.0, sum(1 for indicator in structure_indicators \n",
    "#                                  if indicator in explanation.lower()) * 0.2)\n",
    "    \n",
    "#     clarity_score = (length_score + structure_score) / 2\n",
    "#     plausibility_score += clarity_score * 1.5  # 30% weight\n",
    "#     criteria_count += 1\n",
    "    \n",
    "#     # Criterion 2: Logical flow and transitions (25%)\n",
    "#     flow_score = 0.0\n",
    "#     if len(reasoning_steps) > 1:\n",
    "#         # Check for smooth transitions between steps\n",
    "#         transition_quality = 0.0\n",
    "#         transition_words = ['because', 'therefore', 'thus', 'since', 'so', 'hence', \n",
    "#                           'as a result', 'consequently', 'furthermore', 'moreover', 'however']\n",
    "        \n",
    "#         for i in range(len(reasoning_steps) - 1):\n",
    "#             current_step = reasoning_steps[i]\n",
    "#             next_step = reasoning_steps[i + 1]\n",
    "            \n",
    "#             # Check for explicit transition words\n",
    "#             has_transitions = any(word in next_step.lower() for word in transition_words)\n",
    "            \n",
    "#             # Check semantic continuity\n",
    "#             try:\n",
    "#                 semantic_continuity = cosine_similarity(\n",
    "#                     semantic_model.encode([current_step]).reshape(1, -1),\n",
    "#                     semantic_model.encode([next_step]).reshape(1, -1)\n",
    "#                 )[0][0]\n",
    "#             except:\n",
    "#                 semantic_continuity = 0.5  # Default continuity\n",
    "            \n",
    "#             step_flow = (0.4 if has_transitions else 0) + (semantic_continuity * 0.6)\n",
    "#             transition_quality += step_flow\n",
    "        \n",
    "#         flow_score = transition_quality / (len(reasoning_steps) - 1)\n",
    "#     else:\n",
    "#         flow_score = 0.8  # Single step gets moderate score\n",
    "    \n",
    "#     plausibility_score += flow_score * 1.25  # 25% weight\n",
    "#     criteria_count += 1\n",
    "    \n",
    "#     # Criterion 3: Domain knowledge appropriateness (25%)\n",
    "#     domain_score = 0.0\n",
    "    \n",
    "#     # Check for domain-specific terms (Middle Eastern/North African context)\n",
    "#     domain_terms = ['middle east', 'north africa', 'mena', 'arab', 'islamic', 'ottoman', \n",
    "#                    'persian', 'mediterranean', 'gulf', 'levant', 'maghreb', 'arabia']\n",
    "    \n",
    "#     explanation_lower = explanation.lower()\n",
    "#     domain_relevance = sum(1 for term in domain_terms if term in explanation_lower)\n",
    "#     domain_score = min(1.0, domain_relevance * 0.3)\n",
    "    \n",
    "#     # Check for geographical/historical accuracy patterns\n",
    "#     geo_patterns = ['capital', 'country', 'city', 'region', 'border', 'located', 'empire', 'dynasty']\n",
    "#     pattern_usage = sum(1 for pattern in geo_patterns if pattern in explanation_lower)\n",
    "#     pattern_score = min(1.0, pattern_usage * 0.2)\n",
    "    \n",
    "#     domain_score = (domain_score + pattern_score) / 2\n",
    "#     plausibility_score += domain_score * 1.25  # 25% weight\n",
    "#     criteria_count += 1\n",
    "    \n",
    "#     # Criterion 4: Confidence and assertiveness (20%)\n",
    "#     confidence_score = 0.0\n",
    "    \n",
    "#     # Check for confidence indicators\n",
    "#     confident_phrases = ['clearly', 'obviously', 'definitely', 'certainly', 'undoubtedly']\n",
    "#     uncertain_phrases = ['maybe', 'perhaps', 'possibly', 'might', 'could be', 'unclear']\n",
    "    \n",
    "#     confident_count = sum(1 for phrase in confident_phrases if phrase in explanation_lower)\n",
    "#     uncertain_count = sum(1 for phrase in uncertain_phrases if phrase in explanation_lower)\n",
    "    \n",
    "#     # Balanced confidence is good (not overconfident, not too uncertain)\n",
    "#     if confident_count > 0 and uncertain_count == 0:\n",
    "#         confidence_score = 0.9\n",
    "#     elif confident_count == 0 and uncertain_count > 2:\n",
    "#         confidence_score = 0.3\n",
    "#     elif confident_count <= 2 and uncertain_count <= 1:\n",
    "#         confidence_score = 0.8\n",
    "#     else:\n",
    "#         confidence_score = 0.6\n",
    "    \n",
    "#     plausibility_score += confidence_score * 1.0  # 20% weight\n",
    "#     criteria_count += 1\n",
    "    \n",
    "#     # Normalize to 1-5 scale\n",
    "#     if criteria_count > 0:\n",
    "#         normalized_score = (plausibility_score / criteria_count) * 4 + 1\n",
    "#         return min(5.0, max(1.0, normalized_score))\n",
    "    \n",
    "#     return 1.0\n",
    "\n",
    "# def detect_hallucination(faithfulness_score: float, plausibility_score: float) -> bool:\n",
    "#     \"\"\"\n",
    "#     Detect hallucinated reasoning: High plausibility + Low faithfulness\n",
    "#     \"\"\"\n",
    "#     # Hallucination threshold: plausible but not faithful\n",
    "#     plausibility_threshold = 3.5  # Reasonably convincing\n",
    "#     faithfulness_threshold = 2.5  # Poor faithfulness\n",
    "    \n",
    "#     return (plausibility_score >= plausibility_threshold and \n",
    "#             faithfulness_score <= faithfulness_threshold)\n",
    "\n",
    "# # --- COMPREHENSIVE EVALUATION WITH FULL DATASET SUPPORT ---\n",
    "# def run_comprehensive_evaluation(df: pd.DataFrame, model: QwenExplanationModel, \n",
    "#                                 batch_size: int = None, save_progress: bool = True,\n",
    "#                                 checkpoint_interval: int = 100) -> pd.DataFrame:\n",
    "#     \"\"\"Run comprehensive evaluation with full dataset support, batching, and checkpointing.\"\"\"\n",
    "    \n",
    "#     if batch_size is None:\n",
    "#         batch_size = model.batch_size\n",
    "    \n",
    "#     total_questions = len(df)\n",
    "#     print(f\"Starting evaluation of {total_questions} questions...\")\n",
    "#     print(f\"Using batch size: {batch_size}\")\n",
    "    \n",
    "#     # Create results directory for checkpoints\n",
    "#     checkpoint_dir = './evaluation_checkpoints'\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "#     # Check for existing checkpoint\n",
    "#     checkpoint_file = f'{checkpoint_dir}/evaluation_checkpoint.pkl'\n",
    "#     start_idx = 0\n",
    "    \n",
    "#     if os.path.exists(checkpoint_file) and save_progress:\n",
    "#         try:\n",
    "#             checkpoint_df = pd.read_pickle(checkpoint_file)\n",
    "#             # Find where we left off\n",
    "#             completed_mask = (checkpoint_df['model_prediction'] != '') & (checkpoint_df['model_prediction'] != 'Error')\n",
    "#             start_idx = completed_mask.sum()\n",
    "#             if start_idx > 0:\n",
    "#                 df.iloc[:start_idx] = checkpoint_df.iloc[:start_idx]\n",
    "#                 print(f\"Resuming from checkpoint at question {start_idx}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not load checkpoint: {e}\")\n",
    "#             start_idx = 0\n",
    "    \n",
    "#     # Progress tracking\n",
    "#     start_time = time.time()\n",
    "#     processed_count = start_idx\n",
    "    \n",
    "#     # Process in batches\n",
    "#     for batch_start in tqdm(range(start_idx, total_questions, batch_size), \n",
    "#                            desc=\"Processing batches\", \n",
    "#                            initial=start_idx//batch_size):\n",
    "        \n",
    "#         batch_end = min(batch_start + batch_size, total_questions)\n",
    "#         batch_df = df.iloc[batch_start:batch_end]\n",
    "        \n",
    "#         # Extract questions for this batch\n",
    "#         questions = batch_df['question'].tolist()\n",
    "        \n",
    "#         try:\n",
    "#             # Generate explanations for batch\n",
    "#             if len(questions) == 1:\n",
    "#                 # Use single generation for batch size 1\n",
    "#                 results = [model.generate_explanation_single(questions[0])]\n",
    "#             else:\n",
    "#                 # Use batch generation\n",
    "#                 results = model.generate_explanation_batch(questions)\n",
    "            \n",
    "#             # Process results\n",
    "#             for i, (idx, row) in enumerate(batch_df.iterrows()):\n",
    "#                 try:\n",
    "#                     pred_answer, explanation, reasoning_steps = results[i]\n",
    "#                     question = row['question']\n",
    "#                     gold_answer = row['answer']\n",
    "                    \n",
    "#                     # Evaluate faithfulness\n",
    "#                     faithfulness = evaluate_faithfulness(\n",
    "#                         explanation, reasoning_steps, question, pred_answer, model.semantic_model\n",
    "#                     )\n",
    "                    \n",
    "#                     # Evaluate plausibility\n",
    "#                     plausibility = evaluate_plausibility(\n",
    "#                         explanation, reasoning_steps, question, model.semantic_model\n",
    "#                     )\n",
    "                    \n",
    "#                     # Detect hallucination\n",
    "#                     hallucination = detect_hallucination(faithfulness, plausibility)\n",
    "                    \n",
    "#                     # Update dataframe\n",
    "#                     df.at[idx, 'model_prediction'] = pred_answer\n",
    "#                     df.at[idx, 'model_explanation'] = explanation\n",
    "#                     df.at[idx, 'model_reasoning_steps'] = reasoning_steps\n",
    "#                     df.at[idx, 'faithfulness_score'] = faithfulness\n",
    "#                     df.at[idx, 'plausibility_score'] = plausibility\n",
    "#                     df.at[idx, 'hallucination_indicator'] = hallucination\n",
    "                    \n",
    "#                     processed_count += 1\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing question {idx}: {e}\")\n",
    "#                     # Set default values for failed cases\n",
    "#                     df.at[idx, 'model_prediction'] = \"Error\"\n",
    "#                     df.at[idx, 'model_explanation'] = \"Error in generation\"\n",
    "#                     df.at[idx, 'model_reasoning_steps'] = []\n",
    "#                     df.at[idx, 'faithfulness_score'] = 1.0\n",
    "#                     df.at[idx, 'plausibility_score'] = 1.0\n",
    "#                     df.at[idx, 'hallucination_indicator'] = False\n",
    "#                     processed_count += 1\n",
    "#                     continue\n",
    "            \n",
    "#             # Save checkpoint every N questions\n",
    "#             if save_progress and (batch_end % checkpoint_interval == 0 or batch_end == total_questions):\n",
    "#                 try:\n",
    "#                     df.to_pickle(checkpoint_file)\n",
    "#                     elapsed_time = time.time() - start_time\n",
    "#                     rate = processed_count / elapsed_time if elapsed_time > 0 else 0\n",
    "#                     eta = (total_questions - processed_count) / rate if rate > 0 else 0\n",
    "#                     print(f\"Checkpoint saved. Processed: {processed_count}/{total_questions} \"\n",
    "#                           f\"({processed_count/total_questions:.1%}). \"\n",
    "#                           f\"Rate: {rate:.1f} q/s. ETA: {eta/60:.1f} min\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to save checkpoint: {e}\")\n",
    "            \n",
    "#             # Memory cleanup\n",
    "#             if torch.cuda.is_available():\n",
    "#                 torch.cuda.empty_cache()\n",
    "#             gc.collect()\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing batch {batch_start}-{batch_end}: {e}\")\n",
    "#             # Process individually as fallback\n",
    "#             for idx, row in batch_df.iterrows():\n",
    "#                 try:\n",
    "#                     question = row['question']\n",
    "#                     gold_answer = row['answer']\n",
    "                    \n",
    "#                     pred_answer, explanation, reasoning_steps = model.generate_explanation_single(question)\n",
    "                    \n",
    "#                     faithfulness = evaluate_faithfulness(\n",
    "#                         explanation, reasoning_steps, question, pred_answer, model.semantic_model\n",
    "#                     )\n",
    "#                     plausibility = evaluate_plausibility(\n",
    "#                         explanation, reasoning_steps, question, model.semantic_model\n",
    "#                     )\n",
    "#                     hallucination = detect_hallucination(faithfulness, plausibility)\n",
    "                    \n",
    "#                     df.at[idx, 'model_prediction'] = pred_answer\n",
    "#                     df.at[idx, 'model_explanation'] = explanation\n",
    "#                     df.at[idx, 'model_reasoning_steps'] = reasoning_steps\n",
    "#                     df.at[idx, 'faithfulness_score'] = faithfulness\n",
    "#                     df.at[idx, 'plausibility_score'] = plausibility\n",
    "#                     df.at[idx, 'hallucination_indicator'] = hallucination\n",
    "                    \n",
    "#                 except Exception as e2:\n",
    "#                     print(f\"Error processing individual question {idx}: {e2}\")\n",
    "#                     df.at[idx, 'model_prediction'] = \"Error\"\n",
    "#                     df.at[idx, 'model_explanation'] = \"Error in generation\"\n",
    "#                     df.at[idx, 'model_reasoning_steps'] = []\n",
    "#                     df.at[idx, 'faithfulness_score'] = 1.0\n",
    "#                     df.at[idx, 'plausibility_score'] = 1.0\n",
    "#                     df.at[idx, 'hallucination_indicator'] = False\n",
    "                \n",
    "#                 processed_count += 1\n",
    "    \n",
    "#     # Clean up checkpoint file\n",
    "#     if os.path.exists(checkpoint_file):\n",
    "#         os.remove(checkpoint_file)\n",
    "    \n",
    "#     total_time = time.time() - start_time\n",
    "#     print(f\"\\nEvaluation completed!\")\n",
    "#     print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "#     print(f\"Average rate: {total_questions/total_time:.1f} questions/second\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # --- ACCURACY METRICS ---\n",
    "# def compute_exact_match(prediction: str, gold: str) -> int:\n",
    "#     \"\"\"Compute exact match score.\"\"\"\n",
    "#     return int(str(prediction).strip().lower() == str(gold).strip().lower())\n",
    "\n",
    "# def compute_f1(prediction: str, gold: str) -> float:\n",
    "#     \"\"\"Compute F1 score.\"\"\"\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "#     common = set(pred_tokens) & set(gold_tokens)\n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "#     prec = len(common) / len(pred_tokens)\n",
    "#     rec = len(common) / len(gold_tokens)\n",
    "#     if prec + rec == 0:\n",
    "#         return 0.0\n",
    "#     return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "# def compute_bleu(prediction: str, gold: str) -> float:\n",
    "#     \"\"\"Compute BLEU score.\"\"\"\n",
    "#     pred_tokens = str(prediction).lower().split()\n",
    "#     gold_tokens = str(gold).lower().split()\n",
    "#     if not pred_tokens or not gold_tokens:\n",
    "#         return 0.0\n",
    "#     reference = [gold_tokens]\n",
    "#     candidate = pred_tokens\n",
    "#     weights = (0.25, 0.25, 0.25, 0.25)\n",
    "#     chencherry = SmoothingFunction()\n",
    "#     try:\n",
    "#         bleu_score = sentence_bleu(reference, candidate, weights=weights, smoothing_function=chencherry.method1)\n",
    "#     except:\n",
    "#         bleu_score = 0.0\n",
    "#     return bleu_score\n",
    "\n",
    "# def compute_comprehensive_metrics(df: pd.DataFrame) -> Dict:\n",
    "#     \"\"\"Compute all evaluation metrics.\"\"\"\n",
    "    \n",
    "#     # Filter out error cases\n",
    "#     valid_df = df[df['model_prediction'] != 'Error'].copy()\n",
    "#     total_questions = len(df)\n",
    "#     valid_questions = len(valid_df)\n",
    "    \n",
    "#     print(f\"Computing metrics on {valid_questions}/{total_questions} valid predictions \"\n",
    "#           f\"({valid_questions/total_questions:.1%})\")\n",
    "    \n",
    "#     if valid_questions == 0:\n",
    "#         print(\"No valid predictions found!\")\n",
    "#         return {}\n",
    "    \n",
    "#     # Basic accuracy metrics\n",
    "#     em_scores = [compute_exact_match(row['model_prediction'], row['answer']) \n",
    "#                 for _, row in valid_df.iterrows()]\n",
    "#     f1_scores = [compute_f1(row['model_prediction'], row['answer']) \n",
    "#                 for _, row in valid_df.iterrows()]\n",
    "#     bleu_scores = [compute_bleu(row['model_prediction'], row['answer']) \n",
    "#                   for _, row in valid_df.iterrows()]\n",
    "    \n",
    "#     # Explanation quality metrics\n",
    "#     faithfulness_scores = valid_df['faithfulness_score'].tolist()\n",
    "#     plausibility_scores = valid_df['plausibility_score'].tolist()\n",
    "#     hallucination_rates = valid_df['hallucination_indicator'].tolist()\n",
    "    \n",
    "#     # Hop-wise analysis\n",
    "#     hop_wise_results = {}\n",
    "#     for hop_cat in valid_df['hop_category'].unique():\n",
    "#         if pd.isna(hop_cat):\n",
    "#             continue\n",
    "        \n",
    "#         subset = valid_df[valid_df['hop_category'] == hop_cat]\n",
    "#         if len(subset) == 0:\n",
    "#             continue\n",
    "            \n",
    "#                 hop_wise_results[str(hop_cat)] = {\n",
    "#             'count': len(subset),\n",
    "#             'EM': np.mean([compute_exact_match(row['model_prediction'], row['answer']) \n",
    "#                           for _, row in subset.iterrows()]),\n",
    "#             'F1': np.mean([compute_f1(row['model_prediction'], row['answer']) \n",
    "#                           for _, row in subset.iterrows()]),\n",
    "#             'BLEU': np.mean([compute_bleu(row['model_prediction'], row['answer']) \n",
    "#                             for _, row in subset.iterrows()]),\n",
    "#             'Faithfulness': np.mean(subset['faithfulness_score']),\n",
    "#             'Plausibility': np.mean(subset['plausibility_score']),\n",
    "#             'Hallucination_Rate': np.mean(subset['hallucination_indicator'])\n",
    "#         }\n",
    "    \n",
    "#     # Question type analysis (if available)\n",
    "#     type_wise_results = {}\n",
    "#     if 'type' in valid_df.columns:\n",
    "#         for q_type in valid_df['type'].unique():\n",
    "#             if pd.isna(q_type) or q_type == '':\n",
    "#                 continue\n",
    "            \n",
    "#             subset = valid_df[valid_df['type'] == q_type]\n",
    "#             if len(subset) == 0:\n",
    "#                 continue\n",
    "                \n",
    "#             type_wise_results[str(q_type)] = {\n",
    "#                 'count': len(subset),\n",
    "#                 'EM': np.mean([compute_exact_match(row['model_prediction'], row['answer']) \n",
    "#                               for _, row in subset.iterrows()]),\n",
    "#                 'F1': np.mean([compute_f1(row['model_prediction'], row['answer']) \n",
    "#                               for _, row in subset.iterrows()]),\n",
    "#                 'BLEU': np.mean([compute_bleu(row['model_prediction'], row['answer']) \n",
    "#                                 for _, row in subset.iterrows()]),\n",
    "#                 'Faithfulness': np.mean(subset['faithfulness_score']),\n",
    "#                 'Plausibility': np.mean(subset['plausibility_score']),\n",
    "#                 'Hallucination_Rate': np.mean(subset['hallucination_indicator'])\n",
    "#             }\n",
    "    \n",
    "#     # Correlation analysis\n",
    "#     try:\n",
    "#         faithfulness_plausibility_corr = pearsonr(faithfulness_scores, plausibility_scores)[0]\n",
    "#         faithfulness_accuracy_corr = pearsonr(faithfulness_scores, f1_scores)[0]\n",
    "#         plausibility_accuracy_corr = pearsonr(plausibility_scores, f1_scores)[0]\n",
    "#     except:\n",
    "#         faithfulness_plausibility_corr = 0.0\n",
    "#         faithfulness_accuracy_corr = 0.0\n",
    "#         plausibility_accuracy_corr = 0.0\n",
    "    \n",
    "#     return {\n",
    "#         'overall_metrics': {\n",
    "#             'Exact_Match': np.mean(em_scores),\n",
    "#             'F1_Score': np.mean(f1_scores),\n",
    "#             'BLEU_Score': np.mean(bleu_scores),\n",
    "#             'Faithfulness': np.mean(faithfulness_scores),\n",
    "#             'Plausibility': np.mean(plausibility_scores),\n",
    "#             'Hallucination_Rate': np.mean(hallucination_rates),\n",
    "#             'Total_Questions': total_questions,\n",
    "#             'Valid_Questions': valid_questions,\n",
    "#             'Success_Rate': valid_questions / total_questions\n",
    "#         },\n",
    "#         'hop_wise_metrics': hop_wise_results,\n",
    "#         'type_wise_metrics': type_wise_results,\n",
    "#         'correlations': {\n",
    "#             'Faithfulness_Plausibility': faithfulness_plausibility_corr,\n",
    "#             'Faithfulness_Accuracy': faithfulness_accuracy_corr,\n",
    "#             'Plausibility_Accuracy': plausibility_accuracy_corr\n",
    "#         },\n",
    "#         'detailed_scores': {\n",
    "#             'faithfulness_scores': faithfulness_scores,\n",
    "#             'plausibility_scores': plausibility_scores,\n",
    "#             'hallucination_indicators': hallucination_rates,\n",
    "#             'accuracy_scores': f1_scores,\n",
    "#             'em_scores': em_scores,\n",
    "#             'bleu_scores': bleu_scores\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "# # --- ADVANCED VISUALIZATIONS ---\n",
    "# def create_comprehensive_visualizations(results: Dict, save_dir: str = './results'):\n",
    "#     \"\"\"Create comprehensive visualizations for all metrics.\"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # Set up the plotting style\n",
    "#     plt.rcParams['figure.figsize'] = (12, 8)\n",
    "#     plt.rcParams['font.size'] = 10\n",
    "    \n",
    "#     # 1. Overall Performance Dashboard\n",
    "#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "#     # Overall metrics bar chart\n",
    "#     overall = results['overall_metrics']\n",
    "#     metrics = ['Exact_Match', 'F1_Score', 'BLEU_Score', 'Faithfulness', 'Plausibility']\n",
    "#     values = [overall[metric] for metric in metrics]\n",
    "#     colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "    \n",
    "#     bars = ax1.bar(metrics, values, color=colors, alpha=0.8)\n",
    "#     ax1.set_ylabel('Score', fontweight='bold')\n",
    "#     ax1.set_title(f'Overall Performance Metrics\\n(n={overall[\"Valid_Questions\"]}/{overall[\"Total_Questions\"]} valid)', \n",
    "#                   fontweight='bold', fontsize=14)\n",
    "#     ax1.set_ylim(0, max(5, max(values) * 1.1))\n",
    "    \n",
    "#     # Add value labels on bars\n",
    "#     for bar, value in zip(bars, values):\n",
    "#         height = bar.get_height()\n",
    "#         ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "#                 f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "#     ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "#     # Faithfulness vs Plausibility scatter plot\n",
    "#     faith_scores = results['detailed_scores']['faithfulness_scores']\n",
    "#     plaus_scores = results['detailed_scores']['plausibility_scores']\n",
    "#     halluc_indicators = results['detailed_scores']['hallucination_indicators']\n",
    "    \n",
    "#     # Color points by hallucination status\n",
    "#     colors_scatter = ['red' if h else 'blue' for h in halluc_indicators]\n",
    "#     scatter = ax2.scatter(faith_scores, plaus_scores, c=colors_scatter, alpha=0.6)\n",
    "#     ax2.set_xlabel('Faithfulness Score', fontweight='bold')\n",
    "#     ax2.set_ylabel('Plausibility Score', fontweight='bold')\n",
    "#     ax2.set_title('Faithfulness vs Plausibility\\n(Red = Hallucination)', fontweight='bold')\n",
    "#     ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add diagonal line and quadrant labels\n",
    "#     ax2.plot([1, 5], [1, 5], 'k--', alpha=0.5)\n",
    "#     ax2.text(4.5, 2, 'High Plausibility\\nLow Faithfulness\\n(Hallucination Zone)', \n",
    "#              bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"red\", alpha=0.3))\n",
    "    \n",
    "#     # Hop-wise performance\n",
    "#     hop_data = results['hop_wise_metrics']\n",
    "#     if hop_data:\n",
    "#         hop_categories = list(hop_data.keys())\n",
    "#         hop_f1 = [hop_data[cat]['F1'] for cat in hop_categories]\n",
    "#         hop_faith = [hop_data[cat]['Faithfulness'] for cat in hop_categories]\n",
    "#         hop_plaus = [hop_data[cat]['Plausibility'] for cat in hop_categories]\n",
    "        \n",
    "#         x = np.arange(len(hop_categories))\n",
    "#         width = 0.25\n",
    "        \n",
    "#         ax3.bar(x - width, hop_f1, width, label='F1 Score', color='#3498db', alpha=0.8)\n",
    "#         ax3.bar(x, hop_faith, width, label='Faithfulness', color='#e74c3c', alpha=0.8)\n",
    "#         ax3.bar(x + width, hop_plaus, width, label='Plausibility', color='#9b59b6', alpha=0.8)\n",
    "        \n",
    "#         ax3.set_xlabel('Reasoning Complexity', fontweight='bold')\n",
    "#         ax3.set_ylabel('Score', fontweight='bold')\n",
    "#         ax3.set_title('Performance by Reasoning Complexity', fontweight='bold')\n",
    "#         ax3.set_xticks(x)\n",
    "#         ax3.set_xticklabels(hop_categories)\n",
    "#         ax3.legend()\n",
    "#         ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "#         # Hallucination rate by complexity\n",
    "#         hop_halluc = [hop_data[cat]['Hallucination_Rate'] * 100 for cat in hop_categories]\n",
    "        \n",
    "#         bars_halluc = ax4.bar(hop_categories, hop_halluc, color='#e74c3c', alpha=0.7)\n",
    "#         ax4.set_ylabel('Hallucination Rate (%)', fontweight='bold')\n",
    "#         ax4.set_xlabel('Reasoning Complexity', fontweight='bold')\n",
    "#         ax4.set_title('Hallucination Rate by Complexity', fontweight='bold')\n",
    "#         ax4.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "#         # Add percentage labels\n",
    "#         for bar, rate in zip(bars_halluc, hop_halluc):\n",
    "#             height = bar.get_height()\n",
    "#             ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "#                     f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{save_dir}/comprehensive_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 2. Question Type Analysis (if available)\n",
    "#     type_data = results.get('type_wise_metrics', {})\n",
    "#     if type_data:\n",
    "#         fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "#         type_categories = list(type_data.keys())\n",
    "#         type_counts = [type_data[cat]['count'] for cat in type_categories]\n",
    "#         type_f1 = [type_data[cat]['F1'] for cat in type_categories]\n",
    "#         type_faith = [type_data[cat]['Faithfulness'] for cat in type_categories]\n",
    "#         type_plaus = [type_data[cat]['Plausibility'] for cat in type_categories]\n",
    "#         type_halluc = [type_data[cat]['Hallucination_Rate'] * 100 for cat in type_categories]\n",
    "        \n",
    "#         # Question type distribution\n",
    "#         ax1.pie(type_counts, labels=type_categories, autopct='%1.1f%%', startangle=90)\n",
    "#         ax1.set_title('Question Type Distribution', fontweight='bold')\n",
    "        \n",
    "#         # Performance by type\n",
    "#         x = np.arange(len(type_categories))\n",
    "#         width = 0.25\n",
    "        \n",
    "#         ax2.bar(x - width, type_f1, width, label='F1 Score', color='#3498db', alpha=0.8)\n",
    "#         ax2.bar(x, type_faith, width, label='Faithfulness', color='#e74c3c', alpha=0.8)\n",
    "#         ax2.bar(x + width, type_plaus, width, label='Plausibility', color='#9b59b6', alpha=0.8)\n",
    "        \n",
    "#         ax2.set_xlabel('Question Type', fontweight='bold')\n",
    "#         ax2.set_ylabel('Score', fontweight='bold')\n",
    "#         ax2.set_title('Performance by Question Type', fontweight='bold')\n",
    "#         ax2.set_xticks(x)\n",
    "#         ax2.set_xticklabels(type_categories, rotation=45)\n",
    "#         ax2.legend()\n",
    "#         ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "#         # Hallucination by type\n",
    "#         bars_type_halluc = ax3.bar(type_categories, type_halluc, color='#e74c3c', alpha=0.7)\n",
    "#         ax3.set_ylabel('Hallucination Rate (%)', fontweight='bold')\n",
    "#         ax3.set_xlabel('Question Type', fontweight='bold')\n",
    "#         ax3.set_title('Hallucination Rate by Question Type', fontweight='bold')\n",
    "#         ax3.tick_params(axis='x', rotation=45)\n",
    "#         ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "#         # Add percentage labels\n",
    "#         for bar, rate in zip(bars_type_halluc, type_halluc):\n",
    "#             height = bar.get_height()\n",
    "#             ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "#                     f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "#         # Sample counts by type\n",
    "#         ax4.bar(type_categories, type_counts, color='#2ecc71', alpha=0.7)\n",
    "#         ax4.set_ylabel('Number of Questions', fontweight='bold')\n",
    "#         ax4.set_xlabel('Question Type', fontweight='bold')\n",
    "#         ax4.set_title('Sample Size by Question Type', fontweight='bold')\n",
    "#         ax4.tick_params(axis='x', rotation=45)\n",
    "#         ax4.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "#         # Add count labels\n",
    "#         for bar, count in zip(ax4.patches, type_counts):\n",
    "#             height = bar.get_height()\n",
    "#             ax4.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "#                     f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'{save_dir}/question_type_analysis.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "    \n",
    "#     # 3. Correlation Heatmap\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "    \n",
    "#     # Create correlation matrix\n",
    "#     corr_data = {\n",
    "#         'Faithfulness': faith_scores,\n",
    "#         'Plausibility': plaus_scores,\n",
    "#         'F1_Score': results['detailed_scores']['accuracy_scores'],\n",
    "#         'EM_Score': results['detailed_scores']['em_scores'],\n",
    "#         'BLEU_Score': results['detailed_scores']['bleu_scores'],\n",
    "#         'Hallucination': [int(h) for h in halluc_indicators]\n",
    "#     }\n",
    "    \n",
    "#     corr_df = pd.DataFrame(corr_data)\n",
    "#     correlation_matrix = corr_df.corr()\n",
    "    \n",
    "#     # Create heatmap\n",
    "#     mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "#     sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', \n",
    "#                 center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "#     plt.title('Correlation Matrix: Explanation Quality vs Performance', \n",
    "#               fontweight='bold', fontsize=14)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{save_dir}/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 4. Distribution Analysis\n",
    "#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "#     # Faithfulness distribution\n",
    "#     ax1.hist(faith_scores, bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "#     ax1.axvline(np.mean(faith_scores), color='blue', linestyle='--', \n",
    "#                 label=f'Mean: {np.mean(faith_scores):.2f}')\n",
    "#     ax1.axvline(np.median(faith_scores), color='green', linestyle='--', \n",
    "#                 label=f'Median: {np.median(faith_scores):.2f}')\n",
    "#     ax1.set_xlabel('Faithfulness Score')\n",
    "#     ax1.set_ylabel('Frequency')\n",
    "#     ax1.set_title('Distribution of Faithfulness Scores', fontweight='bold')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "#     # Plausibility distribution\n",
    "#     ax2.hist(plaus_scores, bins=30, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "#     ax2.axvline(np.mean(plaus_scores), color='blue', linestyle='--', \n",
    "#                 label=f'Mean: {np.mean(plaus_scores):.2f}')\n",
    "#     ax2.axvline(np.median(plaus_scores), color='green', linestyle='--', \n",
    "#                 label=f'Median: {np.median(plaus_scores):.2f}')\n",
    "#     ax2.set_xlabel('Plausibility Score')\n",
    "#     ax2.set_ylabel('Frequency')\n",
    "#     ax2.set_title('Distribution of Plausibility Scores', fontweight='bold')\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "#     # F1 Score distribution\n",
    "#     f1_scores = results['detailed_scores']['accuracy_scores']\n",
    "#     ax3.hist(f1_scores, bins=30, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "#     ax3.axvline(np.mean(f1_scores), color='blue', linestyle='--', \n",
    "#                 label=f'Mean: {np.mean(f1_scores):.2f}')\n",
    "#     ax3.axvline(np.median(f1_scores), color='green', linestyle='--', \n",
    "#                 label=f'Median: {np.median(f1_scores):.2f}')\n",
    "#     ax3.set_xlabel('F1 Score')\n",
    "#     ax3.set_ylabel('Frequency')\n",
    "#     ax3.set_title('Distribution of F1 Scores', fontweight='bold')\n",
    "#     ax3.legend()\n",
    "#     ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "#     # Hallucination analysis\n",
    "#     halluc_counts = [sum(halluc_indicators), len(halluc_indicators) - sum(halluc_indicators)]\n",
    "#     labels = ['Hallucinated', 'Non-Hallucinated']\n",
    "#     colors_pie = ['#e74c3c', '#2ecc71']\n",
    "    \n",
    "#     wedges, texts, autotexts = ax4.pie(halluc_counts, labels=labels, colors=colors_pie, \n",
    "#                                       autopct='%1.1f%%', startangle=90)\n",
    "#     ax4.set_title('Hallucination Distribution', fontweight='bold')\n",
    "    \n",
    "#     # Make percentage text bold\n",
    "#     for autotext in autotexts:\n",
    "#         autotext.set_color('white')\n",
    "#         autotext.set_fontweight('bold')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{save_dir}/distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 5. Performance vs Quality Scatter Plots\n",
    "#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "#     # F1 vs Faithfulness\n",
    "#     ax1.scatter(faith_scores, f1_scores, alpha=0.6, color='#3498db')\n",
    "#     ax1.set_xlabel('Faithfulness Score')\n",
    "#     ax1.set_ylabel('F1 Score')\n",
    "#     ax1.set_title('F1 Score vs Faithfulness')\n",
    "#     ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add correlation coefficient\n",
    "#     corr_f1_faith = np.corrcoef(faith_scores, f1_scores)[0, 1]\n",
    "#     ax1.text(0.05, 0.95, f'r = {corr_f1_faith:.3f}', transform=ax1.transAxes, \n",
    "#              bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "#     # F1 vs Plausibility\n",
    "#     ax2.scatter(plaus_scores, f1_scores, alpha=0.6, color='#e74c3c')\n",
    "#     ax2.set_xlabel('Plausibility Score')\n",
    "#     ax2.set_ylabel('F1 Score')\n",
    "#     ax2.set_title('F1 Score vs Plausibility')\n",
    "#     ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "#         # Add correlation coefficient\n",
    "#     corr_f1_plaus = np.corrcoef(plaus_scores, f1_scores)[0, 1]\n",
    "#     ax2.text(0.05, 0.95, f'r = {corr_f1_plaus:.3f}', transform=ax2.transAxes, \n",
    "#              bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "#     # EM vs Faithfulness\n",
    "#     em_scores = results['detailed_scores']['em_scores']\n",
    "#     ax3.scatter(faith_scores, em_scores, alpha=0.6, color='#f39c12')\n",
    "#     ax3.set_xlabel('Faithfulness Score')\n",
    "#     ax3.set_ylabel('Exact Match Score')\n",
    "#     ax3.set_title('Exact Match vs Faithfulness')\n",
    "#     ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add correlation coefficient\n",
    "#     corr_em_faith = np.corrcoef(faith_scores, em_scores)[0, 1]\n",
    "#     ax3.text(0.05, 0.95, f'r = {corr_em_faith:.3f}', transform=ax3.transAxes, \n",
    "#              bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "#     # BLEU vs Plausibility\n",
    "#     bleu_scores = results['detailed_scores']['bleu_scores']\n",
    "#     ax4.scatter(plaus_scores, bleu_scores, alpha=0.6, color='#9b59b6')\n",
    "#     ax4.set_xlabel('Plausibility Score')\n",
    "#     ax4.set_ylabel('BLEU Score')\n",
    "#     ax4.set_title('BLEU Score vs Plausibility')\n",
    "#     ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add correlation coefficient\n",
    "#     corr_bleu_plaus = np.corrcoef(plaus_scores, bleu_scores)[0, 1]\n",
    "#     ax4.text(0.05, 0.95, f'r = {corr_bleu_plaus:.3f}', transform=ax4.transAxes, \n",
    "#              bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{save_dir}/performance_quality_scatter.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 6. Detailed Performance Analysis by Question Complexity\n",
    "#     if hop_data:\n",
    "#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "#         # Performance comparison: Hallucinated vs Non-Hallucinated by complexity\n",
    "#         halluc_by_hop = {}\n",
    "#         non_halluc_by_hop = {}\n",
    "        \n",
    "#         for hop_cat in hop_data.keys():\n",
    "#             # Get data for this complexity level\n",
    "#             hop_faith = []\n",
    "#             hop_f1 = []\n",
    "#             hop_halluc = []\n",
    "            \n",
    "#             # This would need the original dataframe to extract properly\n",
    "#             # For now, we'll create a simplified version\n",
    "            \n",
    "#         # Box plots for comparison\n",
    "#         faith_data_halluc = [f for f, h in zip(faith_scores, halluc_indicators) if h]\n",
    "#         faith_data_non_halluc = [f for f, h in zip(faith_scores, halluc_indicators) if not h]\n",
    "#         f1_data_halluc = [f for f, h in zip(f1_scores, halluc_indicators) if h]\n",
    "#         f1_data_non_halluc = [f for f, h in zip(f1_scores, halluc_indicators) if not h]\n",
    "        \n",
    "#         faith_data = [faith_data_non_halluc, faith_data_halluc]\n",
    "#         f1_data = [f1_data_non_halluc, f1_data_halluc]\n",
    "        \n",
    "#         bp1 = ax1.boxplot(faith_data, labels=['Non-Hallucinated', 'Hallucinated'], \n",
    "#                          patch_artist=True)\n",
    "#         bp1['boxes'][0].set_facecolor('#2ecc71')\n",
    "#         bp1['boxes'][1].set_facecolor('#e74c3c')\n",
    "#         ax1.set_ylabel('Faithfulness Score')\n",
    "#         ax1.set_title('Faithfulness: Hallucinated vs Non-Hallucinated', fontweight='bold')\n",
    "#         ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "#         bp2 = ax2.boxplot(f1_data, labels=['Non-Hallucinated', 'Hallucinated'], \n",
    "#                          patch_artist=True)\n",
    "#         bp2['boxes'][0].set_facecolor('#2ecc71')\n",
    "#         bp2['boxes'][1].set_facecolor('#e74c3c')\n",
    "#         ax2.set_ylabel('F1 Score')\n",
    "#         ax2.set_title('Accuracy: Hallucinated vs Non-Hallucinated', fontweight='bold')\n",
    "#         ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'{save_dir}/hallucination_comparison.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "\n",
    "# def print_comprehensive_results(results: Dict, model_name: str):\n",
    "#     \"\"\"Print detailed results summary.\"\"\"\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(f\"COMPREHENSIVE EVALUATION RESULTS - {model_name}\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     overall = results['overall_metrics']\n",
    "#     print(f\"\\nüìä OVERALL PERFORMANCE:\")\n",
    "#     print(f\"   ‚Ä¢ Total Questions Evaluated: {overall['Total_Questions']}\")\n",
    "#     print(f\"   ‚Ä¢ Valid Predictions: {overall['Valid_Questions']} ({overall['Success_Rate']:.1%})\")\n",
    "#     print(f\"   ‚Ä¢ Exact Match: {overall['Exact_Match']:.3f}\")\n",
    "#     print(f\"   ‚Ä¢ F1 Score: {overall['F1_Score']:.3f}\")\n",
    "#     print(f\"   ‚Ä¢ BLEU Score: {overall['BLEU_Score']:.3f}\")\n",
    "    \n",
    "#     print(f\"\\nüîç EXPLANATION QUALITY:\")\n",
    "#     print(f\"   ‚Ä¢ Average Faithfulness: {overall['Faithfulness']:.3f}/5.0\")\n",
    "#     print(f\"   ‚Ä¢ Average Plausibility: {overall['Plausibility']:.3f}/5.0\")\n",
    "#     print(f\"   ‚Ä¢ Hallucination Rate: {overall['Hallucination_Rate']:.1%}\")\n",
    "    \n",
    "#     print(f\"\\nüìà CORRELATIONS:\")\n",
    "#     corr = results['correlations']\n",
    "#     print(f\"   ‚Ä¢ Faithfulness ‚Üî Plausibility: {corr['Faithfulness_Plausibility']:.3f}\")\n",
    "#     print(f\"   ‚Ä¢ Faithfulness ‚Üî Accuracy: {corr['Faithfulness_Accuracy']:.3f}\")\n",
    "#     print(f\"   ‚Ä¢ Plausibility ‚Üî Accuracy: {corr['Plausibility_Accuracy']:.3f}\")\n",
    "    \n",
    "#     print(f\"\\nüéØ PERFORMANCE BY REASONING COMPLEXITY:\")\n",
    "#     hop_metrics = results['hop_wise_metrics']\n",
    "#     for hop_cat, metrics in hop_metrics.items():\n",
    "#         print(f\"\\n   {hop_cat} Questions (n={metrics['count']}):\")\n",
    "#         print(f\"      ‚îú‚îÄ Exact Match: {metrics['EM']:.3f}\")\n",
    "#         print(f\"      ‚îú‚îÄ F1 Score: {metrics['F1']:.3f}\")\n",
    "#         print(f\"      ‚îú‚îÄ BLEU Score: {metrics['BLEU']:.3f}\")\n",
    "#         print(f\"      ‚îú‚îÄ Faithfulness: {metrics['Faithfulness']:.3f}\")\n",
    "#         print(f\"      ‚îú‚îÄ Plausibility: {metrics['Plausibility']:.3f}\")\n",
    "#         print(f\"      ‚îî‚îÄ Hallucination Rate: {metrics['Hallucination_Rate']:.1%}\")\n",
    "    \n",
    "#     # Question type analysis\n",
    "#     type_metrics = results.get('type_wise_metrics', {})\n",
    "#     if type_metrics:\n",
    "#         print(f\"\\nüìù PERFORMANCE BY QUESTION TYPE:\")\n",
    "#         for q_type, metrics in type_metrics.items():\n",
    "#             print(f\"\\n   {q_type} Questions (n={metrics['count']}):\")\n",
    "#             print(f\"      ‚îú‚îÄ Exact Match: {metrics['EM']:.3f}\")\n",
    "#             print(f\"      ‚îú‚îÄ F1 Score: {metrics['F1']:.3f}\")\n",
    "#             print(f\"      ‚îú‚îÄ BLEU Score: {metrics['BLEU']:.3f}\")\n",
    "#             print(f\"      ‚îú‚îÄ Faithfulness: {metrics['Faithfulness']:.3f}\")\n",
    "#             print(f\"      ‚îú‚îÄ Plausibility: {metrics['Plausibility']:.3f}\")\n",
    "#             print(f\"      ‚îî‚îÄ Hallucination Rate: {metrics['Hallucination_Rate']:.1%}\")\n",
    "    \n",
    "#     # Statistical summary\n",
    "#     detailed = results['detailed_scores']\n",
    "#     print(f\"\\nüìä STATISTICAL SUMMARY:\")\n",
    "#     print(f\"   Faithfulness: Œº={np.mean(detailed['faithfulness_scores']):.3f}, \"\n",
    "#           f\"œÉ={np.std(detailed['faithfulness_scores']):.3f}, \"\n",
    "#           f\"median={np.median(detailed['faithfulness_scores']):.3f}\")\n",
    "#     print(f\"   Plausibility: Œº={np.mean(detailed['plausibility_scores']):.3f}, \"\n",
    "#           f\"œÉ={np.std(detailed['plausibility_scores']):.3f}, \"\n",
    "#           f\"median={np.median(detailed['plausibility_scores']):.3f}\")\n",
    "#     print(f\"   F1 Score: Œº={np.mean(detailed['accuracy_scores']):.3f}, \"\n",
    "#           f\"œÉ={np.std(detailed['accuracy_scores']):.3f}, \"\n",
    "#           f\"median={np.median(detailed['accuracy_scores']):.3f}\")\n",
    "    \n",
    "#     # Interpretation\n",
    "#     print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "#     if overall['Hallucination_Rate'] > 0.3:\n",
    "#         print(f\"   üö® Very high hallucination rate detected ({overall['Hallucination_Rate']:.1%})\")\n",
    "#     elif overall['Hallucination_Rate'] > 0.2:\n",
    "#         print(f\"   ‚ö†Ô∏è  High hallucination rate detected ({overall['Hallucination_Rate']:.1%})\")\n",
    "#     elif overall['Hallucination_Rate'] > 0.1:\n",
    "#         print(f\"   ‚ö†Ô∏è  Moderate hallucination rate ({overall['Hallucination_Rate']:.1%})\")\n",
    "#     else:\n",
    "#         print(f\"   ‚úÖ Low hallucination rate ({overall['Hallucination_Rate']:.1%})\")\n",
    "    \n",
    "#     if corr['Faithfulness_Plausibility'] < 0.3:\n",
    "#         print(f\"   ‚ö†Ô∏è  Very low correlation between faithfulness and plausibility\")\n",
    "#     elif corr['Faithfulness_Plausibility'] < 0.5:\n",
    "#         print(f\"   ‚ö†Ô∏è  Low correlation between faithfulness and plausibility\")\n",
    "    \n",
    "#     if overall['Faithfulness'] < 2.5:\n",
    "#         print(f\"   üö® Very low faithfulness scores indicate serious reasoning issues\")\n",
    "#     elif overall['Faithfulness'] < 3.0:\n",
    "#         print(f\"   ‚ö†Ô∏è  Low faithfulness scores indicate reasoning issues\")\n",
    "    \n",
    "#     if overall['Plausibility'] > 3.5 and overall['Faithfulness'] < 3.0:\n",
    "#         print(f\"   üö® High plausibility with low faithfulness suggests systematic hallucination\")\n",
    "    \n",
    "#     if overall['Success_Rate'] < 0.8:\n",
    "#         print(f\"   ‚ö†Ô∏è  Low success rate ({overall['Success_Rate']:.1%}) - many generation failures\")\n",
    "    \n",
    "#     # Performance trends\n",
    "#     if hop_metrics:\n",
    "#         hop_f1_trend = [hop_metrics[cat]['F1'] for cat in sorted(hop_metrics.keys())]\n",
    "#         if len(hop_f1_trend) > 1:\n",
    "#             if hop_f1_trend[-1] < hop_f1_trend[0] * 0.8:\n",
    "#                 print(f\"   üìâ Performance degrades significantly with complexity\")\n",
    "#             elif hop_f1_trend[-1] > hop_f1_trend[0] * 1.1:\n",
    "#                 print(f\"   üìà Performance surprisingly improves with complexity\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# # --- ENHANCED MAIN EXECUTION ---\n",
    "# def main(max_questions: int = None, batch_size: int = 8, model_name: str = \"Qwen/Qwen2-0.5B\"):\n",
    "#     \"\"\"Run the complete evaluation pipeline with full dataset support.\"\"\"\n",
    "#     print(\"üöÄ MenatQA Comprehensive Evaluation with Full Dataset Support\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Load dataset\n",
    "#     print(\"\\n1Ô∏è‚É£ Loading MenatQA dataset...\")\n",
    "#     try:\n",
    "#         data = load_menatqa_dataset('./MenatQA.json')\n",
    "#         df = preprocess_dataset(data)\n",
    "#         print(f\"‚úÖ Successfully loaded and preprocessed {len(df)} questions\")\n",
    "        \n",
    "#         if max_questions and max_questions < len(df):\n",
    "#             print(f\"üìä Limiting evaluation to {max_questions} questions for testing\")\n",
    "#             df = df.sample(n=max_questions, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error loading dataset: {e}\")\n",
    "#         print(\"üìù Creating sample dataset for demonstration...\")\n",
    "#         sample_data = [\n",
    "#             {'ID': 1, 'question': 'What is the capital of Egypt?', 'answer': 'Cairo', 'type': 'geography'},\n",
    "#             {'ID': 2, 'question': 'Which empire controlled most of the Middle East in the 16th century?', 'answer': 'Ottoman Empire', 'type': 'history'},\n",
    "#             {'ID': 3, 'question': 'What language is primarily spoken in Morocco?', 'answer': 'Arabic', 'type': 'culture'},\n",
    "#             {'ID': 4, 'question': 'Which country borders both Iraq and Iran?', 'answer': 'Turkey', 'type': 'geography'},\n",
    "#             {'ID': 5, 'question': 'When did the Suez Canal open?', 'answer': '1869', 'type': 'history'}\n",
    "#         ]\n",
    "#         df = preprocess_dataset(sample_data)\n",
    "    \n",
    "#     # Initialize model\n",
    "#     print(f\"\\n2Ô∏è‚É£ Initializing {model_name}...\")\n",
    "#     try:\n",
    "#         model = QwenExplanationModel(model_name, batch_size=batch_size)\n",
    "#         print(f\"‚úÖ Successfully loaded {model.model_name}\")\n",
    "#         print(f\"üîß Using batch size: {batch_size}\")\n",
    "#         if torch.cuda.is_available():\n",
    "#             print(f\"üöÄ CUDA available - using GPU acceleration\")\n",
    "#         else:\n",
    "#             print(f\"‚ö° Using CPU (consider GPU for faster processing)\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error loading model: {e}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     # Run evaluation\n",
    "#     print(f\"\\n3Ô∏è‚É£ Running comprehensive evaluation on {len(df)} questions...\")\n",
    "#     print(f\"‚è±Ô∏è  Estimated time: {len(df) * 3 / 60:.1f} minutes (rough estimate)\")\n",
    "    \n",
    "#     try:\n",
    "#         df_results = run_comprehensive_evaluation(\n",
    "#             df, model, \n",
    "#             batch_size=batch_size, \n",
    "#             save_progress=True,\n",
    "#             checkpoint_interval=50\n",
    "#         )\n",
    "#         print(\"‚úÖ Evaluation completed successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error during evaluation: {e}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     # Compute metrics\n",
    "#     print(\"\\n4Ô∏è‚É£ Computing comprehensive metrics...\")\n",
    "#     try:\n",
    "#         results = compute_comprehensive_metrics(df_results)\n",
    "#         print(\"‚úÖ Metrics computation completed!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error computing metrics: {e}\")\n",
    "#         return df_results, None\n",
    "    \n",
    "#     # Generate visualizations\n",
    "#     print(\"\\n5Ô∏è‚É£ Generating visualizations...\")\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     results_dir = f'./menatqa_full_results_{timestamp}'\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         create_comprehensive_visualizations(results, results_dir)\n",
    "#         print(\"‚úÖ Visualizations generated successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  Error generating visualizations: {e}\")\n",
    "    \n",
    "#     # Print results\n",
    "#     print_comprehensive_results(results, model.model_name)\n",
    "    \n",
    "#     # Save detailed results\n",
    "#     print(f\"\\n6Ô∏è‚É£ Saving detailed results to {results_dir}...\")\n",
    "    \n",
    "#     try:\n",
    "#         # Save metrics\n",
    "#         with open(f'{results_dir}/comprehensive_metrics.json', 'w') as f:\n",
    "#             # Convert numpy types to native Python types for JSON serialization\n",
    "#             json_results = {}\n",
    "#             for key, value in results.items():\n",
    "#                 if key == 'detailed_scores':\n",
    "#                     json_results[key] = {k: [float(x) if not isinstance(x, bool) else x for x in v] \n",
    "#                                        for k, v in value.items()}\n",
    "#                 elif key == 'overall_metrics':\n",
    "#                     json_results[key] = {k: float(v) if not isinstance(v, (int, str)) else v \n",
    "#                                        for k, v in value.items()}\n",
    "#                 elif key in ['hop_wise_metrics', 'type_wise_metrics']:\n",
    "#                     json_results[key] = {k: {k2: float(v2) if not isinstance(v2, (int, str)) else v2 \n",
    "#                                            for k2, v2 in v.items()} \n",
    "#                                        for k, v in value.items()}\n",
    "#                 elif key == 'correlations':\n",
    "#                     json_results[key] = {k: float(v) for k, v in value.items()}\n",
    "#                 else:\n",
    "#                     json_results[key] = value\n",
    "            \n",
    "#             json.dump(json_results, f, indent=2)\n",
    "        \n",
    "#                 # Save detailed examples (ALL examples for complete analysis)\n",
    "#         examples_for_inspection = []\n",
    "#         print(f\"üìÑ Saving detailed results for all {len(df_results)} examples...\")\n",
    "        \n",
    "#         for idx, row in df_results.iterrows():\n",
    "#             example = {\n",
    "#                 'ID': row.get('ID', idx),\n",
    "#                 'question': row['question'],\n",
    "#                 'gold_answer': row['answer'],\n",
    "#                 'predicted_answer': row['model_prediction'],\n",
    "#                 'explanation': row['model_explanation'],\n",
    "#                 'reasoning_steps': row['model_reasoning_steps'],\n",
    "#                 'faithfulness_score': float(row['faithfulness_score']),\n",
    "#                 'plausibility_score': float(row['plausibility_score']),\n",
    "#                 'hallucination_indicator': bool(row['hallucination_indicator']),\n",
    "#                 'hop_category': str(row['hop_category']),\n",
    "#                 'hop_count': int(row['hop_count']),\n",
    "#                 'question_type': row.get('type', 'unknown'),\n",
    "#                 'time_scope': row.get('time_scope', ''),\n",
    "#                 'exact_match': compute_exact_match(row['model_prediction'], row['answer']),\n",
    "#                 'f1_score': compute_f1(row['model_prediction'], row['answer']),\n",
    "#                 'bleu_score': compute_bleu(row['model_prediction'], row['answer'])\n",
    "#             }\n",
    "#             examples_for_inspection.append(example)\n",
    "        \n",
    "#         # Save all examples\n",
    "#         with open(f'{results_dir}/all_example_evaluations.json', 'w') as f:\n",
    "#             json.dump(examples_for_inspection, f, indent=2)\n",
    "        \n",
    "#         # Save a subset of best and worst examples for quick inspection\n",
    "#         df_results_copy = df_results.copy()\n",
    "#         df_results_copy = df_results_copy[df_results_copy['model_prediction'] != 'Error']\n",
    "        \n",
    "#         if len(df_results_copy) > 0:\n",
    "#             # Best examples (high faithfulness and accuracy)\n",
    "#             df_results_copy['combined_score'] = (\n",
    "#                 df_results_copy['faithfulness_score'] + \n",
    "#                 df_results_copy.apply(lambda x: compute_f1(x['model_prediction'], x['answer']), axis=1)\n",
    "#             ) / 2\n",
    "            \n",
    "#             best_examples = df_results_copy.nlargest(10, 'combined_score')\n",
    "#             worst_examples = df_results_copy.nsmallest(10, 'combined_score')\n",
    "#             hallucinated_examples = df_results_copy[df_results_copy['hallucination_indicator'] == True].head(10)\n",
    "            \n",
    "#             inspection_subsets = {\n",
    "#                 'best_examples': [],\n",
    "#                 'worst_examples': [],\n",
    "#                 'hallucinated_examples': []\n",
    "#             }\n",
    "            \n",
    "#             for subset_name, subset_df in [\n",
    "#                 ('best_examples', best_examples),\n",
    "#                 ('worst_examples', worst_examples), \n",
    "#                 ('hallucinated_examples', hallucinated_examples)\n",
    "#             ]:\n",
    "#                 for idx, row in subset_df.iterrows():\n",
    "#                     example = {\n",
    "#                         'ID': row.get('ID', idx),\n",
    "#                         'question': row['question'],\n",
    "#                         'gold_answer': row['answer'],\n",
    "#                         'predicted_answer': row['model_prediction'],\n",
    "#                         'explanation': row['model_explanation'],\n",
    "#                         'reasoning_steps': row['model_reasoning_steps'],\n",
    "#                         'faithfulness_score': float(row['faithfulness_score']),\n",
    "#                         'plausibility_score': float(row['plausibility_score']),\n",
    "#                         'hallucination_indicator': bool(row['hallucination_indicator']),\n",
    "#                         'hop_category': str(row['hop_category']),\n",
    "#                         'question_type': row.get('type', 'unknown'),\n",
    "#                         'exact_match': compute_exact_match(row['model_prediction'], row['answer']),\n",
    "#                         'f1_score': compute_f1(row['model_prediction'], row['answer']),\n",
    "#                         'bleu_score': compute_bleu(row['model_prediction'], row['answer'])\n",
    "#                     }\n",
    "#                     inspection_subsets[subset_name].append(example)\n",
    "            \n",
    "#             with open(f'{results_dir}/example_subsets_for_inspection.json', 'w') as f:\n",
    "#                 json.dump(inspection_subsets, f, indent=2)\n",
    "        \n",
    "#         # Save evaluation DataFrame\n",
    "#         df_results.to_csv(f'{results_dir}/full_evaluation_results.csv', index=False)\n",
    "#         df_results.to_pickle(f'{results_dir}/full_evaluation_results.pkl')\n",
    "        \n",
    "#         # Save summary statistics\n",
    "#         summary_stats = {\n",
    "#             'evaluation_info': {\n",
    "#                 'model_name': model.model_name,\n",
    "#                 'total_questions': len(df_results),\n",
    "#                 'evaluation_date': timestamp,\n",
    "#                 'batch_size': batch_size,\n",
    "#                 'evaluation_time_minutes': (time.time() - start_time) / 60\n",
    "#             },\n",
    "#             'performance_summary': results['overall_metrics'],\n",
    "#             'correlation_summary': results['correlations']\n",
    "#         }\n",
    "        \n",
    "#         with open(f'{results_dir}/evaluation_summary.json', 'w') as f:\n",
    "#             json.dump(summary_stats, f, indent=2)\n",
    "        \n",
    "#         print(\"‚úÖ All results saved successfully!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  Error saving results: {e}\")\n",
    "    \n",
    "#     # Generate detailed report\n",
    "#     print(f\"\\n7Ô∏è‚É£ Generating detailed evaluation report...\")\n",
    "#     try:\n",
    "#         generate_detailed_report(results, df_results, model.model_name, results_dir)\n",
    "#         print(\"‚úÖ Detailed report generated!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  Error generating report: {e}\")\n",
    "    \n",
    "#     total_time = time.time() - start_time\n",
    "#     print(f\"\\n‚úÖ Evaluation complete! Total time: {total_time/60:.1f} minutes\")\n",
    "#     print(f\"üìÅ Results saved to: {results_dir}\")\n",
    "#     print(f\"üìä Files generated:\")\n",
    "#     print(f\"   ‚Ä¢ comprehensive_dashboard.png - Main performance overview\")\n",
    "#     print(f\"   ‚Ä¢ correlation_heatmap.png - Correlation analysis\") \n",
    "#     print(f\"   ‚Ä¢ distribution_analysis.png - Score distributions\")\n",
    "#     print(f\"   ‚Ä¢ performance_quality_scatter.png - Quality vs performance plots\")\n",
    "#     print(f\"   ‚Ä¢ hallucination_comparison.png - Hallucination analysis\")\n",
    "#     if results.get('type_wise_metrics'):\n",
    "#         print(f\"   ‚Ä¢ question_type_analysis.png - Analysis by question type\")\n",
    "#     print(f\"   ‚Ä¢ comprehensive_metrics.json - All computed metrics\")\n",
    "#     print(f\"   ‚Ä¢ all_example_evaluations.json - Complete results for all examples\")\n",
    "#     print(f\"   ‚Ä¢ example_subsets_for_inspection.json - Best/worst/hallucinated examples\")\n",
    "#     print(f\"   ‚Ä¢ full_evaluation_results.csv - Complete results in CSV format\")\n",
    "#     print(f\"   ‚Ä¢ full_evaluation_results.pkl - Complete results in pickle format\")\n",
    "#     print(f\"   ‚Ä¢ evaluation_summary.json - High-level summary\")\n",
    "#     print(f\"   ‚Ä¢ detailed_evaluation_report.txt - Human-readable report\")\n",
    "    \n",
    "#     return results, df_results\n",
    "\n",
    "# def generate_detailed_report(results: Dict, df_results: pd.DataFrame, model_name: str, save_dir: str):\n",
    "#     \"\"\"Generate a comprehensive human-readable evaluation report.\"\"\"\n",
    "    \n",
    "#     report_path = f'{save_dir}/detailed_evaluation_report.txt'\n",
    "    \n",
    "#     with open(report_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(\"=\" * 100 + \"\\n\")\n",
    "#         f.write(f\"COMPREHENSIVE MENATQA EVALUATION REPORT\\n\")\n",
    "#         f.write(f\"Model: {model_name}\\n\")\n",
    "#         f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "#         f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "        \n",
    "#         # Executive Summary\n",
    "#         overall = results['overall_metrics']\n",
    "#         f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")\n",
    "#         f.write(f\"Total Questions Evaluated: {overall['Total_Questions']}\\n\")\n",
    "#         f.write(f\"Successful Evaluations: {overall['Valid_Questions']} ({overall['Success_Rate']:.1%})\\n\")\n",
    "#         f.write(f\"Overall Accuracy (F1): {overall['F1_Score']:.3f}\\n\")\n",
    "#         f.write(f\"Explanation Faithfulness: {overall['Faithfulness']:.2f}/5.0\\n\")\n",
    "#         f.write(f\"Explanation Plausibility: {overall['Plausibility']:.2f}/5.0\\n\")\n",
    "#         f.write(f\"Hallucination Rate: {overall['Hallucination_Rate']:.1%}\\n\\n\")\n",
    "        \n",
    "#         # Detailed Performance Metrics\n",
    "#         f.write(\"DETAILED PERFORMANCE METRICS\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")\n",
    "#         f.write(f\"Exact Match Score: {overall['Exact_Match']:.3f}\\n\")\n",
    "#         f.write(f\"F1 Score: {overall['F1_Score']:.3f}\\n\")\n",
    "#         f.write(f\"BLEU Score: {overall['BLEU_Score']:.3f}\\n\")\n",
    "#         f.write(f\"Average Faithfulness: {overall['Faithfulness']:.3f} ¬± {np.std(results['detailed_scores']['faithfulness_scores']):.3f}\\n\")\n",
    "#         f.write(f\"Average Plausibility: {overall['Plausibility']:.3f} ¬± {np.std(results['detailed_scores']['plausibility_scores']):.3f}\\n\\n\")\n",
    "        \n",
    "#         # Correlation Analysis\n",
    "#         f.write(\"CORRELATION ANALYSIS\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")\n",
    "#         corr = results['correlations']\n",
    "#         f.write(f\"Faithfulness ‚Üî Plausibility: {corr['Faithfulness_Plausibility']:.3f}\\n\")\n",
    "#         f.write(f\"Faithfulness ‚Üî F1 Score: {corr['Faithfulness_Accuracy']:.3f}\\n\")\n",
    "#         f.write(f\"Plausibility ‚Üî F1 Score: {corr['Plausibility_Accuracy']:.3f}\\n\\n\")\n",
    "        \n",
    "#         # Performance by Complexity\n",
    "#         f.write(\"PERFORMANCE BY REASONING COMPLEXITY\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")\n",
    "#         hop_metrics = results['hop_wise_metrics']\n",
    "#         for hop_cat, metrics in hop_metrics.items():\n",
    "#             f.write(f\"\\n{hop_cat.upper()} QUESTIONS (n={metrics['count']}):\\n\")\n",
    "#             f.write(f\"  Exact Match: {metrics['EM']:.3f}\\n\")\n",
    "#             f.write(f\"  F1 Score: {metrics['F1']:.3f}\\n\")\n",
    "#             f.write(f\"  BLEU Score: {metrics['BLEU']:.3f}\\n\")\n",
    "#             f.write(f\"  Faithfulness: {metrics['Faithfulness']:.3f}\\n\")\n",
    "#             f.write(f\"  Plausibility: {metrics['Plausibility']:.3f}\\n\")\n",
    "#             f.write(f\"  Hallucination Rate: {metrics['Hallucination_Rate']:.1%}\\n\")\n",
    "        \n",
    "#         # Performance by Question Type\n",
    "#         type_metrics = results.get('type_wise_metrics', {})\n",
    "#         if type_metrics:\n",
    "#             f.write(f\"\\nPERFORMANCE BY QUESTION TYPE\\n\")\n",
    "#             f.write(\"-\" * 50 + \"\\n\")\n",
    "#             for q_type, metrics in type_metrics.items():\n",
    "#                 f.write(f\"\\n{q_type.upper()} QUESTIONS (n={metrics['count']}):\\n\")\n",
    "#                 f.write(f\"  Exact Match: {metrics['EM']:.3f}\\n\")\n",
    "#                 f.write(f\"  F1 Score: {metrics['F1']:.3f}\\n\")\n",
    "#                 f.write(f\"  BLEU Score: {metrics['BLEU']:.3f}\\n\")\n",
    "#                 f.write(f\"  Faithfulness: {metrics['Faithfulness']:.3f}\\n\")\n",
    "#                 f.write(f\"  Plausibility: {metrics['Plausibility']:.3f}\\n\")\n",
    "#                 f.write(f\"  Hallucination Rate: {metrics['Hallucination_Rate']:.1%}\\n\")\n",
    "        \n",
    "#         # Statistical Analysis\n",
    "#         f.write(f\"\\nSTATISTICAL ANALYSIS\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")\n",
    "#         detailed = results['detailed_scores']\n",
    "        \n",
    "#         f.write(\"Score Distributions:\\n\")\n",
    "#         f.write(f\"  Faithfulness: Œº={np.mean(detailed['faithfulness_scores']):.3f}, \"\n",
    "#                 f\"œÉ={np.std(detailed['faithfulness_scores']):.3f}, \"\n",
    "#                 f\"median={np.median(detailed['faithfulness_scores']):.3f}\\n\")\n",
    "#         f.write(f\"  Plausibility: Œº={np.mean(detailed['plausibility_scores']):.3f}, \"\n",
    "#                 f\"œÉ={np.std(detailed['plausibility_scores']):.3f}, \"\n",
    "#                 f\"median={np.median(detailed['plausibility_scores']):.3f}\\n\")\n",
    "#         f.write(f\"  F1 Scores: Œº={np.mean(detailed['accuracy_scores']):.3f}, \"\n",
    "#                 f\"œÉ={np.std(detailed['accuracy_scores']):.3f}, \"\n",
    "#                 f\"median={np.median(detailed['accuracy_scores']):.3f}\\n\")\n",
    "#         f.write(f\"  BLEU Scores: Œº={np.mean(detailed['bleu_scores']):.3f}, \"\n",
    "#                 f\"œÉ={np.std(detailed['bleu_scores']):.3f}, \"\n",
    "#                 f\"median={np.median(detailed['bleu_scores']):.3f}\\n\")\n",
    "        \n",
    "#         # Key Findings and Recommendations\n",
    "#         f.write(f\"\\nKEY FINDINGS AND RECOMMENDATIONS\\n\")\n",
    "#         f.write(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "#         # Performance Assessment\n",
    "#         if overall['F1_Score'] > 0.8:\n",
    "#             f.write(\"‚úÖ EXCELLENT: High accuracy performance\\n\")\n",
    "#         elif overall['F1_Score'] > 0.6:\n",
    "#             f.write(\"‚úÖ GOOD: Satisfactory accuracy performance\\n\")\n",
    "#         elif overall['F1_Score'] > 0.4:\n",
    "#             f.write(\"‚ö†Ô∏è  MODERATE: Room for improvement in accuracy\\n\")\n",
    "#         else:\n",
    "#             f.write(\"‚ùå POOR: Significant accuracy issues\\n\")\n",
    "        \n",
    "#         # Explanation Quality Assessment\n",
    "#         if overall['Faithfulness'] > 4.0:\n",
    "#             f.write(\"‚úÖ EXCELLENT: High-quality, faithful explanations\\n\")\n",
    "#         elif overall['Faithfulness'] > 3.0:\n",
    "#             f.write(\"‚úÖ GOOD: Generally faithful explanations\\n\")\n",
    "#         elif overall['Faithfulness'] > 2.0:\n",
    "#             f.write(\"‚ö†Ô∏è  MODERATE: Some faithfulness issues in explanations\\n\")\n",
    "#         else:\n",
    "#             f.write(\"‚ùå POOR: Significant faithfulness problems\\n\")\n",
    "        \n",
    "#         # Hallucination Assessment\n",
    "#         if overall['Hallucination_Rate'] < 0.1:\n",
    "#             f.write(\"‚úÖ EXCELLENT: Very low hallucination rate\\n\")\n",
    "#         elif overall['Hallucination_Rate'] < 0.2:\n",
    "#             f.write(\"‚úÖ GOOD: Low hallucination rate\\n\")\n",
    "#         elif overall['Hallucination_Rate'] < 0.3:\n",
    "#             f.write(\"‚ö†Ô∏è  MODERATE: Notable hallucination rate\\n\")\n",
    "#         else:\n",
    "#             f.write(\"‚ùå CONCERNING: High hallucination rate\\n\")\n",
    "        \n",
    "#         # Specific Recommendations\n",
    "#         f.write(\"\\nSPECIFIC RECOMMENDATIONS:\\n\")\n",
    "        \n",
    "#         if overall['Hallucination_Rate'] > 0.2:\n",
    "#             f.write(\"‚Ä¢ Address high hallucination rate through improved training or filtering\\n\")\n",
    "        \n",
    "#         if corr['Faithfulness_Plausibility'] < 0.3:\n",
    "#             f.write(\"‚Ä¢ Low faithfulness-plausibility correlation suggests explanation quality issues\\n\")\n",
    "        \n",
    "#         if overall['Success_Rate'] < 0.9:\n",
    "#             f.write(\"‚Ä¢ Improve model robustness to reduce generation failures\\n\")\n",
    "        \n",
    "#         # Performance trends across complexity\n",
    "#         if hop_metrics:\n",
    "#             hop_f1_values = [(cat, metrics['F1']) for cat, metrics in hop_metrics.items()]\n",
    "#             hop_f1_values.sort(key=lambda x: x[0])\n",
    "#             if len(hop_f1_values) > 1:\n",
    "#                 if hop_f1_values[-1][1] < hop_f1_values[0][1] * 0.7:\n",
    "#                     f.write(\"‚Ä¢ Performance degrades significantly with increased reasoning complexity\\n\")\n",
    "#                     f.write(\"‚Ä¢ Consider specialized training for multi-hop reasoning\\n\")\n",
    "        \n",
    "#         if corr['Faithfulness_Accuracy'] < 0.3:\n",
    "#             f.write(\"‚Ä¢ Low correlation between faithfulness and accuracy suggests explanation-answer misalignment\\n\")\n",
    "        \n",
    "#         # Error Analysis\n",
    "#         error_count = overall['Total_Questions'] - overall['Valid_Questions']\n",
    "#         if error_count > 0:\n",
    "#             error_rate = error_count / overall['Total_Questions']\n",
    "#             f.write(f\"\\nERROR ANALYSIS:\\n\")\n",
    "#             f.write(f\"‚Ä¢ {error_count} questions ({error_rate:.1%}) resulted in generation errors\\n\")\n",
    "#             f.write(f\"‚Ä¢ Consider investigating common failure patterns\\n\")\n",
    "        \n",
    "#         f.write(f\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "#         f.write(f\"End of Report\\n\")\n",
    "#         f.write(f\"=\" * 100 + \"\\n\")\n",
    "\n",
    "# # --- ADDITIONAL UTILITY FUNCTIONS ---\n",
    "# def analyze_failure_patterns(df_results: pd.DataFrame, save_dir: str):\n",
    "#     \"\"\"Analyze patterns in failed predictions.\"\"\"\n",
    "    \n",
    "#     # Separate successful vs failed predictions\n",
    "#     failed_df = df_results[df_results['model_prediction'] == 'Error']\n",
    "#     successful_df = df_results[df_results['model_prediction'] != 'Error']\n",
    "    \n",
    "#     if len(failed_df) == 0:\n",
    "#         print(\"No failed predictions to analyze.\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"\\nAnalyzing {len(failed_df)} failed predictions...\")\n",
    "    \n",
    "#     # Analyze failure patterns\n",
    "#     failure_analysis = {\n",
    "#         'total_failures': len(failed_df),\n",
    "#         'failure_rate': len(failed_df) / len(df_results),\n",
    "#         'failures_by_hop_category': failed_df['hop_category'].value_counts().to_dict(),\n",
    "#         'failures_by_type': failed_df.get('type', pd.Series()).value_counts().to_dict(),\n",
    "#                 'avg_question_length': failed_df['question'].str.len().mean(),\n",
    "#         'successful_avg_question_length': successful_df['question'].str.len().mean() if len(successful_df) > 0 else 0\n",
    "#     }\n",
    "    \n",
    "#     # Save failure analysis\n",
    "#     with open(f'{save_dir}/failure_analysis.json', 'w') as f:\n",
    "#         json.dump(failure_analysis, f, indent=2)\n",
    "    \n",
    "#     print(f\"Failure analysis saved to {save_dir}/failure_analysis.json\")\n",
    "\n",
    "# def create_performance_comparison_chart(results: Dict, save_dir: str):\n",
    "#     \"\"\"Create a comprehensive performance comparison chart.\"\"\"\n",
    "    \n",
    "#     plt.figure(figsize=(15, 10))\n",
    "    \n",
    "#     # Create a radar chart for overall performance\n",
    "#     categories = ['Accuracy\\n(F1)', 'Exactness\\n(EM)', 'Fluency\\n(BLEU)', \n",
    "#                   'Faithfulness', 'Plausibility', 'Reliability\\n(1-Halluc.)']\n",
    "    \n",
    "#     overall = results['overall_metrics']\n",
    "#     values = [\n",
    "#         overall['F1_Score'],\n",
    "#         overall['Exact_Match'], \n",
    "#         overall['BLEU_Score'],\n",
    "#         overall['Faithfulness'] / 5.0,  # Normalize to 0-1\n",
    "#         overall['Plausibility'] / 5.0,  # Normalize to 0-1\n",
    "#         1 - overall['Hallucination_Rate']  # Reliability = 1 - hallucination rate\n",
    "#     ]\n",
    "    \n",
    "#     # Number of variables\n",
    "#     N = len(categories)\n",
    "    \n",
    "#     # Compute angle for each axis\n",
    "#     angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "#     angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "#     # Add values to complete the circle\n",
    "#     values += values[:1]\n",
    "    \n",
    "#     # Initialize the plot\n",
    "#     ax = plt.subplot(111, projection='polar')\n",
    "    \n",
    "#     # Draw the plot\n",
    "#     ax.plot(angles, values, 'o-', linewidth=2, label='Model Performance', color='#2E86AB')\n",
    "#     ax.fill(angles, values, alpha=0.25, color='#2E86AB')\n",
    "    \n",
    "#     # Add category labels\n",
    "#     ax.set_xticks(angles[:-1])\n",
    "#     ax.set_xticklabels(categories)\n",
    "    \n",
    "#     # Set y-axis limits\n",
    "#     ax.set_ylim(0, 1)\n",
    "#     ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "#     ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])\n",
    "#     ax.grid(True)\n",
    "    \n",
    "#     # Add title\n",
    "#     plt.title('Comprehensive Model Performance Profile', size=16, fontweight='bold', pad=20)\n",
    "    \n",
    "#     # Add value annotations\n",
    "#     for angle, value, category in zip(angles[:-1], values[:-1], categories):\n",
    "#         ax.annotate(f'{value:.3f}', \n",
    "#                    xy=(angle, value), \n",
    "#                    xytext=(10, 10), \n",
    "#                    textcoords='offset points',\n",
    "#                    fontsize=9,\n",
    "#                    fontweight='bold')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{save_dir}/performance_radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "# def save_model_configuration(model: QwenExplanationModel, save_dir: str):\n",
    "#     \"\"\"Save model configuration and system information.\"\"\"\n",
    "    \n",
    "#     config_info = {\n",
    "#         'model_info': {\n",
    "#             'model_name': model.model_name,\n",
    "#             'batch_size': model.batch_size,\n",
    "#             'model_type': str(type(model.model)),\n",
    "#             'tokenizer_type': str(type(model.tokenizer))\n",
    "#         },\n",
    "#         'system_info': {\n",
    "#             'cuda_available': torch.cuda.is_available(),\n",
    "#             'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "#             'python_version': '.'.join(map(str, __import__('sys').version_info[:3])),\n",
    "#             'torch_version': torch.__version__,\n",
    "#             'transformers_version': __import__('transformers').__version__\n",
    "#         },\n",
    "#         'evaluation_settings': {\n",
    "#             'max_new_tokens': 512,\n",
    "#             'temperature': 0.1,\n",
    "#             'do_sample': True\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     if torch.cuda.is_available():\n",
    "#         config_info['system_info']['cuda_device_name'] = torch.cuda.get_device_name(0)\n",
    "#         config_info['system_info']['cuda_memory_total'] = torch.cuda.get_device_properties(0).total_memory\n",
    "    \n",
    "#     with open(f'{save_dir}/model_configuration.json', 'w') as f:\n",
    "#         json.dump(config_info, f, indent=2)\n",
    "\n",
    "# # --- ENHANCED MAIN EXECUTION WITH FULL FEATURES ---\n",
    "# def main(max_questions: int = None, batch_size: int = 8, model_name: str = \"Qwen/Qwen2-0.5B\"):\n",
    "#     \"\"\"Run the complete evaluation pipeline with full dataset support.\"\"\"\n",
    "#     print(\"üöÄ MenatQA Comprehensive Evaluation with Full Dataset Support\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Load dataset\n",
    "#     print(\"\\n1Ô∏è‚É£ Loading MenatQA dataset...\")\n",
    "#     try:\n",
    "#         data = load_menatqa_dataset('./MenatQA.json')\n",
    "#         df = preprocess_dataset(data)\n",
    "#         print(f\"‚úÖ Successfully loaded and preprocessed {len(df)} questions\")\n",
    "        \n",
    "#         if max_questions and max_questions < len(df):\n",
    "#             print(f\"üìä Limiting evaluation to {max_questions} questions for testing\")\n",
    "#             df = df.sample(n=max_questions, random_state=42).reset_index(drop=True)\n",
    "#         else:\n",
    "#             print(f\"üìä Evaluating on full dataset: {len(df)} questions\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error loading dataset: {e}\")\n",
    "#         print(\"üìù Creating sample dataset for demonstration...\")\n",
    "#         sample_data = [\n",
    "#             {'ID': 1, 'question': 'What is the capital of Egypt?', 'answer': 'Cairo', 'type': 'geography'},\n",
    "#             {'ID': 2, 'question': 'Which empire controlled most of the Middle East in the 16th century?', 'answer': 'Ottoman Empire', 'type': 'history'},\n",
    "#             {'ID': 3, 'question': 'What language is primarily spoken in Morocco?', 'answer': 'Arabic', 'type': 'culture'},\n",
    "#             {'ID': 4, 'question': 'Which country borders both Iraq and Iran?', 'answer': 'Turkey', 'type': 'geography'},\n",
    "#             {'ID': 5, 'question': 'When did the Suez Canal open?', 'answer': '1869', 'type': 'history'}\n",
    "#         ]\n",
    "#         df = preprocess_dataset(sample_data)\n",
    "    \n",
    "#     # Initialize model\n",
    "#     print(f\"\\n2Ô∏è‚É£ Initializing {model_name}...\")\n",
    "#     try:\n",
    "#         model = QwenExplanationModel(model_name, batch_size=batch_size)\n",
    "#         print(f\"‚úÖ Successfully loaded {model.model_name}\")\n",
    "#         print(f\"üîß Using batch size: {batch_size}\")\n",
    "#         if torch.cuda.is_available():\n",
    "#             print(f\"üöÄ CUDA available - using GPU acceleration\")\n",
    "#             print(f\"üî• GPU: {torch.cuda.get_device_name(0)}\")\n",
    "#         else:\n",
    "#             print(f\"‚ö° Using CPU (consider GPU for faster processing)\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error loading model: {e}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     # Run evaluation\n",
    "#     print(f\"\\n3Ô∏è‚É£ Running comprehensive evaluation on {len(df)} questions...\")\n",
    "#     estimated_time = len(df) * 3 / 60  # Rough estimate: 3 seconds per question\n",
    "#     print(f\"‚è±Ô∏è  Estimated time: {estimated_time:.1f} minutes\")\n",
    "#     print(f\"üíæ Progress will be saved every 50 questions\")\n",
    "    \n",
    "#     try:\n",
    "#         df_results = run_comprehensive_evaluation(\n",
    "#             df, model, \n",
    "#             batch_size=batch_size, \n",
    "#             save_progress=True,\n",
    "#             checkpoint_interval=50\n",
    "#         )\n",
    "#         print(\"‚úÖ Evaluation completed successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error during evaluation: {e}\")\n",
    "#         return None, None\n",
    "    \n",
    "#     # Compute metrics\n",
    "#     print(\"\\n4Ô∏è‚É£ Computing comprehensive metrics...\")\n",
    "#     try:\n",
    "#         results = compute_comprehensive_metrics(df_results)\n",
    "#         print(\"‚úÖ Metrics computation completed!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error computing metrics: {e}\")\n",
    "#         return df_results, None\n",
    "    \n",
    "#     # Generate visualizations\n",
    "#     print(\"\\n5Ô∏è‚É£ Generating visualizations...\")\n",
    "#     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     results_dir = f'./menatqa_full_results_{timestamp}'\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         create_comprehensive_visualizations(results, results_dir)\n",
    "#         create_performance_comparison_chart(results, results_dir)\n",
    "#         print(\"‚úÖ Visualizations generated successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  Error generating visualizations: {e}\")\n",
    "    \n",
    "#     # Print results\n",
    "#     print_comprehensive_results(results, model.model_name)\n",
    "    \n",
    "#     # Save detailed results\n",
    "#     print(f\"\\n6Ô∏è‚É£ Saving detailed results to {results_dir}...\")\n",
    "    \n",
    "#     try:\n",
    "#         # Save metrics\n",
    "#         with open(f'{results_dir}/comprehensive_metrics.json', 'w') as f:\n",
    "#             # Convert numpy types to native Python types for JSON serialization\n",
    "#             json_results = {}\n",
    "#             for key, value in results.items():\n",
    "#                 if key == 'detailed_scores':\n",
    "#                     json_results[key] = {k: [float(x) if not isinstance(x, bool) else x for x in v] \n",
    "#                                        for k, v in value.items()}\n",
    "#                 elif key == 'overall_metrics':\n",
    "#                     json_results[key] = {k: float(v) if not isinstance(v, (int, str)) else v \n",
    "#                                        for k, v in value.items()}\n",
    "#                 elif key in ['hop_wise_metrics', 'type_wise_metrics']:\n",
    "#                     json_results[key] = {k: {k2: float(v2) if not isinstance(v2, (int, str)) else v2 \n",
    "#                                            for k2, v2 in v.items()} \n",
    "#                                        for k, v in value.items()}\n",
    "#                 elif key == 'correlations':\n",
    "#                     json_results[key] = {k: float(v) for k, v in value.items()}\n",
    "#                 else:\n",
    "#                     json_results[key] = value\n",
    "            \n",
    "#             json.dump(json_results, f, indent=2)\n",
    "        \n",
    "#         # Save detailed examples (ALL examples)\n",
    "#         examples_for_inspection = []\n",
    "#         print(f\"üìÑ Saving detailed results for all {len(df_results)} examples...\")\n",
    "        \n",
    "#         for idx, row in df_results.iterrows():\n",
    "#             example = {\n",
    "#                 'ID': row.get('ID', idx),\n",
    "#                 'question': row['question'],\n",
    "#                 'gold_answer': row['answer'],\n",
    "#                 'predicted_answer': row['model_prediction'],\n",
    "#                 'explanation': row['model_explanation'],\n",
    "#                 'reasoning_steps': row['model_reasoning_steps'],\n",
    "#                 'faithfulness_score': float(row['faithfulness_score']),\n",
    "#                 'plausibility_score': float(row['plausibility_score']),\n",
    "#                 'hallucination_indicator': bool(row['hallucination_indicator']),\n",
    "#                 'hop_category': str(row['hop_category']),\n",
    "#                 'hop_count': int(row['hop_count']),\n",
    "#                 'question_type': row.get('type', 'unknown'),\n",
    "#                 'time_scope': row.get('time_scope', ''),\n",
    "#                 'exact_match': compute_exact_match(row['model_prediction'], row['answer']),\n",
    "#                 'f1_score': compute_f1(row['model_prediction'], row['answer']),\n",
    "#                 'bleu_score': compute_bleu(row['model_prediction'], row['answer'])\n",
    "#             }\n",
    "#             examples_for_inspection.append(example)\n",
    "        \n",
    "#         # Save all examples\n",
    "#         with open(f'{results_dir}/all_example_evaluations.json', 'w') as f:\n",
    "#             json.dump(examples_for_inspection, f, indent=2)\n",
    "        \n",
    "#         # Save a subset of best and worst examples for quick inspection\n",
    "#         df_results_copy = df_results.copy()\n",
    "#         df_results_copy = df_results_copy[df_results_copy['model_prediction'] != 'Error']\n",
    "        \n",
    "#         if len(df_results_copy) > 0:\n",
    "#             # Best examples (high faithfulness and accuracy)\n",
    "#             df_results_copy['combined_score'] = (\n",
    "#                 df_results_copy['faithfulness_score'] + \n",
    "#                 df_results_copy.apply(lambda x: compute_f1(x['model_prediction'], x['answer']), axis=1)\n",
    "#             ) / 2\n",
    "            \n",
    "#             best_examples = df_results_copy.nlargest(10, 'combined_score')\n",
    "#             worst_examples = df_results_copy.nsmallest(10, 'combined_score')\n",
    "#             hallucinated_examples = df_results_copy[df_results_copy['hallucination_indicator'] == True].head(10)\n",
    "            \n",
    "#             inspection_subsets = {\n",
    "#                 'best_examples': [],\n",
    "#                 'worst_examples': [],\n",
    "#                 'hallucinated_examples': []\n",
    "#             }\n",
    "            \n",
    "#             for subset_name, subset_df in [\n",
    "#                 ('best_examples', best_examples),\n",
    "#                 ('worst_examples', worst_examples), \n",
    "#                 ('hallucinated_examples', hallucinated_examples)\n",
    "#             ]:\n",
    "#                 for idx, row in subset_df.iterrows():\n",
    "#                     example = {\n",
    "#                         'ID': row.get('ID', idx),\n",
    "#                         'question': row['question'],\n",
    "#                         'gold_answer': row['answer'],\n",
    "#                         'predicted_answer': row['model_prediction'],\n",
    "#                         'explanation': row['model_explanation'],\n",
    "#                         'reasoning_steps': row['model_reasoning_steps'],\n",
    "#                         'faithfulness_score': float(row['faithfulness_score']),\n",
    "#                         'plausibility_score': float(row['plausibility_score']),\n",
    "#                         'hallucination_indicator': bool(row['hallucination_indicator']),\n",
    "#                         'hop_category': str(row['hop_category']),\n",
    "#                         'question_type': row.get('type', 'unknown'),\n",
    "#                         'exact_match': compute_exact_match(row['model_prediction'], row['answer']),\n",
    "#                         'f1_score': compute_f1(row['model_prediction'], row['answer']),\n",
    "#                         'bleu_score': compute_bleu(row['model_prediction'], row['answer'])\n",
    "#                     }\n",
    "#                     inspection_subsets[subset_name].append(example)\n",
    "            \n",
    "#             with open(f'{results_dir}/example_subsets_for_inspection.json', 'w') as f:\n",
    "#                 json.dump(inspection_subsets, f, indent=2)\n",
    "        \n",
    "#         # Save evaluation DataFrame\n",
    "#         df_results.to_csv(f'{results_dir}/full_evaluation_results.csv', index=False)\n",
    "#         df_results.to_pickle(f'{results_dir}/full_evaluation_results.pkl')\n",
    "        \n",
    "#         # Save summary statistics\n",
    "#         summary_stats = {\n",
    "#             'evaluation_info': {\n",
    "#                 'model_name': model.model_name,\n",
    "#                 'total_questions': len(df_results),\n",
    "#                 'evaluation_date': timestamp,\n",
    "#                 'batch_size': batch_size,\n",
    "#                 'evaluation_time_minutes': (time.time() - start_time) / 60\n",
    "#             },\n",
    "#             'performance_summary': results['overall_metrics'],\n",
    "#             'correlation_summary': results['correlations']\n",
    "#         }\n",
    "        \n",
    "#         with open(f'{results_dir}/evaluation_summary.json', 'w') as f:\n",
    "#             json.dump(summary_stats, f, indent=2)\n",
    "        \n",
    "#         # Save model configuration\n",
    "#         save_model_configuration(model, results_dir)\n",
    "        \n",
    "#         # Analyze failure patterns\n",
    "#         analyze_failure_patterns(df_results, results_dir)\n",
    "        \n",
    "#         print(\"‚úÖ All results saved successfully!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  Error saving results: {e}\")\n",
    "    \n",
    "#     # Generate detailed report\n",
    "#     print(f\"\\n7Ô∏è‚É£ Generating detailed evaluation report...\")\n",
    "#     try:\n",
    "#         generate_detailed_report(results, df_results, model.model_name, results_dir)\n",
    "#         print(\"‚úÖ Detailed report generated!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è  Error generating report: {e}\")\n",
    "    \n",
    "#     total_time = time.time() - start_time\n",
    "#     print(f\"\\n‚úÖ Evaluation complete! Total time: {total_time/60:.1f} minutes\")\n",
    "#     print(f\"üìÅ Results saved to: {results_dir}\")\n",
    "#     print(f\"üìä Files generated:\")\n",
    "#         print(f\"   ‚Ä¢ comprehensive_dashboard.png - Main performance overview\")\n",
    "#     print(f\"   ‚Ä¢ correlation_heatmap.png - Correlation analysis\") \n",
    "#     print(f\"   ‚Ä¢ distribution_analysis.png - Score distributions\")\n",
    "#     print(f\"   ‚Ä¢ performance_quality_scatter.png - Quality vs performance plots\")\n",
    "#     print(f\"   ‚Ä¢ performance_radar_chart.png - Comprehensive performance profile\")\n",
    "#     print(f\"   ‚Ä¢ hallucination_comparison.png - Hallucination analysis\")\n",
    "#     if results.get('type_wise_metrics'):\n",
    "#         print(f\"   ‚Ä¢ question_type_analysis.png - Analysis by question type\")\n",
    "#     print(f\"   ‚Ä¢ comprehensive_metrics.json - All computed metrics\")\n",
    "#     print(f\"   ‚Ä¢ all_example_evaluations.json - Complete results for all {len(df_results)} examples\")\n",
    "#     print(f\"   ‚Ä¢ example_subsets_for_inspection.json - Best/worst/hallucinated examples\")\n",
    "#     print(f\"   ‚Ä¢ full_evaluation_results.csv - Complete results in CSV format\")\n",
    "#     print(f\"   ‚Ä¢ full_evaluation_results.pkl - Complete results in pickle format\")\n",
    "#     print(f\"   ‚Ä¢ evaluation_summary.json - High-level summary\")\n",
    "#     print(f\"   ‚Ä¢ detailed_evaluation_report.txt - Human-readable report\")\n",
    "#     print(f\"   ‚Ä¢ model_configuration.json - Model and system configuration\")\n",
    "#     print(f\"   ‚Ä¢ failure_analysis.json - Analysis of failed predictions\")\n",
    "    \n",
    "#     # Final summary\n",
    "#     print(f\"\\nüìà QUICK SUMMARY:\")\n",
    "#     print(f\"   üéØ Accuracy (F1): {overall['F1_Score']:.1%}\")\n",
    "#     print(f\"   üîç Faithfulness: {overall['Faithfulness']:.1f}/5.0\")\n",
    "#     print(f\"   üß† Plausibility: {overall['Plausibility']:.1f}/5.0\") \n",
    "#     print(f\"   ‚ö†Ô∏è  Hallucination: {overall['Hallucination_Rate']:.1%}\")\n",
    "#     print(f\"   ‚ö° Processing rate: {len(df_results)/(total_time/60):.1f} questions/minute\")\n",
    "    \n",
    "#     return results, df_results\n",
    "\n",
    "# # --- COMMAND LINE INTERFACE ---\n",
    "# def parse_arguments():\n",
    "#     \"\"\"Parse command line arguments for flexible execution.\"\"\"\n",
    "#     import argparse\n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='MenatQA Comprehensive Evaluation')\n",
    "#     parser.add_argument('--model', default='Qwen/Qwen2-0.5B', \n",
    "#                        help='Model name or path (default: Qwen/Qwen2-0.5B)')\n",
    "#     parser.add_argument('--batch_size', type=int, default=8,\n",
    "#                        help='Batch size for evaluation (default: 8)')\n",
    "#     parser.add_argument('--max_questions', type=int, default=None,\n",
    "#                        help='Maximum number of questions to evaluate (default: all)')\n",
    "#     parser.add_argument('--dataset_path', default='./MenatQA.json',\n",
    "#                        help='Path to MenatQA dataset (default: ./MenatQA.json)')\n",
    "#     parser.add_argument('--output_dir', default=None,\n",
    "#                        help='Output directory for results (default: auto-generated)')\n",
    "#     parser.add_argument('--no_visualizations', action='store_true',\n",
    "#                        help='Skip generating visualizations')\n",
    "#     parser.add_argument('--checkpoint_interval', type=int, default=50,\n",
    "#                        help='Save progress every N questions (default: 50)')\n",
    "    \n",
    "#     return parser.parse_args()\n",
    "\n",
    "# def run_evaluation_with_args():\n",
    "#     \"\"\"Run evaluation with command line arguments.\"\"\"\n",
    "#     args = parse_arguments()\n",
    "    \n",
    "#     print(f\"üöÄ Starting MenatQA evaluation with:\")\n",
    "#     print(f\"   Model: {args.model}\")\n",
    "#     print(f\"   Batch size: {args.batch_size}\")\n",
    "#     print(f\"   Max questions: {args.max_questions or 'All'}\")\n",
    "#     print(f\"   Dataset: {args.dataset_path}\")\n",
    "    \n",
    "#     return main(\n",
    "#         max_questions=args.max_questions,\n",
    "#         batch_size=args.batch_size,\n",
    "#         model_name=args.model\n",
    "#     )\n",
    "\n",
    "# # --- BATCH EVALUATION FOR MULTIPLE MODELS ---\n",
    "# def evaluate_multiple_models(model_list: List[str], max_questions: int = None, \n",
    "#                            batch_size: int = 8, comparison_output_dir: str = './model_comparison'):\n",
    "#     \"\"\"Evaluate multiple models and generate comparison report.\"\"\"\n",
    "    \n",
    "#     print(f\"üîÑ Starting multi-model evaluation:\")\n",
    "#     for i, model in enumerate(model_list, 1):\n",
    "#         print(f\"   {i}. {model}\")\n",
    "    \n",
    "#     all_results = {}\n",
    "#     os.makedirs(comparison_output_dir, exist_ok=True)\n",
    "    \n",
    "#     for model_name in model_list:\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Evaluating model: {model_name}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "        \n",
    "#         try:\n",
    "#             results, df_results = main(\n",
    "#                 max_questions=max_questions,\n",
    "#                 batch_size=batch_size,\n",
    "#                 model_name=model_name\n",
    "#             )\n",
    "            \n",
    "#             if results is not None:\n",
    "#                 all_results[model_name] = {\n",
    "#                     'metrics': results['overall_metrics'],\n",
    "#                     'correlations': results['correlations'],\n",
    "#                     'hop_wise': results['hop_wise_metrics']\n",
    "#                 }\n",
    "#             else:\n",
    "#                 print(f\"‚ùå Failed to evaluate {model_name}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Error evaluating {model_name}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     # Generate comparison report\n",
    "#     if len(all_results) > 1:\n",
    "#         print(f\"\\nüìä Generating model comparison report...\")\n",
    "#         generate_model_comparison_report(all_results, comparison_output_dir)\n",
    "    \n",
    "#     return all_results\n",
    "\n",
    "# def generate_model_comparison_report(all_results: Dict, output_dir: str):\n",
    "#     \"\"\"Generate a comprehensive comparison report for multiple models.\"\"\"\n",
    "    \n",
    "#     # Create comparison DataFrame\n",
    "#     comparison_data = []\n",
    "#     for model_name, results in all_results.items():\n",
    "#         metrics = results['metrics']\n",
    "#         comparison_data.append({\n",
    "#             'Model': model_name,\n",
    "#             'Exact_Match': metrics['Exact_Match'],\n",
    "#             'F1_Score': metrics['F1_Score'],\n",
    "#             'BLEU_Score': metrics['BLEU_Score'],\n",
    "#             'Faithfulness': metrics['Faithfulness'],\n",
    "#             'Plausibility': metrics['Plausibility'],\n",
    "#             'Hallucination_Rate': metrics['Hallucination_Rate'],\n",
    "#             'Success_Rate': metrics['Success_Rate']\n",
    "#         })\n",
    "    \n",
    "#     comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "#     # Save comparison table\n",
    "#     comparison_df.to_csv(f'{output_dir}/model_comparison.csv', index=False)\n",
    "    \n",
    "#     # Create comparison visualizations\n",
    "#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "#     models = comparison_df['Model'].values\n",
    "    \n",
    "#     # Accuracy comparison\n",
    "#     ax1.bar(models, comparison_df['F1_Score'], alpha=0.8, color='skyblue')\n",
    "#     ax1.set_title('F1 Score Comparison', fontweight='bold')\n",
    "#     ax1.set_ylabel('F1 Score')\n",
    "#     ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "#     # Explanation quality comparison\n",
    "#     x = np.arange(len(models))\n",
    "#     width = 0.35\n",
    "#     ax2.bar(x - width/2, comparison_df['Faithfulness'], width, label='Faithfulness', alpha=0.8)\n",
    "#     ax2.bar(x + width/2, comparison_df['Plausibility'], width, label='Plausibility', alpha=0.8)\n",
    "#     ax2.set_title('Explanation Quality Comparison', fontweight='bold')\n",
    "#     ax2.set_ylabel('Score (1-5)')\n",
    "#     ax2.set_xticks(x)\n",
    "#     ax2.set_xticklabels(models, rotation=45)\n",
    "#     ax2.legend()\n",
    "    \n",
    "#     # Hallucination rate comparison\n",
    "#     ax3.bar(models, comparison_df['Hallucination_Rate'] * 100, alpha=0.8, color='salmon')\n",
    "#     ax3.set_title('Hallucination Rate Comparison', fontweight='bold')\n",
    "#     ax3.set_ylabel('Hallucination Rate (%)')\n",
    "#     ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "#     # Overall performance radar\n",
    "#     categories = ['Accuracy', 'Exactness', 'Fluency', 'Faithfulness', 'Plausibility', 'Reliability']\n",
    "#     angles = [n / float(len(categories)) * 2 * np.pi for n in range(len(categories))]\n",
    "#     angles += angles[:1]\n",
    "    \n",
    "#     ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
    "    \n",
    "#     colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "#     for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "#         values = [\n",
    "#             row['F1_Score'],\n",
    "#             row['Exact_Match'],\n",
    "#             row['BLEU_Score'],\n",
    "#             row['Faithfulness'] / 5.0,\n",
    "#             row['Plausibility'] / 5.0,\n",
    "#             1 - row['Hallucination_Rate']\n",
    "#         ]\n",
    "#         values += values[:1]\n",
    "        \n",
    "#         ax4.plot(angles, values, 'o-', linewidth=2, \n",
    "#                 label=row['Model'], color=colors[i % len(colors)])\n",
    "#         ax4.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
    "    \n",
    "#     ax4.set_xticks(angles[:-1])\n",
    "#     ax4.set_xticklabels(categories)\n",
    "#     ax4.set_ylim(0, 1)\n",
    "#     ax4.set_title('Overall Performance Profile', fontweight='bold')\n",
    "#     ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'{output_dir}/model_comparison_charts.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Generate text report\n",
    "#     with open(f'{output_dir}/model_comparison_report.txt', 'w') as f:\n",
    "#         f.write(\"MODEL COMPARISON REPORT\\n\")\n",
    "#         f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "#         f.write(\"PERFORMANCE SUMMARY:\\n\")\n",
    "#         f.write(\"-\" * 30 + \"\\n\")\n",
    "#         for _, row in comparison_df.iterrows():\n",
    "#             f.write(f\"\\n{row['Model']}:\\n\")\n",
    "#             f.write(f\"  Accuracy (F1): {row['F1_Score']:.3f}\\n\")\n",
    "#             f.write(f\"  Exactness (EM): {row['Exact_Match']:.3f}\\n\") \n",
    "#             f.write(f\"  Fluency (BLEU): {row['BLEU_Score']:.3f}\\n\")\n",
    "#             f.write(f\"  Faithfulness: {row['Faithfulness']:.2f}/5.0\\n\")\n",
    "#             f.write(f\"  Plausibility: {row['Plausibility']:.2f}/5.0\\n\")\n",
    "#             f.write(f\"  Hallucination Rate: {row['Hallucination_Rate']:.1%}\\n\")\n",
    "#             f.write(f\"  Success Rate: {row['Success_Rate']:.1%}\\n\")\n",
    "        \n",
    "#         # Rankings\n",
    "#         f.write(f\"\\nRANKINGS:\\n\")\n",
    "#         f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "#         rankings = {\n",
    "#             'Best Accuracy': comparison_df.loc[comparison_df['F1_Score'].idxmax(), 'Model'],\n",
    "#             'Best Faithfulness': comparison_df.loc[comparison_df['Faithfulness'].idxmax(), 'Model'],\n",
    "#             'Best Plausibility': comparison_df.loc[comparison_df['Plausibility'].idxmax(), 'Model'],\n",
    "#             'Lowest Hallucination': comparison_df.loc[comparison_df['Hallucination_Rate'].idxmin(), 'Model'],\n",
    "#             'Highest Success Rate': comparison_df.loc[comparison_df['Success_Rate'].idxmax(), 'Model']\n",
    "#         }\n",
    "        \n",
    "#         for category, model in rankings.items():\n",
    "#             f.write(f\"{category}: {model}\\n\")\n",
    "    \n",
    "#     print(f\"‚úÖ Model comparison report saved to {output_dir}\")\n",
    "\n",
    "# # --- MAIN EXECUTION BLOCK ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "    \n",
    "#     # Check if running with command line arguments\n",
    "#     if len(sys.argv) > 1:\n",
    "#         results, df_results = run_evaluation_with_args()\n",
    "#     else:\n",
    "#         # Default execution - evaluate on full dataset\n",
    "#         print(\"üî• Running full dataset evaluation...\")\n",
    "#         print(\"üí° Tip: Use command line arguments for more control:\")\n",
    "#         print(\"   python script.py --model Qwen/Qwen2-1.5B --batch_size 4 --max_questions 100\")\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "#         # Run with default settings on full dataset\n",
    "#         results, df_results = main(\n",
    "#             max_questions=None,  # Evaluate ALL questions\n",
    "#             batch_size=8,        # Adjust based on your GPU memory\n",
    "#             model_name=\"Qwen/Qwen2-0.5B\"\n",
    "#         )\n",
    "        \n",
    "#         # Optional: Run comparison with multiple models\n",
    "#         # Uncomment the following lines to compare multiple models:\n",
    "#         \"\"\"\n",
    "#         model_comparison = [\n",
    "#             \"Qwen/Qwen2-0.5B\",\n",
    "#             \"Qwen/Qwen2-1.5B\", \n",
    "#             \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "#         ]\n",
    "        \n",
    "#         print(f\"\\nüîÑ Starting multi-model comparison...\")\n",
    "#         comparison_results = evaluate_multiple_models(\n",
    "#             model_list=model_comparison,\n",
    "#             max_questions=100,  # Limit for comparison\n",
    "#             batch_size=4\n",
    "#         )\n",
    "#         \"\"\"\n",
    "\n",
    "# # --- EXAMPLE USAGE FUNCTIONS ---\n",
    "# def quick_test(num_questions: int = 10):\n",
    "#     \"\"\"Quick test with a small number of questions.\"\"\"\n",
    "#     print(f\"üß™ Quick test with {num_questions} questions\")\n",
    "#     return main(max_questions=num_questions, batch_size=2)\n",
    "\n",
    "# def full_evaluation():\n",
    "#     \"\"\"Run evaluation on the complete MenatQA dataset.\"\"\"\n",
    "#     print(\"üöÄ Full dataset evaluation\")\n",
    "#     return main(max_questions=None, batch_size=8)\n",
    "\n",
    "# def gpu_optimized_evaluation():\n",
    "#     \"\"\"Optimized evaluation for systems with good GPU memory.\"\"\"\n",
    "#     print(\"üöÄ GPU-optimized evaluation\")\n",
    "#     return main(max_questions=None, batch_size=16)\n",
    "\n",
    "# def cpu_evaluation():\n",
    "#     \"\"\"CPU-friendly evaluation with smaller batch size.\"\"\"\n",
    "#     print(\"üíª CPU evaluation\")\n",
    "#     return main(max_questions=None, batch_size=1)\n",
    "\n",
    "# # Print usage information\n",
    "# print(__doc__ if __doc__ else \"MenatQA Comprehensive Evaluation Script\")\n",
    "# print(\"\\nüîß Usage Examples:\")\n",
    "# print(\"  quick_test(10)           # Test with 10 questions\")\n",
    "# print(\"  full_evaluation()        # Evaluate complete dataset\") \n",
    "# print(\"  gpu_optimized_evaluation() # Use larger batches for GPU\")\n",
    "# print(\"  cpu_evaluation()         # CPU-friendly smaller batches\")\n",
    "# print(\"\\nüìö Or run directly: python script.py --help for command line options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5162c84-8d2b-48e1-a5bc-7a318abd9cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c36da-a25d-479b-b837-bf28a244c950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcde66-8a2d-4b40-acc3-1f92605eee05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b4bae-aa0c-4cb5-83b2-7302b819e209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31102352-a844-4a78-bd79-c1a2b61c2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-26 05:29:29.542288: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 05:29:29.556078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753507769.573558    3313 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753507769.579111    3313 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753507769.592370    3313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753507769.592385    3313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753507769.592388    3313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753507769.592389    3313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-26 05:29:29.596530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MenatQA Explanation Faithfulness Evaluation ===\n",
      "\n",
      "Loading MenatQA dataset...\n",
      "Loaded MenatQA dataset with 999 examples\n",
      "Preprocessing dataset...\n",
      "Dataset preprocessed: 999 questions\n",
      "1-hop questions: 0 (0.0%)\n",
      "2-hop questions: 135 (13.5%)\n",
      "3-hop questions: 698 (69.9%)\n",
      "4-hop questions: 166 (16.6%)\n",
      "Running Qwen/Qwen3-0.6B with explanation prompts on 999 questions...\n",
      "Loading Qwen/Qwen3-0.6B on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Generating predictions with explanations:   1%|          | 10/999 [00:49<1:21:12,  4.93s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating predictions with explanations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [1:22:03<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating explanation faithfulness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating explanation faithfulness: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [00:00<00:00, 4975.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPLANATION FAITHFULNESS RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Results (n=999):\n",
      "Average Faithfulness: 4.32/5\n",
      "Average Plausibility: 4.15/5\n",
      "Hallucination Rate: 0.5%\n",
      "\n",
      "--- Results by Complexity Category ---\n",
      "3-hop: Faithfulness=4.34/5, Plausibility=4.15/5, Hallucination=0.7% (n=698)\n",
      "2-hop: Faithfulness=4.27/5, Plausibility=4.19/5, Hallucination=0.0% (n=135)\n",
      "4+-hop: Faithfulness=4.30/5, Plausibility=4.11/5, Hallucination=0.0% (n=166)\n",
      "\n",
      "--- Score Distributions ---\n",
      "Faithfulness Distribution:\n",
      "  Score 1: 0 (0.0%)\n",
      "  Score 2: 57 (5.7%)\n",
      "  Score 3: 121 (12.1%)\n",
      "  Score 4: 267 (26.7%)\n",
      "  Score 5: 554 (55.5%)\n",
      "Plausibility Distribution:\n",
      "  Score 1: 0 (0.0%)\n",
      "  Score 2: 0 (0.0%)\n",
      "  Score 3: 169 (16.9%)\n",
      "  Score 4: 510 (51.1%)\n",
      "  Score 5: 320 (32.0%)\n",
      "\n",
      "Generating faithfulness visualization...\n",
      "Faithfulness visualization saved to ./menatqa_faithfulness_analysis.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving detailed results...\n",
      "\n",
      "--- Example Cases ---\n",
      "\n",
      "GOOD EXPLANATION (Faithfulness=5, Plausibility=5):\n",
      "Question: Which team did the player Attaphol Buspakom belong to from 1996 to 1998?\n",
      "Gold Answer: Thailand national football team\n",
      "Model Answer: But how does that work? If the team is the same name, then the answer would be the team's name. But the answer given is just \"Buspakom\". That seems like a mistake.\n",
      "Explanation Steps: ['Buspakom was a player for the team of the same name, and he was a member of the 1996‚Äì97 and 1997‚Äì98 seasons. So, the answer is Buspakom.', 'Answer: Buspakom']...\n",
      "\n",
      "HALLUCINATED REASONING (Faithfulness=2, Plausibility=4):\n",
      "Question: Which position did Peter Beattie hold from Dec 1989 to 1996?\n",
      "Gold Answer: Queensland Parliament as MP for Brisbane Central\n",
      "Model Answer: Explanation: The first president of the United Nations General Assembly\n",
      "Explanation Steps: ['He was the first president of the United Nations General Assembly, and he was the first president of the United Nations General Assembly in 1989.', 'Options: A. First President of the United Nations General Assembly B. First President of the United Nations General Assembly C. First President of the United Nations General Assembly D. First President of the United Nations General Assembly']...\n",
      "\n",
      "Evaluation complete!\n",
      "Detailed results saved to: ./menatqa_faithfulness_results.json\n",
      "Visualization saved to: ./menatqa_faithfulness_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# explanation_faithfulness_evaluation.py\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        import urllib.request\n",
    "        print(f\"Downloading MenatQA to {file_path}...\")\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "            str(file_path)\n",
    "        )\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "    return data\n",
    "\n",
    "def extract_reasoning_hops(example):\n",
    "    question = example.get('question', '')\n",
    "    answer = example.get('answer', '')\n",
    "    q_type = example.get('type', '')\n",
    "    time_scope = example.get('time_scope', '')\n",
    "    \n",
    "    sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "    clauses = len([c for c in re.split(r'and|or|but|because|when|if', question.lower()) if c.strip()])\n",
    "    capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "    complexity_score = 1\n",
    "    complexity_score += min(1, len(sentences) - 1)\n",
    "    complexity_score += min(1, (clauses - 1) // 2)\n",
    "    complexity_score += min(1, capitalized_words // 3)\n",
    "    if time_scope:\n",
    "        complexity_score += 1\n",
    "    complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "    return complexity_score\n",
    "\n",
    "def preprocess_dataset(data):\n",
    "    processed_data = []\n",
    "    hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "    for item in data:\n",
    "        complexity_score = extract_reasoning_hops(item)\n",
    "        hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "        entry = {\n",
    "            'ID': item.get('ID', ''),\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'type': item.get('type', ''),\n",
    "            'time_scope': item.get('time_scope', ''),\n",
    "            'hop_count': complexity_score,\n",
    "            'model_prediction': '',\n",
    "            'model_reasoning_steps': []\n",
    "        }\n",
    "        processed_data.append(entry)\n",
    "    \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df['hop_category'] = pd.cut(\n",
    "        df['hop_count'],\n",
    "        bins=[-1, 1, 2, 3, float('inf')],\n",
    "        labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "    for i in range(1, 5):\n",
    "        print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- MODEL GENERATION ---\n",
    "def run_qwen_predictions_with_explanations(df, model_name=\"Qwen/Qwen3-0.6B\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading {model_name} on {device}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        device=0 if device == 'cuda' else -1,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.01,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    def format_explanation_prompt(question):\n",
    "        return (f\"Answer the following question and provide a detailed explanation for your reasoning. \"\n",
    "                f\"Explain each step of your thinking process.\\n\"\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Answer with explanation:\")\n",
    "\n",
    "    all_preds, all_steps = [], []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating predictions with explanations\"):\n",
    "        prompt = format_explanation_prompt(row[\"question\"])\n",
    "        generation = pipe(prompt)[0]['generated_text']\n",
    "        output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "        steps = [x.strip() for x in output.split('\\n') if x.strip()]\n",
    "        \n",
    "        if not steps:\n",
    "            steps = [\"No explanation provided\"]\n",
    "            \n",
    "        model_answer = steps[-1] if steps else \"No answer\"\n",
    "        all_preds.append(model_answer)\n",
    "        all_steps.append(steps)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['model_prediction'] = all_preds\n",
    "    df['model_reasoning_steps'] = all_steps\n",
    "    return df\n",
    "\n",
    "# --- FAITHFULNESS EVALUATION ---\n",
    "def evaluate_faithfulness(question, gold_answer, model_prediction, model_steps):\n",
    "    \"\"\"\n",
    "    Rate faithfulness (1-5): Does the explanation truly reflect the computation that produced the answer?\n",
    "    \"\"\"\n",
    "    if not model_steps or len(model_steps) == 0:\n",
    "        return 1\n",
    "    \n",
    "    score = 1\n",
    "    explanation_text = ' '.join(model_steps).lower()\n",
    "    question_lower = question.lower()\n",
    "    gold_answer_lower = str(gold_answer).lower()\n",
    "    model_prediction_lower = str(model_prediction).lower()\n",
    "    \n",
    "    # Check if explanation mentions key elements from question\n",
    "    question_keywords = set(re.findall(r'\\b\\w+\\b', question_lower))\n",
    "    explanation_keywords = set(re.findall(r'\\b\\w+\\b', explanation_text))\n",
    "    question_overlap = len(question_keywords & explanation_keywords) / len(question_keywords) if question_keywords else 0\n",
    "    \n",
    "    if question_overlap >= 0.3:\n",
    "        score += 1\n",
    "    \n",
    "    # Check if explanation leads to the predicted answer\n",
    "    if model_prediction_lower.strip() in explanation_text or any(word in explanation_text for word in model_prediction_lower.split() if len(word) > 2):\n",
    "        score += 1\n",
    "    \n",
    "    # Check logical flow in steps\n",
    "    if len(model_steps) >= 2:\n",
    "        logical_flow = 0\n",
    "        for i in range(len(model_steps) - 1):\n",
    "            current_step = model_steps[i].lower()\n",
    "            next_step = model_steps[i + 1].lower()\n",
    "            current_words = set(re.findall(r'\\b\\w+\\b', current_step))\n",
    "            next_words = set(re.findall(r'\\b\\w+\\b', next_step))\n",
    "            if len(current_words & next_words) > 0:\n",
    "                logical_flow += 1\n",
    "        \n",
    "        if logical_flow >= len(model_steps) * 0.3:\n",
    "            score += 1\n",
    "    \n",
    "    # Check if reasoning addresses the question type appropriately\n",
    "    reasoning_indicators = ['because', 'since', 'therefore', 'thus', 'so', 'hence', 'as a result']\n",
    "    if any(indicator in explanation_text for indicator in reasoning_indicators):\n",
    "        score += 1\n",
    "    \n",
    "    return min(5, score)\n",
    "\n",
    "def evaluate_plausibility(question, model_prediction, model_steps):\n",
    "    \"\"\"\n",
    "    Rate plausibility (1-5): Is the explanation convincing, regardless of correctness?\n",
    "    \"\"\"\n",
    "    if not model_steps or len(model_steps) == 0:\n",
    "        return 1\n",
    "    \n",
    "    score = 1\n",
    "    explanation_text = ' '.join(model_steps).lower()\n",
    "    \n",
    "    # Check explanation length and detail\n",
    "    total_words = len(explanation_text.split())\n",
    "    if total_words >= 10:\n",
    "        score += 1\n",
    "    if total_words >= 20:\n",
    "        score += 1\n",
    "    \n",
    "    # Check for structured reasoning\n",
    "    structured_indicators = ['first', 'second', 'next', 'then', 'finally', 'step', 'because', 'therefore']\n",
    "    structure_count = sum(1 for indicator in structured_indicators if indicator in explanation_text)\n",
    "    if structure_count >= 2:\n",
    "        score += 1\n",
    "    \n",
    "    # Check for domain-appropriate vocabulary\n",
    "    domain_terms = ['year', 'time', 'period', 'event', 'history', 'occurred', 'happened', 'during', 'after', 'before']\n",
    "    domain_count = sum(1 for term in domain_terms if term in explanation_text)\n",
    "    if domain_count >= 2:\n",
    "        score += 1\n",
    "    \n",
    "    # Check coherence (no contradictions or nonsensical statements)\n",
    "    coherence_flags = ['not not', 'never always', 'impossible possible', 'true false']\n",
    "    has_contradictions = any(flag in explanation_text for flag in coherence_flags)\n",
    "    if not has_contradictions and len(model_steps) > 1:\n",
    "        coherent_flow = True\n",
    "        for step in model_steps:\n",
    "            if len(step.strip()) < 3 or step.strip().count(' ') == 0:\n",
    "                coherent_flow = False\n",
    "                break\n",
    "        if coherent_flow:\n",
    "            score += 1\n",
    "    \n",
    "    return min(5, score)\n",
    "\n",
    "def evaluate_explanation_faithfulness(df):\n",
    "    \"\"\"\n",
    "    Evaluate faithfulness and plausibility of model explanations\n",
    "    \"\"\"\n",
    "    faithfulness_scores = []\n",
    "    plausibility_scores = []\n",
    "    hallucinated_reasoning_count = 0\n",
    "    total_evaluated = 0\n",
    "    \n",
    "    category_results = defaultdict(lambda: {\n",
    "        'faithfulness_scores': [], \n",
    "        'plausibility_scores': [], \n",
    "        'hallucinated_count': 0, \n",
    "        'total': 0\n",
    "    })\n",
    "    \n",
    "    detailed_results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating explanation faithfulness\"):\n",
    "        question = row['question']\n",
    "        gold_answer = row['answer']\n",
    "        model_prediction = row['model_prediction']\n",
    "        model_steps = row['model_reasoning_steps']\n",
    "        hop_category = row['hop_category']\n",
    "        \n",
    "        if not model_steps or not model_steps[0] or hop_category is None:\n",
    "            continue\n",
    "            \n",
    "        total_evaluated += 1\n",
    "        category_results[hop_category]['total'] += 1\n",
    "        \n",
    "        # Evaluate Faithfulness (1-5)\n",
    "        faithfulness = evaluate_faithfulness(question, gold_answer, model_prediction, model_steps)\n",
    "        \n",
    "        # Evaluate Plausibility (1-5)\n",
    "        plausibility = evaluate_plausibility(question, model_prediction, model_steps)\n",
    "        \n",
    "        faithfulness_scores.append(faithfulness)\n",
    "        plausibility_scores.append(plausibility)\n",
    "        category_results[hop_category]['faithfulness_scores'].append(faithfulness)\n",
    "        category_results[hop_category]['plausibility_scores'].append(plausibility)\n",
    "        \n",
    "        # Detect hallucinated reasoning (high plausibility + low faithfulness)\n",
    "        is_hallucinated = plausibility >= 4 and faithfulness <= 2\n",
    "        if is_hallucinated:\n",
    "            hallucinated_reasoning_count += 1\n",
    "            category_results[hop_category]['hallucinated_count'] += 1\n",
    "        \n",
    "        # Store detailed results\n",
    "        detailed_results.append({\n",
    "            'ID': row['ID'],\n",
    "            'question': question,\n",
    "            'gold_answer': gold_answer,\n",
    "            'model_prediction': model_prediction,\n",
    "            'model_steps': model_steps,\n",
    "            'hop_category': hop_category,\n",
    "            'faithfulness_score': faithfulness,\n",
    "            'plausibility_score': plausibility,\n",
    "            'is_hallucinated': is_hallucinated\n",
    "        })\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_faithfulness = np.mean(faithfulness_scores) if faithfulness_scores else 0\n",
    "    avg_plausibility = np.mean(plausibility_scores) if plausibility_scores else 0\n",
    "    hallucination_rate = hallucinated_reasoning_count / total_evaluated if total_evaluated > 0 else 0\n",
    "    \n",
    "    # Calculate category-wise metrics\n",
    "    category_metrics = {}\n",
    "    for category, data in category_results.items():\n",
    "        if data['total'] > 0:\n",
    "            category_metrics[str(category)] = {\n",
    "                'avg_faithfulness': np.mean(data['faithfulness_scores']) if data['faithfulness_scores'] else 0,\n",
    "                'avg_plausibility': np.mean(data['plausibility_scores']) if data['plausibility_scores'] else 0,\n",
    "                'hallucination_rate': data['hallucinated_count'] / data['total'],\n",
    "                'total_samples': data['total']\n",
    "            }\n",
    "    \n",
    "    results = {\n",
    "        'overall_faithfulness': avg_faithfulness,\n",
    "        'overall_plausibility': avg_plausibility,\n",
    "        'overall_hallucination_rate': hallucination_rate,\n",
    "        'total_evaluated': total_evaluated,\n",
    "        'by_category': category_metrics,\n",
    "        'faithfulness_distribution': {\n",
    "            '1': sum(1 for s in faithfulness_scores if s == 1),\n",
    "            '2': sum(1 for s in faithfulness_scores if s == 2),\n",
    "            '3': sum(1 for s in faithfulness_scores if s == 3),\n",
    "            '4': sum(1 for s in faithfulness_scores if s == 4),\n",
    "            '5': sum(1 for s in faithfulness_scores if s == 5)\n",
    "        },\n",
    "        'plausibility_distribution': {\n",
    "            '1': sum(1 for s in plausibility_scores if s == 1),\n",
    "            '2': sum(1 for s in plausibility_scores if s == 2),\n",
    "            '3': sum(1 for s in plausibility_scores if s == 3),\n",
    "            '4': sum(1 for s in plausibility_scores if s == 4),\n",
    "            '5': sum(1 for s in plausibility_scores if s == 5)\n",
    "        },\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "def visualize_faithfulness_results(faithfulness_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize explanation faithfulness and plausibility results\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Overall Faithfulness vs Plausibility by Category\n",
    "    categories = [cat for cat in faithfulness_results['by_category'].keys()]\n",
    "    faithfulness_scores = [faithfulness_results['by_category'][cat]['avg_faithfulness'] for cat in categories]\n",
    "    plausibility_scores = [faithfulness_results['by_category'][cat]['avg_plausibility'] for cat in categories]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, faithfulness_scores, width, label='Faithfulness', color='#3498db', alpha=0.8)\n",
    "    ax1.bar(x + width/2, plausibility_scores, width, label='Plausibility', color='#e74c3c', alpha=0.8)\n",
    "    ax1.set_xlabel('Reasoning Complexity')\n",
    "    ax1.set_ylabel('Average Score (1-5)')\n",
    "    ax1.set_title('Faithfulness vs Plausibility by Complexity')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(categories)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 5.5)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add sample counts\n",
    "    for i, cat in enumerate(categories):\n",
    "        count = faithfulness_results['by_category'][cat]['total_samples']\n",
    "        ax1.text(i, 0.1, f'n={count}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Hallucination Rate by Category\n",
    "    hallucination_rates = [faithfulness_results['by_category'][cat]['hallucination_rate'] * 100 for cat in categories]\n",
    "    ax2.bar(categories, hallucination_rates, color='#f39c12', alpha=0.8)\n",
    "    ax2.set_xlabel('Reasoning Complexity')\n",
    "    ax2.set_ylabel('Hallucination Rate (%)')\n",
    "    ax2.set_title('Hallucinated Reasoning by Complexity\\n(High Plausibility + Low Faithfulness)')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Faithfulness Distribution\n",
    "    faith_dist = faithfulness_results['faithfulness_distribution']\n",
    "    scores = list(faith_dist.keys())\n",
    "    counts = list(faith_dist.values())\n",
    "    ax3.bar(scores, counts, color='#3498db', alpha=0.8)\n",
    "    ax3.set_xlabel('Faithfulness Score')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Distribution of Faithfulness Scores')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "        # 4. Plausibility Distribution\n",
    "    plaus_dist = faithfulness_results['plausibility_distribution']\n",
    "    scores = list(plaus_dist.keys())\n",
    "    counts = list(plaus_dist.values())\n",
    "    ax4.bar(scores, counts, color='#e74c3c', alpha=0.8)\n",
    "    ax4.set_xlabel('Plausibility Score')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.set_title('Distribution of Plausibility Scores')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add overall statistics\n",
    "    overall_text = (f\"Overall Results (n={faithfulness_results['total_evaluated']}):\\n\"\n",
    "                   f\"Avg Faithfulness: {faithfulness_results['overall_faithfulness']:.2f}/5 | \"\n",
    "                   f\"Avg Plausibility: {faithfulness_results['overall_plausibility']:.2f}/5\\n\"\n",
    "                   f\"Hallucination Rate: {faithfulness_results['overall_hallucination_rate']:.1%}\")\n",
    "    \n",
    "    fig.suptitle('Explanation Faithfulness Analysis - Qwen/Qwen3-0.6B on MenatQA', fontsize=16, fontweight='bold')\n",
    "    plt.figtext(0.5, 0.02, overall_text, ha='center', fontsize=12, fontweight='bold',\n",
    "                bbox=dict(facecolor='lightgray', alpha=0.8, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Faithfulness visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    DATASET_FILE = './MenatQA.json'\n",
    "    MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "    OUTPUT_FAITHFULNESS_IMAGE = './menatqa_faithfulness_analysis.png'\n",
    "    OUTPUT_FAITHFULNESS_RESULTS = './menatqa_faithfulness_results.json'\n",
    "\n",
    "    print(\"=== MenatQA Explanation Faithfulness Evaluation ===\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading MenatQA dataset...\")\n",
    "    data = load_menatqa_dataset(DATASET_FILE)\n",
    "    \n",
    "    # Preprocess\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    df = preprocess_dataset(data)\n",
    "    \n",
    "    # Run model with explanation prompts\n",
    "    print(f\"Running {MODEL_NAME} with explanation prompts on {len(df)} questions...\")\n",
    "    df_results = run_qwen_predictions_with_explanations(df, model_name=MODEL_NAME)\n",
    "    \n",
    "    # Evaluate explanation faithfulness\n",
    "    print(\"Evaluating explanation faithfulness...\")\n",
    "    faithfulness_results = evaluate_explanation_faithfulness(df_results)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPLANATION FAITHFULNESS RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOverall Results (n={faithfulness_results['total_evaluated']}):\")\n",
    "    print(f\"Average Faithfulness: {faithfulness_results['overall_faithfulness']:.2f}/5\")\n",
    "    print(f\"Average Plausibility: {faithfulness_results['overall_plausibility']:.2f}/5\")\n",
    "    print(f\"Hallucination Rate: {faithfulness_results['overall_hallucination_rate']:.1%}\")\n",
    "    \n",
    "    print(\"\\n--- Results by Complexity Category ---\")\n",
    "    for category, metrics in faithfulness_results['by_category'].items():\n",
    "        print(f\"{category}: \"\n",
    "              f\"Faithfulness={metrics['avg_faithfulness']:.2f}/5, \"\n",
    "              f\"Plausibility={metrics['avg_plausibility']:.2f}/5, \"\n",
    "              f\"Hallucination={metrics['hallucination_rate']:.1%} \"\n",
    "              f\"(n={metrics['total_samples']})\")\n",
    "    \n",
    "    print(\"\\n--- Score Distributions ---\")\n",
    "    print(\"Faithfulness Distribution:\")\n",
    "    for score, count in faithfulness_results['faithfulness_distribution'].items():\n",
    "        percentage = count / faithfulness_results['total_evaluated'] * 100 if faithfulness_results['total_evaluated'] > 0 else 0\n",
    "        print(f\"  Score {score}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"Plausibility Distribution:\")\n",
    "    for score, count in faithfulness_results['plausibility_distribution'].items():\n",
    "        percentage = count / faithfulness_results['total_evaluated'] * 100 if faithfulness_results['total_evaluated'] > 0 else 0\n",
    "        print(f\"  Score {score}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"\\nGenerating faithfulness visualization...\")\n",
    "    visualize_faithfulness_results(\n",
    "        faithfulness_results, \n",
    "        save_path=OUTPUT_FAITHFULNESS_IMAGE\n",
    "    )\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(\"Saving detailed results...\")\n",
    "    with open(OUTPUT_FAITHFULNESS_RESULTS, 'w') as f:\n",
    "        json.dump(faithfulness_results, f, indent=2)\n",
    "    \n",
    "    # Print some example cases\n",
    "    print(\"\\n--- Example Cases ---\")\n",
    "    detailed_results = faithfulness_results['detailed_results']\n",
    "    \n",
    "    # High faithfulness, high plausibility (good case)\n",
    "    good_cases = [r for r in detailed_results if r['faithfulness_score'] >= 4 and r['plausibility_score'] >= 4]\n",
    "    if good_cases:\n",
    "        example = good_cases[0]\n",
    "        print(f\"\\nGOOD EXPLANATION (Faithfulness={example['faithfulness_score']}, Plausibility={example['plausibility_score']}):\")\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "        print(f\"Model Answer: {example['model_prediction']}\")\n",
    "        print(f\"Explanation Steps: {example['model_steps'][:2]}...\")  # First 2 steps\n",
    "    \n",
    "    # Hallucinated reasoning cases\n",
    "    hallucinated_cases = [r for r in detailed_results if r['is_hallucinated']]\n",
    "    if hallucinated_cases:\n",
    "        example = hallucinated_cases[0]\n",
    "        print(f\"\\nHALLUCINATED REASONING (Faithfulness={example['faithfulness_score']}, Plausibility={example['plausibility_score']}):\")\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "        print(f\"Model Answer: {example['model_prediction']}\")\n",
    "        print(f\"Explanation Steps: {example['model_steps'][:2]}...\")  # First 2 steps\n",
    "    \n",
    "    # Low faithfulness, low plausibility (bad case)\n",
    "    bad_cases = [r for r in detailed_results if r['faithfulness_score'] <= 2 and r['plausibility_score'] <= 2]\n",
    "    if bad_cases:\n",
    "        example = bad_cases[0]\n",
    "        print(f\"\\nPOOR EXPLANATION (Faithfulness={example['faithfulness_score']}, Plausibility={example['plausibility_score']}):\")\n",
    "        print(f\"Question: {example['question']}\")\n",
    "        print(f\"Gold Answer: {example['gold_answer']}\")\n",
    "        print(f\"Model Answer: {example['model_prediction']}\")\n",
    "        print(f\"Explanation Steps: {example['model_steps'][:2]}...\")  # First 2 steps\n",
    "    \n",
    "    print(f\"\\nEvaluation complete!\")\n",
    "    print(f\"Detailed results saved to: {OUTPUT_FAITHFULNESS_RESULTS}\")\n",
    "    print(f\"Visualization saved to: {OUTPUT_FAITHFULNESS_IMAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5237f3-b1d2-4e4f-aeae-ea7de5061ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a28a51-ae22-4c8a-9c79-3a6ce774d81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0eed3-0885-414b-8f5e-304772e83950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fd6a0-dae9-432a-becd-c29c2d688f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
