{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ece3f9-46a0-48e4-88d7-2144fc36492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.32.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (220 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in ./.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (220 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.4/220.4 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (21.2.0)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.1/206.1 KB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (327 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 KB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 KB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.6 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2025.3.0 multidict-6.4.4 multiprocess-0.70.16 propcache-0.3.1 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2d1074-cc75-4c4d-a905-8313347c0c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 03:36:40,762 - WARNING - bitsandbytes not found. Running without 4-bit quantization.\n",
      "2025-06-01 03:36:41,210 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-06-01 03:36:41,688 - INFO - Loaded MenatQA dataset with 999 examples\n",
      "Evaluating MenatQA:   0%|          | 0/999 [00:00<?, ?it/s]\n",
      "2025-06-01 03:36:41,691 - ERROR - Evaluation failed: 'factor'\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import re\n",
    "# from typing import List, Dict, Tuple\n",
    "# import logging\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from pathlib import Path\n",
    "# import urllib.request\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Set seaborn style for better visuals\n",
    "# sns.set(style=\"whitegrid\")\n",
    "\n",
    "# # Check for bitsandbytes availability\n",
    "# try:\n",
    "#     import bitsandbytes\n",
    "#     logger.info(f\"bitsandbytes version: {bitsandbytes.__version__}\")\n",
    "#     use_quantization = True\n",
    "# except ImportError:\n",
    "#     logger.warning(\"bitsandbytes not found. Running without 4-bit quantization.\")\n",
    "#     use_quantization = False\n",
    "\n",
    "# # Initialize Qwen3-0.6B model and tokenizer\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "# try:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#     model_kwargs = {\n",
    "#         \"device_map\": \"auto\",\n",
    "#         \"trust_remote_code\": True\n",
    "#     }\n",
    "#     if use_quantization:\n",
    "#         model_kwargs[\"load_in_4bit\"] = True\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "#     model.eval()\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Error loading model: {e}\")\n",
    "#     raise\n",
    "\n",
    "# # Load MenatQA dataset\n",
    "# def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "#     file_path = Path(file_path)\n",
    "    \n",
    "#     if not file_path.exists():\n",
    "#         logger.info(f\"Downloading MenatQA to {file_path}...\")\n",
    "#         try:\n",
    "#             urllib.request.urlretrieve(\n",
    "#                 \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "#                 str(file_path)\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Failed to download MenatQA: {e}\")\n",
    "#             raise\n",
    "    \n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     logger.info(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "#     return data\n",
    "\n",
    "# # Extract reasoning hops from model output\n",
    "# def extract_hops(response: str) -> List[str]:\n",
    "#     hop_pattern = r'<think>(.*?)</think>|(\\d+\\.\\s*[^.]+)'\n",
    "#     hops = re.findall(hop_pattern, response, re.DOTALL)\n",
    "#     hops = [hop[0] if hop[0] else hop[1].strip() for hop in hops]\n",
    "#     return [hop for hop in hops if hop]\n",
    "\n",
    "# # Generate forward reasoning\n",
    "# def forward_reasoning(question: str, context: str) -> Tuple[str, List[str]]:\n",
    "#     prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer the question step-by-step, showing your reasoning.\"\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     try:\n",
    "#         text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "#         with torch.no_grad():\n",
    "#             response_ids = model.generate(**inputs, max_new_tokens=4096)[0][len(inputs.input_ids[0]):]\n",
    "#         response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "#         answer = response.split(\"Final Answer:\")[-1].strip() if \"Final Answer:\" in response else response.strip().split('\\n')[-1]\n",
    "#         hops = extract_hops(response)\n",
    "#         return answer, hops\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error in forward reasoning: {e}\")\n",
    "#         return \"\", []\n",
    "\n",
    "# # Generate backward reasoning\n",
    "# def backward_reasoning(question: str, context: str, answer: str) -> List[str]:\n",
    "#     prompt = f\"Context: {context}\\nThe answer to the question '{question}' is '{answer}'. What context information and reasoning steps led to this answer? Provide the steps in reverse order.\"\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     try:\n",
    "#         text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "#         with torch.no_grad():\n",
    "#             response_ids = model.generate(**inputs, max_new_tokens=4096)[0][len(inputs.input_ids[0]):]\n",
    "#         response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "#         return extract_hops(response)\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error in backward reasoning: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # Check consistency between forward and backward hops\n",
    "# def check_consistency(forward_hops: List[str], backward_hops: List[str]) -> bool:\n",
    "#     if not forward_hops or not backward_hops:\n",
    "#         return False\n",
    "#     reversed_forward = forward_hops[::-1]\n",
    "#     if len(forward_hops) != len(backward_hops):\n",
    "#         return False\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     all_hops = forward_hops + backward_hops\n",
    "#     tfidf_matrix = vectorizer.fit_transform(all_hops)\n",
    "#     forward_vectors = tfidf_matrix[:len(forward_hops)]\n",
    "#     backward_vectors = tfidf_matrix[len(forward_hops):]\n",
    "#     similarities = [cosine_similarity(forward_vectors[i], backward_vectors[i])[0][0] for i in range(len(forward_hops))]\n",
    "#     avg_similarity = np.mean(similarities)\n",
    "#     return avg_similarity > 0.85\n",
    "\n",
    "# # Generate transitive closure and check compliance\n",
    "# def transitivity_check(hops: List[str]) -> Tuple[List[str], float]:\n",
    "#     inferred_facts = []\n",
    "#     for i in range(len(hops)):\n",
    "#         for j in range(i + 1, len(hops)):\n",
    "#             # Numeric transitivity\n",
    "#             if \" = \" in hops[i] and \"> \" in hops[j]:\n",
    "#                 value_match = re.search(r'(\\w+)\\s*=\\s*(\\d+)', hops[i])\n",
    "#                 comparison_match = re.search(r'(\\w+)\\s*>\\s*(\\d+)', hops[j])\n",
    "#                 if value_match and comparison_match and value_match.group(1) == comparison_match.group(1):\n",
    "#                     inferred_facts.append(f\"{value_match.group(1)} > {comparison_match.group(2)}\")\n",
    "#             # Temporal transitivity\n",
    "#             if \"before\" in hops[i] and \"after\" in hops[j]:\n",
    "#                 before_match = re.search(r'(\\w+)\\s+before\\s+(\\w+)', hops[i])\n",
    "#                 after_match = re.search(r'(\\w+)\\s+after\\s+(\\w+)', hops[j])\n",
    "#                 if before_match and after_match and before_match.group(2) == after_match.group(1):\n",
    "#                     inferred_facts.append(f\"{before_match.group(1)} before {after_match.group(2)}\")\n",
    "#     if not inferred_facts:\n",
    "#         return [], 1.0\n",
    "#     present_facts = sum(1 for fact in inferred_facts if any(fact in hop for hop in hops))\n",
    "#     compliance_rate = present_facts / len(inferred_facts)\n",
    "#     return inferred_facts, compliance_rate\n",
    "\n",
    "# # Compute F1 score\n",
    "# def compute_f1(pred_answer: str, true_answer: str) -> float:\n",
    "#     pred_tokens = set(pred_answer.lower().split())\n",
    "#     true_tokens = set(true_answer.lower().split())\n",
    "#     common = pred_tokens.intersection(true_tokens)\n",
    "#     if not common:\n",
    "#         return 0.0\n",
    "#     precision = len(common) / len(pred_tokens)\n",
    "#     recall = len(common) / len(true_tokens)\n",
    "#     return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "# # Visualize results\n",
    "# def visualize_results(metrics: Dict):\n",
    "#     # Overall metrics bar plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     overall_metrics = {\n",
    "#         \"Consistency\": metrics[\"consistency_score\"],\n",
    "#         \"Transitivity\": metrics[\"transitivity_compliance_rate\"],\n",
    "#         \"EM Score\": metrics[\"em_score\"],\n",
    "#         \"F1 Score\": metrics[\"f1_score\"],\n",
    "#         \"Reasoning Accuracy\": metrics[\"reasoning_step_accuracy\"]\n",
    "#     }\n",
    "#     sns.barplot(x=list(overall_metrics.values()), y=list(overall_metrics.keys()), palette=\"Blues_d\")\n",
    "#     plt.title(\"Overall Evaluation Metrics\", fontsize=14)\n",
    "#     plt.xlabel(\"Score\", fontsize=12)\n",
    "#     plt.xlim(0, 1)\n",
    "#     for i, v in enumerate(overall_metrics.values()):\n",
    "#         plt.text(v + 0.01, i, f\"{v:.3f}\", va=\"center\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Temporal sensitivity bar plot\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     temporal_metrics = metrics[\"temporal_sensitivity\"]\n",
    "#     sns.barplot(x=list(temporal_metrics.values()), y=list(temporal_metrics.keys()), palette=\"Greens_d\")\n",
    "#     plt.title(\"Temporal Sensitivity Scores by Factor\", fontsize=14)\n",
    "#     plt.xlabel(\"Score\", fontsize=12)\n",
    "#     plt.xlim(0, 1)\n",
    "#     for i, v in enumerate(temporal_metrics.values()):\n",
    "#         plt.text(v + 0.01, i, f\"{v:.3f}\", va=\"center\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Pie chart for factor distribution\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     factor_counts = metrics[\"factor_counts\"]\n",
    "#     plt.pie(factor_counts.values(), labels=factor_counts.keys(), autopct='%1.1f%%', colors=sns.color_palette(\"Pastel1\"))\n",
    "#     plt.title(\"Dataset Factor Distribution\", fontsize=14)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Main evaluation function\n",
    "# def evaluate_menatqa(dataset: List[Dict]):\n",
    "#     metrics = {\n",
    "#         \"consistency_score\": 0.0,\n",
    "#         \"transitivity_compliance_rate\": 0.0,\n",
    "#         \"em_score\": 0.0,\n",
    "#         \"f1_score\": 0.0,\n",
    "#         \"reasoning_step_accuracy\": 0.0,\n",
    "#         \"temporal_sensitivity\": {\"scope\": 0.0, \"order\": 0.0, \"counterfactual\": 0.0},\n",
    "#         \"total_samples\": len(dataset),\n",
    "#         \"factor_counts\": {\"scope\": 0, \"order\": 0, \"counterfactual\": 0}\n",
    "#     }\n",
    "    \n",
    "#     consistent_pairs = 0\n",
    "#     total_transitivity_rate = 0.0\n",
    "#     em_correct = 0\n",
    "#     total_f1 = 0.0\n",
    "#     total_hops = 0\n",
    "#     correct_hops = 0\n",
    "    \n",
    "#     for sample in tqdm(dataset, desc=\"Evaluating MenatQA\"):\n",
    "#         question = sample[\"question\"]\n",
    "#         context = sample[\"context\"]\n",
    "#         true_answer = sample[\"answer\"]\n",
    "#         factor = sample[\"factor\"]\n",
    "#         metrics[\"factor_counts\"][factor] += 1\n",
    "        \n",
    "#         pred_answer, forward_hops = forward_reasoning(question, context)\n",
    "#         backward_hops = backward_reasoning(question, context, pred_answer)\n",
    "        \n",
    "#         if check_consistency(forward_hops, backward_hops):\n",
    "#             consistent_pairs += 1\n",
    "        \n",
    "#         inferred_facts, compliance_rate = transitivity_check(forward_hops)\n",
    "#         total_transitivity_rate += compliance_rate\n",
    "        \n",
    "#         if pred_answer.strip().lower() == true_answer.strip().lower():\n",
    "#             em_correct += 1\n",
    "#             metrics[\"temporal_sensitivity\"][factor] += 1\n",
    "        \n",
    "#         f1 = compute_f1(pred_answer, true_answer)\n",
    "#         total_f1 += f1\n",
    "        \n",
    "#         total_hops += len(forward_hops)\n",
    "#         correct_hops += len(forward_hops) if f1 > 0.5 else 0\n",
    "    \n",
    "#     metrics[\"consistency_score\"] = consistent_pairs / len(dataset) if len(dataset) > 0 else 0.0\n",
    "#     metrics[\"transitivity_compliance_rate\"] = total_transitivity_rate / len(dataset) if len(dataset) > 0 else 0.0\n",
    "#     metrics[\"em_score\"] = em_correct / len(dataset) if len(dataset) > 0 else 0.0\n",
    "#     metrics[\"f1_score\"] = total_f1 / len(dataset) if len(dataset) > 0 else 0.0\n",
    "#     metrics[\"reasoning_step_accuracy\"] = correct_hops / total_hops if total_hops > 0 else 0.0\n",
    "#     for factor in metrics[\"temporal_sensitivity\"]:\n",
    "#         if metrics[\"factor_counts\"][factor] > 0:\n",
    "#             metrics[\"temporal_sensitivity\"][factor] /= metrics[\"factor_counts\"][factor]\n",
    "    \n",
    "#     # Print results to console\n",
    "#     print(\"\\n=== Evaluation Results ===\")\n",
    "#     print(f\"Total Samples: {metrics['total_samples']}\")\n",
    "#     print(f\"Consistency Score: {metrics['consistency_score']:.3f}\")\n",
    "#     print(f\"Transitivity Compliance Rate: {metrics['transitivity_compliance_rate']:.3f}\")\n",
    "#     print(f\"EM Score: {metrics['em_score']:.3f}\")\n",
    "#     print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n",
    "#     print(f\"Reasoning Step Accuracy: {metrics['reasoning_step_accuracy']:.3f}\")\n",
    "#     print(\"\\nTemporal Sensitivity Scores:\")\n",
    "#     for factor, score in metrics[\"temporal_sensitivity\"].items():\n",
    "#         print(f\"  {factor.capitalize()}: {score:.3f}\")\n",
    "#     print(\"\\nFactor Distribution:\")\n",
    "#     for factor, count in metrics[\"factor_counts\"].items():\n",
    "#         print(f\"  {factor.capitalize()}: {count} samples\")\n",
    "    \n",
    "#     # Visualize results\n",
    "#     visualize_results(metrics)\n",
    "#     return metrics\n",
    "\n",
    "# # Run evaluation\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         dataset = load_menatqa_dataset()\n",
    "#         # For testing, you can use a smaller subset to reduce runtime\n",
    "#         # dataset = dataset[:100]  # Uncomment to test with 100 samples\n",
    "#         results = evaluate_menatqa(dataset)\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39910ea0-1fc2-47bd-8646-4b1536e8f844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46a6c5-aded-4758-a237-b4f287902967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965624cd-85fb-44f2-ad2c-e6c00115e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MenatQA Logical Consistency & Transitivity Evaluation\n",
      "Using Qwen/Qwen3-0.6B + Semantic Analysis\n",
      "============================================================\n",
      "\n",
      "1. Initializing Qwen Model...\n",
      "Loading Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 06:03:56,723 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Device set to use cuda:0\n",
      "2025-06-01 06:03:57,087 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-06-01 06:03:57,087 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading semantic similarity model...\n",
      "\n",
      "2. Loading MenatQA dataset...\n",
      "Loaded MenatQA dataset with 999 examples\n",
      "Successfully loaded 999 examples\n",
      "\n",
      "3. Evaluating Logical Consistency and Transitivity on 999 questions...\n",
      "\n",
      "1. Evaluating Forward-Backward Consistency with Semantic Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating F/B Consistency:   0%|          | 0/999 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.78it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 217.07it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 217.24it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 216.79it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.45it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 229.64it/s]\n",
      "Evaluating F/B Consistency:   0%|          | 1/999 [00:30<8:31:42, 30.76s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.16it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 211.58it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.88it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.23it/s]\n",
      "Evaluating F/B Consistency:   0%|          | 2/999 [01:01<8:28:24, 30.60s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.33it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.05it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.38it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.44it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.61it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.60it/s]\n",
      "Evaluating F/B Consistency:   0%|          | 3/999 [01:31<8:27:25, 30.57s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.27it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.53it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.86it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.37it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.39it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.50it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.46it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 208.01it/s]\n",
      "Evaluating F/B Consistency:   0%|          | 4/999 [02:02<8:26:51, 30.56s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.88it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.04it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.87it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 221.24it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 217.25it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 219.29it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 5/999 [02:32<8:26:32, 30.58s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.16it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.05it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 6/999 [03:03<8:25:16, 30.53s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 185.57it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 216.22it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.37it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.32it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.27it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 222.67it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 236.74it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 7/999 [03:33<8:25:13, 30.56s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.43it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.99it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.72it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.56it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 8/999 [04:04<8:24:23, 30.54s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.75it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.26it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.79it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 9/999 [04:34<8:23:37, 30.52s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.35it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 211.76it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.45it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.34it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 210.43it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 10/999 [05:05<8:22:44, 30.50s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.47it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 212.12it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.24it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.31it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.53it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 11/999 [05:35<8:21:49, 30.48s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.90it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 208.87it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.36it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.98it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.57it/s]\n",
      "Evaluating F/B Consistency:   1%|          | 12/999 [06:06<8:21:41, 30.50s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.45it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.86it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.82it/s]\n",
      "Evaluating F/B Consistency:   1%|▏         | 13/999 [06:36<8:21:08, 30.50s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.62it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.30it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.77it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 212.94it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 209.97it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 213.23it/s]\n",
      "Evaluating F/B Consistency:   1%|▏         | 14/999 [07:07<8:20:33, 30.49s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 193.46it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 215.52it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 227.85it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 216.46it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 215.02it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 15/999 [07:38<8:25:15, 30.81s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.72it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.73it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 220.28it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.21it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 16/999 [08:09<8:23:16, 30.72s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 154.63it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 17/999 [08:39<8:21:25, 30.64s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 185.50it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.89it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.77it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.32it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 18/999 [09:10<8:19:59, 30.58s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.84it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.58it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.70it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.74it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 209.93it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 19/999 [09:40<8:18:40, 30.53s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.78it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.86it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.46it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.31it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 210.44it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 216.54it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 218.84it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 215.07it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 215.30it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 20/999 [10:11<8:17:25, 30.49s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.33it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 21/999 [10:41<8:18:37, 30.59s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 193.06it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.43it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 209.40it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.75it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 214.21it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 214.17it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 209.37it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 22/999 [11:12<8:17:26, 30.55s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.37it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.68it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.20it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.50it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 208.49it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 23/999 [11:42<8:16:13, 30.51s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.23it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.16it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.36it/s]\n",
      "Evaluating F/B Consistency:   2%|▏         | 24/999 [12:13<8:14:56, 30.46s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.99it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.22it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.51it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 208.54it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.44it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 25/999 [12:43<8:14:13, 30.44s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.34it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.47it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 210.54it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.86it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.20it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 26/999 [13:14<8:13:45, 30.45s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.60it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.93it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.27it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.86it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 27/999 [13:44<8:13:22, 30.46s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.40it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.84it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.43it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.98it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 28/999 [14:15<8:14:21, 30.55s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.25it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.94it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.73it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.83it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.15it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 29/999 [14:47<8:23:52, 31.17s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.24it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.87it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.90it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 157.92it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 30/999 [15:20<8:31:05, 31.65s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.30it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.83it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.22it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.10it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.43it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.95it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 151.17it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.76it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.12it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 31/999 [15:53<8:35:52, 31.98s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 158.11it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.79it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.82it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.06it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 32/999 [16:26<8:39:09, 32.21s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.42it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.80it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 157.94it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.03it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 158.29it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 33/999 [16:58<8:41:06, 32.37s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.04it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.57it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.81it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.23it/s]\n",
      "Evaluating F/B Consistency:   3%|▎         | 34/999 [17:31<8:40:37, 32.37s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.45it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.04it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.95it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.19it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.08it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.24it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.56it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.76it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.52it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.99it/s]\n",
      "Evaluating F/B Consistency:   4%|▎         | 35/999 [18:03<8:40:42, 32.41s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.08it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.85it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.78it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.33it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.34it/s]\n",
      "Evaluating F/B Consistency:   4%|▎         | 36/999 [18:36<8:40:09, 32.41s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 154.61it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.86it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.58it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.78it/s]\n",
      "Evaluating F/B Consistency:   4%|▎         | 37/999 [19:08<8:39:34, 32.41s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 154.16it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.44it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.19it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.52it/s]\n",
      "Evaluating F/B Consistency:   4%|▍         | 38/999 [19:41<8:40:11, 32.48s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.26it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.59it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 157.08it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.64it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.60it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.50it/s]\n",
      "Evaluating F/B Consistency:   4%|▍         | 39/999 [20:13<8:41:09, 32.57s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.62it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.83it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.13it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.80it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.53it/s]\n",
      "Evaluating F/B Consistency:   4%|▍         | 40/999 [20:46<8:39:07, 32.48s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.91it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.10it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 170.19it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.23it/s]\n",
      "Evaluating F/B Consistency:   4%|▍         | 41/999 [21:18<8:38:31, 32.48s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.04it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.77it/s]\n",
      "Evaluating F/B Consistency:   4%|▍         | 42/999 [21:51<8:37:41, 32.46s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.69it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.05it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.58it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.03it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.52it/s]\n",
      "Evaluating F/B Consistency:   4%|▍         | 43/999 [22:23<8:37:00, 32.45s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.21it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.32it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.36it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.10it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.67it/s]\n",
      "Evaluating F/B Consistency:   4%|▍         | 44/999 [22:56<8:36:47, 32.47s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 149.61it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.67it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.75it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.14it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.72it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.40it/s]\n",
      "Evaluating F/B Consistency:   5%|▍         | 45/999 [23:28<8:38:01, 32.58s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 145.65it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.62it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.60it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.21it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.69it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.57it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.07it/s]\n",
      "Evaluating F/B Consistency:   5%|▍         | 46/999 [24:01<8:38:50, 32.67s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.60it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.43it/s]\n",
      "Evaluating F/B Consistency:   5%|▍         | 47/999 [24:34<8:39:06, 32.72s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.90it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.20it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.98it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.55it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.67it/s]\n",
      "Evaluating F/B Consistency:   5%|▍         | 48/999 [25:07<8:38:58, 32.74s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.96it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.76it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.48it/s]\n",
      "Evaluating F/B Consistency:   5%|▍         | 49/999 [25:39<8:37:10, 32.66s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 149.99it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 158.13it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.20it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.79it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.16it/s]\n",
      "Evaluating F/B Consistency:   5%|▌         | 50/999 [26:12<8:36:56, 32.68s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.59it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.91it/s]\n",
      "Evaluating F/B Consistency:   5%|▌         | 51/999 [26:45<8:36:33, 32.69s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 158.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.21it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.62it/s]\n",
      "Evaluating F/B Consistency:   5%|▌         | 52/999 [27:17<8:35:24, 32.65s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.79it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.05it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.26it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.12it/s]\n",
      "Evaluating F/B Consistency:   5%|▌         | 53/999 [27:50<8:33:18, 32.56s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.96it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.98it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.85it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.86it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.78it/s]\n",
      "Evaluating F/B Consistency:   5%|▌         | 54/999 [28:22<8:30:03, 32.39s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.62it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.35it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.54it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 55/999 [28:54<8:27:12, 32.24s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.69it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.12it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.40it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.65it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.77it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 146.15it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.01it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.58it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.64it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.79it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.46it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 124.20it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.74it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.39it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.38it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.63it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 146.22it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.23it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.04it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 56/999 [29:25<8:24:39, 32.11s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.53it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 137.01it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.66it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.64it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.68it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.05it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.27it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.14it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.57it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.52it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.70it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.16it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 151.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 154.34it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.82it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.92it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.81it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.43it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 146.54it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 57/999 [29:57<8:22:07, 31.98s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.45it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 146.05it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.32it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.67it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.82it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 58/999 [30:29<8:19:46, 31.87s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.71it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.01it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.02it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.92it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 59/999 [31:00<8:17:25, 31.75s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.40it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.41it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.94it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.92it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.51it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 60/999 [31:32<8:16:16, 31.71s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.59it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.13it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 134.78it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 61/999 [32:03<8:15:21, 31.69s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.40it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 121.96it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.38it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.51it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.10it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.26it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 136.70it/s]\n",
      "Evaluating F/B Consistency:   6%|▌         | 62/999 [32:35<8:14:33, 31.67s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 120.43it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.37it/s]\n",
      "Evaluating F/B Consistency:   6%|▋         | 63/999 [33:07<8:14:22, 31.69s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.31it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.13it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 127.42it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.76it/s]\n",
      "Evaluating F/B Consistency:   6%|▋         | 64/999 [33:39<8:14:45, 31.75s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.16it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.69it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 140.81it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.83it/s]\n",
      "Evaluating F/B Consistency:   7%|▋         | 65/999 [34:11<8:14:38, 31.78s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.66it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 149.37it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.97it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 145.68it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.29it/s]\n",
      "Evaluating F/B Consistency:   7%|▋         | 66/999 [34:43<8:16:51, 31.95s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.54it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.87it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.72it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.06it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.36it/s]\n",
      "Evaluating F/B Consistency:   7%|▋         | 67/999 [35:15<8:15:21, 31.89s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.98it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 130.95it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.25it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 133.22it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.00it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.04it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 146.53it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.30it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 145.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.35it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.18it/s]\n",
      "Evaluating F/B Consistency:   7%|▋         | 68/999 [35:46<8:14:01, 31.84s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.03it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 139.29it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 138.71it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.87it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.76it/s]\n",
      "Evaluating F/B Consistency:   7%|▋         | 69/999 [36:18<8:12:45, 31.79s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Any, Set\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import networkx as nx\n",
    "import torch\n",
    "from pathlib import Path  # Added this import\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PROPER MENATQA DATA LOADING (from first code) ---\n",
    "def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "    \"\"\"Load MenatQA dataset from JSON file with automatic download.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        import urllib.request\n",
    "        print(f\"Downloading MenatQA to {file_path}...\")\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "            str(file_path)\n",
    "        )\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "    return data\n",
    "\n",
    "# --- Model Setup ---\n",
    "\n",
    "class QwenReasoningModel:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\", device=\"auto\"):\n",
    "        \"\"\"Initialize Qwen model for reasoning tasks.\"\"\"\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=device\n",
    "        )\n",
    "        \n",
    "        # Add padding token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=device,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Load semantic similarity model\n",
    "        print(\"Loading semantic similarity model...\")\n",
    "        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def generate_text(self, prompt: str, max_length: int = 512) -> str:\n",
    "        \"\"\"Generate text using Qwen model.\"\"\"\n",
    "        try:\n",
    "            result = self.generator(\n",
    "                prompt,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            generated_text = result[0]['generated_text']\n",
    "            # Remove the input prompt from the generated text\n",
    "            response = generated_text[len(prompt):].strip()\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error in text generation: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# --- Forward vs. Backward Reasoning Analysis ---\n",
    "\n",
    "def generate_forward_chain(question: str, model: QwenReasoningModel, context: str = \"\", max_length=512):\n",
    "    \"\"\"Generate forward reasoning chain: Q → (hop₁, hop₂, …) → A using Qwen model.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Given the following question, provide a step-by-step reasoning chain that leads to the answer.\n",
    "\n",
    "Question: {question}\n",
    "{f\"Context: {context}\" if context else \"\"}\n",
    "\n",
    "Please provide your reasoning in clear steps, where each step builds on the previous one:\n",
    "\n",
    "Step-by-step reasoning:\n",
    "1.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_text(prompt, max_length)\n",
    "        \n",
    "        # Parse the response into reasoning hops\n",
    "        hops = []\n",
    "        lines = response.split('\\n')\n",
    "        \n",
    "        current_step = \"\"\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if re.match(r'^\\d+\\.', line):  # New numbered step\n",
    "                if current_step:\n",
    "                    hops.append(current_step.strip())\n",
    "                current_step = re.sub(r'^\\d+\\.', '', line).strip()\n",
    "            elif line and not line.startswith('Answer:') and not line.startswith('Therefore:'):\n",
    "                current_step += \" \" + line\n",
    "            elif line.startswith('Answer:') or line.startswith('Therefore:'):\n",
    "                if current_step:\n",
    "                    hops.append(current_step.strip())\n",
    "                # Extract final answer\n",
    "                answer = line.replace('Answer:', '').replace('Therefore:', '').strip()\n",
    "                return hops, answer\n",
    "        \n",
    "        if current_step:\n",
    "            hops.append(current_step.strip())\n",
    "            \n",
    "        # Try to extract answer from the last hop or generate a simple one\n",
    "        answer = hops[-1] if hops else \"Unable to determine answer\"\n",
    "        \n",
    "        return hops, answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating forward chain: {e}\")\n",
    "        # Fallback to simple reasoning\n",
    "        return [f\"Analyzing the question: {question}\", \"Processing available information\", \"Determining the answer\"], \"Answer pending\"\n",
    "\n",
    "def generate_backward_chain(answer: str, question: str, model: QwenReasoningModel, context: str = \"\", max_length=512):\n",
    "    \"\"\"Generate backward reasoning chain: A → (hop₁, hop₂, …) → Q using Qwen model.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Given the following answer and question, work backwards to show the reasoning steps that led to this answer.\n",
    "\n",
    "Answer: {answer}\n",
    "Question: {question}\n",
    "{f\"Context: {context}\" if context else \"\"}\n",
    "\n",
    "Starting from the answer, trace back the logical steps that would lead to this conclusion:\n",
    "\n",
    "Backward reasoning (from answer to question):\n",
    "1. Starting with the answer: {answer}\n",
    "2.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_text(prompt, max_length)\n",
    "        \n",
    "        # Parse the response into reasoning hops\n",
    "        hops = []\n",
    "        lines = response.split('\\n')\n",
    "        \n",
    "        current_step = \"\"\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if re.match(r'^\\d+\\.', line):  # New numbered step\n",
    "                if current_step:\n",
    "                    hops.append(current_step.strip())\n",
    "                current_step = re.sub(r'^\\d+\\.', '', line).strip()\n",
    "            elif line and not line.lower().startswith('question:'):\n",
    "                current_step += \" \" + line\n",
    "            elif line.lower().startswith('question:'):\n",
    "                if current_step:\n",
    "                    hops.append(current_step.strip())\n",
    "                break\n",
    "                \n",
    "        if current_step:\n",
    "            hops.append(current_step.strip())\n",
    "            \n",
    "        return hops\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating backward chain: {e}\")\n",
    "        # Fallback reasoning\n",
    "        return [f\"The answer '{answer}' was determined\", \"This required analyzing the question\", f\"The original question was: {question}\"]\n",
    "\n",
    "def compute_semantic_similarity(text1: str, text2: str, semantic_model: SentenceTransformer) -> float:\n",
    "    \"\"\"Compute semantic similarity between two texts using sentence transformers.\"\"\"\n",
    "    try:\n",
    "        embeddings = semantic_model.encode([text1, text2])\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        return float(similarity)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing semantic similarity: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_forward_backward_consistency(df: pd.DataFrame, model: QwenReasoningModel) -> Dict:\n",
    "    \"\"\"Evaluate consistency between forward and backward reasoning chains using semantic similarity.\"\"\"\n",
    "    consistency_scores = []\n",
    "    hop_consistency_matrix = []\n",
    "    total_evaluated = 0\n",
    "    detailed_examples = []\n",
    "\n",
    "    # Use all data points instead of sampling\n",
    "    for idx in tqdm(range(len(df)), desc=\"Evaluating F/B Consistency\"):\n",
    "        try:\n",
    "            row = df.iloc[idx]\n",
    "            question = row['question']\n",
    "            context = row.get('context', '') or row.get('passage', '') or ''\n",
    "\n",
    "            # Generate forward reasoning chain\n",
    "            forward_hops, forward_answer = generate_forward_chain(question, model, context)\n",
    "            \n",
    "            if not forward_hops:\n",
    "                continue\n",
    "\n",
    "            # Generate backward reasoning chain\n",
    "            backward_hops = generate_backward_chain(forward_answer, question, model, context)\n",
    "            \n",
    "            if not backward_hops:\n",
    "                continue\n",
    "\n",
    "            # Check consistency - compare each backward hop with corresponding forward hop in reverse\n",
    "            hop_consistencies = []\n",
    "            min_hops = min(len(forward_hops), len(backward_hops))\n",
    "\n",
    "            for i in range(min_hops):\n",
    "                forward_idx = len(forward_hops) - 1 - i\n",
    "                backward_idx = i\n",
    "\n",
    "                if forward_idx >= 0 and backward_idx < len(backward_hops):\n",
    "                    similarity = compute_semantic_similarity(\n",
    "                        forward_hops[forward_idx], \n",
    "                        backward_hops[backward_idx], \n",
    "                        model.semantic_model\n",
    "                    )\n",
    "                    hop_consistencies.append(similarity)\n",
    "\n",
    "            # Overall consistency for this example\n",
    "            if hop_consistencies:\n",
    "                avg_consistency = sum(hop_consistencies) / len(hop_consistencies)\n",
    "                consistency_scores.append(avg_consistency)\n",
    "                hop_consistency_matrix.append(hop_consistencies)\n",
    "                total_evaluated += 1\n",
    "                \n",
    "                # Store detailed example (only first 5 for memory efficiency)\n",
    "                if len(detailed_examples) < 5:\n",
    "                    detailed_examples.append({\n",
    "                        'question': question,\n",
    "                        'forward_hops': forward_hops,\n",
    "                        'backward_hops': backward_hops,\n",
    "                        'hop_similarities': hop_consistencies,\n",
    "                        'avg_consistency': avg_consistency\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate overall consistency score\n",
    "    overall_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0\n",
    "\n",
    "    # Calculate consistency by hop position\n",
    "    hop_position_consistency = defaultdict(list)\n",
    "    for consistencies in hop_consistency_matrix:\n",
    "        for i, score in enumerate(consistencies):\n",
    "            hop_position_consistency[i].append(score)\n",
    "\n",
    "    avg_hop_position_consistency = {\n",
    "        pos: sum(scores) / len(scores) if scores else 0\n",
    "        for pos, scores in hop_position_consistency.items()\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'overall_consistency': overall_consistency,\n",
    "        'by_hop_position': avg_hop_position_consistency,\n",
    "        'total_evaluated': total_evaluated,\n",
    "        'detailed_examples': detailed_examples\n",
    "    }\n",
    "\n",
    "def evaluate_transitivity(df: pd.DataFrame, model: QwenReasoningModel) -> Dict:\n",
    "    \"\"\"Evaluate transitivity in reasoning chains with semantic similarity.\"\"\"\n",
    "    total_evaluated = 0\n",
    "    transitivity_scores = []\n",
    "    detailed_examples = []\n",
    "\n",
    "    # Use all data points instead of sampling\n",
    "    for idx in tqdm(range(len(df)), desc=\"Evaluating Transitivity\"):\n",
    "        try:\n",
    "            row = df.iloc[idx]\n",
    "            question = row['question']\n",
    "            context = row.get('context', '') or row.get('passage', '') or ''\n",
    "\n",
    "            # Generate reasoning hops using the model\n",
    "            forward_hops, _ = generate_forward_chain(question, model, context)\n",
    "            \n",
    "            if len(forward_hops) < 3:  # Need at least 3 hops for transitivity\n",
    "                continue\n",
    "\n",
    "            # Extract facts from each hop\n",
    "            all_facts = []\n",
    "            hop_facts = []\n",
    "            \n",
    "            for hop in forward_hops:\n",
    "                facts = extract_facts_from_hop(hop)\n",
    "                all_facts.extend(facts)\n",
    "                hop_facts.append(facts)\n",
    "\n",
    "            # Skip if not enough facts for transitivity\n",
    "            if len(all_facts) < 2:\n",
    "                continue\n",
    "\n",
    "            # Compute transitive closure\n",
    "            inferred_facts = compute_transitive_closure(all_facts)\n",
    "\n",
    "            # Skip if no inferred facts\n",
    "            if not inferred_facts:\n",
    "                continue\n",
    "\n",
    "            # Check if inferred facts are contained in later hops\n",
    "            facts_found = 0\n",
    "            facts_total = len(inferred_facts)\n",
    "            fact_details = []\n",
    "\n",
    "            for fact in inferred_facts:\n",
    "                found_in_hops = []\n",
    "                # Check each hop after the first one\n",
    "                for i, hop in enumerate(forward_hops[1:], 1):\n",
    "                    if fact_contained_in_hop(fact, hop, model.semantic_model):\n",
    "                        found_in_hops.append(i)\n",
    "                        \n",
    "                if found_in_hops:\n",
    "                    facts_found += 1\n",
    "                    \n",
    "                fact_details.append({\n",
    "                    'fact': f\"{fact['subject']} {fact['relation']} {fact['object']}\",\n",
    "                    'found_in_hops': found_in_hops,\n",
    "                    'category': fact.get('category', 'general')\n",
    "                })\n",
    "\n",
    "            transitivity_score = facts_found / facts_total if facts_total > 0 else 0\n",
    "            transitivity_scores.append(transitivity_score)\n",
    "            total_evaluated += 1\n",
    "            \n",
    "            # Store detailed example (only first 5 for memory efficiency)\n",
    "            if len(detailed_examples) < 5:\n",
    "                detailed_examples.append({\n",
    "                    'question': question,\n",
    "                    'hops': forward_hops,\n",
    "                    'extracted_facts': [f\"{f['subject']} {f['relation']} {f['object']}\" for f in all_facts],\n",
    "                    'inferred_facts': fact_details,\n",
    "                    'transitivity_score': transitivity_score\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing transitivity for example {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate overall transitivity score\n",
    "    overall_transitivity = sum(transitivity_scores) / len(transitivity_scores) if transitivity_scores else 0\n",
    "\n",
    "    return {\n",
    "        'overall_transitivity': overall_transitivity,\n",
    "        'total_evaluated': total_evaluated,\n",
    "        'scores': transitivity_scores,\n",
    "        'detailed_examples': detailed_examples\n",
    "    }\n",
    "\n",
    "# --- Enhanced Fact Extraction for MenatQA ---\n",
    "\n",
    "def extract_facts_from_hop(hop: str) -> List[Dict]:\n",
    "    \"\"\"Extract facts from a reasoning hop with patterns specific to MenatQA.\"\"\"\n",
    "    facts = []\n",
    "    \n",
    "    # MenatQA specific patterns for Middle Eastern and North African context\n",
    "    \n",
    "    # Geographic relationships\n",
    "    geo_patterns = [\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+is\\s+(?:located\\s+)?in\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', 'located_in'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+borders\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', 'borders'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+is\\s+(?:the\\s+)?capital\\s+of\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', 'capital_of'),\n",
    "    ]\n",
    "    \n",
    "    # Historical relationships\n",
    "    hist_patterns = [\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+(?:ruled|conquered|controlled)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', 'ruled'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+was\\s+(?:founded|established)\\s+(?:in\\s+)?(\\d+)', 'founded_in'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+(?:occurred|happened)\\s+(?:in\\s+)?(\\d+)', 'occurred_in'),\n",
    "    ]\n",
    "    \n",
    "    # Cultural/Religious relationships\n",
    "    cultural_patterns = [\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+(?:speaks|uses)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', 'speaks'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+(?:follows|practices)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', 'practices'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+is\\s+(?:a\\s+)?([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+(?:country|nation|state)', 'is_type'),\n",
    "    ]\n",
    "    \n",
    "    # Numerical relationships\n",
    "    numerical_patterns = [\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+has\\s+(?:a\\s+)?population\\s+of\\s+([0-9,]+)', 'population'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+covers\\s+([0-9,]+)', 'area'),\n",
    "        (r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+is\\s+(larger|smaller|bigger)\\s+than\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', 'size_comparison'),\n",
    "    ]\n",
    "    \n",
    "    # Apply all pattern groups\n",
    "    all_patterns = [\n",
    "        (geo_patterns, 'geographic'),\n",
    "        (hist_patterns, 'historical'), \n",
    "        (cultural_patterns, 'cultural'),\n",
    "        (numerical_patterns, 'numerical')\n",
    "    ]\n",
    "    \n",
    "    for pattern_group, category in all_patterns:\n",
    "        for pattern, relation in pattern_group:\n",
    "            matches = re.finditer(pattern, hop, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                groups = match.groups()\n",
    "                if len(groups) >= 2:\n",
    "                    facts.append({\n",
    "                        'subject': groups[0].strip(),\n",
    "                        'relation': relation,\n",
    "                        'object': groups[1].strip(),\n",
    "                        'category': category\n",
    "                    })\n",
    "    \n",
    "    # General is/was patterns\n",
    "    general_patterns = re.finditer(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+(?:is|was|are|were)\\s+([^,.;]+)', hop)\n",
    "    for match in general_patterns:\n",
    "        facts.append({\n",
    "            'subject': match.group(1).strip(),\n",
    "            'relation': 'is',\n",
    "            'object': match.group(2).strip(),\n",
    "            'category': 'general'\n",
    "        })\n",
    "    \n",
    "    return facts\n",
    "\n",
    "def compute_transitive_closure(facts: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Compute transitive closure of facts with MenatQA-specific rules.\"\"\"\n",
    "    inferred_facts = []\n",
    "    \n",
    "    # Create a directed graph to represent relationships\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add facts as edges in the graph with relation types\n",
    "    for fact in facts:\n",
    "        G.add_edge(fact['subject'], fact['object'], \n",
    "                   relation=fact['relation'], \n",
    "                   category=fact.get('category', 'general'))\n",
    "    \n",
    "    # Apply transitivity rules specific to geographical and historical relationships\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if u != v and G.has_edge(u, v):\n",
    "                for w in G.nodes():\n",
    "                    if v != w and w != u and G.has_edge(v, w):\n",
    "                        uv_relation = G[u][v]['relation']\n",
    "                        vw_relation = G[v][w]['relation']\n",
    "                        \n",
    "                        # Geographic transitivity\n",
    "                        if uv_relation == 'located_in' and vw_relation == 'located_in':\n",
    "                            # If A is in B and B is in C, then A is in C\n",
    "                            inferred_facts.append({\n",
    "                                'subject': u,\n",
    "                                'relation': 'located_in',\n",
    "                                'object': w,\n",
    "                                'category': 'geographic',\n",
    "                                'inferred': True\n",
    "                            })\n",
    "                        \n",
    "                        # Historical transitivity  \n",
    "                        elif uv_relation == 'ruled' and vw_relation == 'ruled':\n",
    "                            # If A ruled B and B ruled C, then A influenced C\n",
    "                            inferred_facts.append({\n",
    "                                'subject': u,\n",
    "                                'relation': 'influenced',\n",
    "                                'object': w,\n",
    "                                'category': 'historical',\n",
    "                                'inferred': True\n",
    "                            })\n",
    "                        \n",
    "                        # Identity transitivity\n",
    "                        elif uv_relation == 'is' and vw_relation == 'is':\n",
    "                            inferred_facts.append({\n",
    "                                'subject': u,\n",
    "                                'relation': 'is',\n",
    "                                'object': w,\n",
    "                                'category': 'general',\n",
    "                                'inferred': True\n",
    "                            })\n",
    "                        \n",
    "                        # Size comparison transitivity\n",
    "                        elif uv_relation == 'larger' and vw_relation == 'larger':\n",
    "                            inferred_facts.append({\n",
    "                                'subject': u,\n",
    "                                'relation': 'larger',\n",
    "                                'object': w,\n",
    "                                'category': 'numerical',\n",
    "                                'inferred': True\n",
    "                            })\n",
    "    \n",
    "    return inferred_facts\n",
    "\n",
    "def fact_contained_in_hop(fact: Dict, hop: str, semantic_model: SentenceTransformer) -> bool:\n",
    "    \"\"\"Check if a fact is mentioned in a reasoning hop using semantic similarity.\"\"\"\n",
    "    # Create fact statement\n",
    "    fact_statement = f\"{fact['subject']} {fact['relation']} {fact['object']}\"\n",
    "    \n",
    "    # Basic keyword matching\n",
    "    subject_present = fact['subject'].lower() in hop.lower()\n",
    "    object_present = fact['object'].lower() in hop.lower()\n",
    "    \n",
    "    if subject_present and object_present:\n",
    "        # Both entities present - check semantic similarity for the relationship\n",
    "        try:\n",
    "            similarity = compute_semantic_similarity(fact_statement, hop, semantic_model)\n",
    "            return similarity > 0.6  # Threshold for semantic similarity\n",
    "        except:\n",
    "            return True  # Fallback to basic presence check\n",
    "    \n",
    "    # If basic keyword matching fails, try semantic similarity\n",
    "    try:\n",
    "        similarity = compute_semantic_similarity(fact_statement, hop, semantic_model)\n",
    "        return similarity > 0.7  # Higher threshold when keywords not directly present\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def evaluate_transitivity(df: pd.DataFrame, model: QwenReasoningModel) -> Dict:\n",
    "    \"\"\"Evaluate transitivity in reasoning chains with semantic similarity.\"\"\"\n",
    "    total_evaluated = 0\n",
    "    transitivity_scores = []\n",
    "    detailed_examples = []\n",
    "\n",
    "    # Sample subset for analysis\n",
    "    sample_size = min(50, len(df))\n",
    "    sample_indices = random.sample(range(len(df)), sample_size)\n",
    "\n",
    "    for idx in tqdm(sample_indices, desc=\"Evaluating Transitivity\"):\n",
    "        try:\n",
    "            row = df.iloc[idx]\n",
    "            question = row['question']\n",
    "            context = row.get('context', '') or row.get('passage', '') or ''\n",
    "\n",
    "            # Generate reasoning hops using the model\n",
    "            forward_hops, _ = generate_forward_chain(question, model, context)\n",
    "            \n",
    "            if len(forward_hops) < 3:  # Need at least 3 hops for transitivity\n",
    "                continue\n",
    "\n",
    "            # Extract facts from each hop\n",
    "            all_facts = []\n",
    "            hop_facts = []\n",
    "            \n",
    "            for hop in forward_hops:\n",
    "                facts = extract_facts_from_hop(hop)\n",
    "                all_facts.extend(facts)\n",
    "                hop_facts.append(facts)\n",
    "\n",
    "            # Skip if not enough facts for transitivity\n",
    "            if len(all_facts) < 2:\n",
    "                continue\n",
    "\n",
    "            # Compute transitive closure\n",
    "            inferred_facts = compute_transitive_closure(all_facts)\n",
    "\n",
    "            # Skip if no inferred facts\n",
    "            if not inferred_facts:\n",
    "                continue\n",
    "\n",
    "            # Check if inferred facts are contained in later hops\n",
    "            facts_found = 0\n",
    "            facts_total = len(inferred_facts)\n",
    "            fact_details = []\n",
    "\n",
    "            for fact in inferred_facts:\n",
    "                found_in_hops = []\n",
    "                # Check each hop after the first one\n",
    "                for i, hop in enumerate(forward_hops[1:], 1):\n",
    "                    if fact_contained_in_hop(fact, hop, model.semantic_model):\n",
    "                        found_in_hops.append(i)\n",
    "                        \n",
    "                if found_in_hops:\n",
    "                    facts_found += 1\n",
    "                    \n",
    "                fact_details.append({\n",
    "                    'fact': f\"{fact['subject']} {fact['relation']} {fact['object']}\",\n",
    "                    'found_in_hops': found_in_hops,\n",
    "                    'category': fact.get('category', 'general')\n",
    "                })\n",
    "\n",
    "            transitivity_score = facts_found / facts_total if facts_total > 0 else 0\n",
    "            transitivity_scores.append(transitivity_score)\n",
    "            total_evaluated += 1\n",
    "            \n",
    "            # Store detailed example\n",
    "            detailed_examples.append({\n",
    "                'question': question,\n",
    "                'hops': forward_hops,\n",
    "                'extracted_facts': [f\"{f['subject']} {f['relation']} {f['object']}\" for f in all_facts],\n",
    "                'inferred_facts': fact_details,\n",
    "                'transitivity_score': transitivity_score\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing transitivity for example {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate overall transitivity score\n",
    "    overall_transitivity = sum(transitivity_scores) / len(transitivity_scores) if transitivity_scores else 0\n",
    "\n",
    "    return {\n",
    "        'overall_transitivity': overall_transitivity,\n",
    "        'total_evaluated': total_evaluated,\n",
    "        'scores': transitivity_scores,\n",
    "        'detailed_examples': detailed_examples[:5]  # Keep first 5 for inspection\n",
    "    }\n",
    "\n",
    "# --- Enhanced Visualization Functions ---\n",
    "\n",
    "def visualize_consistency_results(consistency_results: Dict, save_path=None):\n",
    "    \"\"\"Visualize forward-backward consistency results with semantic evaluation.\"\"\"\n",
    "    overall = consistency_results['overall_consistency']\n",
    "    by_position = consistency_results['by_hop_position']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot by hop position\n",
    "    positions = sorted(by_position.keys())\n",
    "    scores = [by_position[p] for p in positions]\n",
    "\n",
    "    ax1.bar([str(p+1) for p in positions], scores, color='#3498db', alpha=0.8)\n",
    "    ax1.axhline(y=overall, color='r', linestyle='--', label=f'Overall: {overall:.3f}')\n",
    "    ax1.set_xlabel('Hop Position', fontsize=12)\n",
    "    ax1.set_ylabel('Semantic Consistency Score', fontsize=12)\n",
    "    ax1.set_title('Forward-Backward Consistency by Hop Position', fontsize=14)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Distribution of consistency scores\n",
    "    if 'detailed_examples' in consistency_results:\n",
    "        all_similarities = []\n",
    "        for example in consistency_results['detailed_examples']:\n",
    "            all_similarities.extend(example.get('hop_similarities', []))\n",
    "        \n",
    "        if all_similarities:\n",
    "            ax2.hist(all_similarities, bins=20, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "            ax2.axvline(x=np.mean(all_similarities), color='r', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(all_similarities):.3f}')\n",
    "            ax2.set_xlabel('Semantic Similarity Score', fontsize=12)\n",
    "            ax2.set_ylabel('Frequency', fontsize=12)\n",
    "            ax2.set_title('Distribution of Hop-wise Semantic Similarities', fontsize=14)\n",
    "            ax2.legend()\n",
    "            ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Consistency visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_transitivity_results(transitivity_results: Dict, save_path=None):\n",
    "    \"\"\"Visualize transitivity results with category breakdown.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Overall transitivity score\n",
    "    overall = transitivity_results['overall_transitivity']\n",
    "    scores = transitivity_results['scores']\n",
    "    \n",
    "    # Score distribution\n",
    "    ax1.hist(scores, bins=15, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(x=overall, color='blue', linestyle='--', label=f'Mean: {overall:.3f}')\n",
    "    ax1.set_xlabel('Transitivity Score', fontsize=12)\n",
    "    ax1.set_ylabel('Frequency', fontsize=12)\n",
    "    ax1.set_title('Distribution of Transitivity Scores', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Category breakdown if available\n",
    "    if 'detailed_examples' in transitivity_results:\n",
    "        category_counts = defaultdict(int)\n",
    "        category_found = defaultdict(int)\n",
    "        \n",
    "        for example in transitivity_results['detailed_examples']:\n",
    "            for fact_detail in example.get('inferred_facts', []):\n",
    "                category = fact_detail.get('category', 'general')\n",
    "                category_counts[category] += 1\n",
    "                if fact_detail.get('found_in_hops'):\n",
    "                    category_found[category] += 1\n",
    "        \n",
    "        categories = list(category_counts.keys())\n",
    "        if categories:\n",
    "            success_rates = [category_found[cat] / category_counts[cat] if category_counts[cat] > 0 else 0 \n",
    "                           for cat in categories]\n",
    "            \n",
    "            bars = ax2.bar(categories, success_rates, color=['#9b59b6', '#f39c12', '#1abc9c', '#34495e'], alpha=0.8)\n",
    "            ax2.set_ylabel('Transitivity Success Rate', fontsize=12)\n",
    "            ax2.set_title('Transitivity by Fact Category', fontsize=14)\n",
    "            ax2.set_ylim(0, 1.05)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, rate in zip(bars, success_rates):\n",
    "                height = bar.get_height()\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                        f'{rate:.2f}', ha='center', va='bottom')\n",
    "            \n",
    "            ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Transitivity visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_combined_results(consistency_results: Dict, transitivity_results: Dict, save_path=None):\n",
    "    \"\"\"Visualize combined consistency and transitivity results.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Data for plotting\n",
    "    metrics = ['F-B Consistency\\n(Semantic)', 'Transitivity\\n(Graph-based)']\n",
    "    values = [\n",
    "        consistency_results['overall_consistency'],\n",
    "        transitivity_results['overall_transitivity']\n",
    "    ]\n",
    "    colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "    # Create bar plot\n",
    "    bars = ax.bar(metrics, values, color=colors, alpha=0.8, width=0.6)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_title('MenatQA Logical Consistency & Transitivity Evaluation\\n(Qwen/Qwen3-0.6B + Semantic Analysis)', fontsize=16)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add evaluation details as text\n",
    "    eval_text = f\"Evaluated: {consistency_results['total_evaluated']} examples (Consistency)\\n\"\n",
    "    eval_text += f\"Evaluated: {transitivity_results['total_evaluated']} examples (Transitivity)\"\n",
    "    ax.text(0.02, 0.98, eval_text, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Combined visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Main Execution Functions ---\n",
    "\n",
    "def evaluate_logical_consistency(df: pd.DataFrame, model: QwenReasoningModel) -> Dict:\n",
    "    \"\"\"Run complete logical consistency and transitivity evaluation with Qwen model.\"\"\"\n",
    "    print(\"\\n1. Evaluating Forward-Backward Consistency with Semantic Analysis...\")\n",
    "    consistency_results = evaluate_forward_backward_consistency(df, model)\n",
    "\n",
    "    print(f\"\\nForward-Backward Consistency Results:\")\n",
    "    print(f\"Overall Semantic Consistency: {consistency_results['overall_consistency']:.4f}\")\n",
    "    print(f\"Total examples evaluated: {consistency_results['total_evaluated']}\")\n",
    "    print(\"\\nConsistency by Hop Position:\")\n",
    "    for pos, score in sorted(consistency_results['by_hop_position'].items()):\n",
    "        print(f\"Hop {pos+1}: {score:.4f}\")\n",
    "\n",
    "    print(\"\\n2. Evaluating Transitivity with Graph-based Analysis...\")\n",
    "    transitivity_results = evaluate_transitivity(df, model)\n",
    "\n",
    "    print(f\"\\nTransitivity Results:\")\n",
    "    print(f\"Overall Transitivity: {transitivity_results['overall_transitivity']:.4f}\")\n",
    "    print(f\"Total examples evaluated: {transitivity_results['total_evaluated']}\")\n",
    "\n",
    "    # Show sample detailed analysis\n",
    "    if transitivity_results['detailed_examples']:\n",
    "        print(f\"\\nSample Transitivity Analysis:\")\n",
    "        example = transitivity_results['detailed_examples'][0]\n",
    "        print(f\"Question: {example['question'][:100]}...\")\n",
    "        print(f\"Inferred Facts Found: {len([f for f in example['inferred_facts'] if f['found_in_hops']])}/{len(example['inferred_facts'])}\")\n",
    "\n",
    "    # Combine results\n",
    "    combined_results = {\n",
    "        'consistency': consistency_results,\n",
    "        'transitivity': transitivity_results,\n",
    "        'model_info': {\n",
    "            'model_name': 'Qwen/Qwen3-0.6B',\n",
    "            'semantic_model': 'Qwen/Qwen3-0.6B',\n",
    "            'evaluation_type': 'semantic_similarity'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the complete logical consistency and transitivity evaluation pipeline with Qwen model.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MenatQA Logical Consistency & Transitivity Evaluation\")\n",
    "    print(\"Using Qwen/Qwen3-0.6B + Semantic Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"\\n1. Initializing Qwen Model...\")\n",
    "    try:\n",
    "        qwen_model = QwenReasoningModel(\"Qwen/Qwen3-0.6B\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Falling back to alternative model...\")\n",
    "        qwen_model = QwenReasoningModel(\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "    # Load MenatQA dataset using the proper function\n",
    "    print(\"\\n2. Loading MenatQA dataset...\")\n",
    "    try:\n",
    "        menatqa_data = load_menatqa_dataset('./MenatQA.json')  # Now uses the proper function\n",
    "        df = pd.DataFrame(menatqa_data)\n",
    "        print(f\"Successfully loaded {len(df)} examples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        # Create sample data for testing if download fails\n",
    "        df = pd.DataFrame([\n",
    "            {'question': 'What is the capital of Egypt?', 'answer': 'Cairo'},\n",
    "            {'question': 'Which country borders Jordan to the south?', 'answer': 'Saudi Arabia'},\n",
    "            {'question': 'When was the Ottoman Empire founded?', 'answer': '1299'},\n",
    "        ])\n",
    "        print(f\"Using sample data with {len(df)} examples\")\n",
    "\n",
    "    # Create results directory\n",
    "    results_dir = './logical_consistency_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Evaluate logical consistency and transitivity\n",
    "    print(f\"\\n3. Evaluating Logical Consistency and Transitivity on {len(df)} questions...\")\n",
    "    results = evaluate_logical_consistency(df, qwen_model)\n",
    "\n",
    "    # Generate visualizations\n",
    "    print(\"\\n4. Generating Visualizations...\")\n",
    "\n",
    "    # Consistency visualization\n",
    "    consistency_viz_path = f\"{results_dir}/semantic_consistency_analysis.png\"\n",
    "    visualize_consistency_results(results['consistency'], save_path=consistency_viz_path)\n",
    "\n",
    "    # Transitivity visualization\n",
    "    transitivity_viz_path = f\"{results_dir}/transitivity_analysis.png\"\n",
    "    visualize_transitivity_results(results['transitivity'], save_path=transitivity_viz_path)\n",
    "\n",
    "    # Combined visualization\n",
    "    combined_viz_path = f\"{results_dir}/combined_logical_metrics.png\"\n",
    "    visualize_combined_results(results['consistency'], results['transitivity'], save_path=combined_viz_path)\n",
    "\n",
    "    # Save detailed results\n",
    "    print(\"\\n5. Saving Detailed Results...\")\n",
    "    \n",
    "    # Convert to serializable format\n",
    "    serializable_results = {\n",
    "        'model_info': results['model_info'],\n",
    "        'consistency': {\n",
    "            'overall_consistency': float(results['consistency']['overall_consistency']),\n",
    "            'by_hop_position': {int(k): float(v) for k, v in results['consistency']['by_hop_position'].items()},\n",
    "            'total_evaluated': int(results['consistency']['total_evaluated'])\n",
    "        },\n",
    "        'transitivity': {\n",
    "            'overall_transitivity': float(results['transitivity']['overall_transitivity']),\n",
    "            'total_evaluated': int(results['transitivity']['total_evaluated']),\n",
    "            'score_distribution': {\n",
    "                'mean': float(np.mean(results['transitivity']['scores'])) if results['transitivity']['scores'] else 0,\n",
    "                'std': float(np.std(results['transitivity']['scores'])) if results['transitivity']['scores'] else 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save results as JSON\n",
    "    with open(f\"{results_dir}/semantic_evaluation_results.json\", 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "    # Save detailed examples for inspection\n",
    "    detailed_analysis = {\n",
    "        'consistency_examples': results['consistency'].get('detailed_examples', []),\n",
    "        'transitivity_examples': results['transitivity'].get('detailed_examples', [])\n",
    "    }\n",
    "    \n",
    "    with open(f\"{results_dir}/detailed_examples_analysis.json\", 'w') as f:\n",
    "        json.dump(detailed_analysis, f, indent=2)\n",
    "\n",
    "    print(f\"\\n6. Evaluation Summary:\")\n",
    "    print(f\"   • Semantic Consistency Score: {results['consistency']['overall_consistency']:.4f}\")\n",
    "    print(f\"   • Graph-based Transitivity Score: {results['transitivity']['overall_transitivity']:.4f}\")\n",
    "    print(f\"   • Model Used: {results['model_info']['model_name']}\")\n",
    "    print(f\"   • Semantic Model: {results['model_info']['semantic_model']}\")\n",
    "    print(f\"   • Results saved to: {results_dir}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Evaluation Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ea818-aa22-4125-972a-d09937d869a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655572aa-7f1b-4214-9dc6-7aca6965174f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
