{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a30f7-c3d9-44c2-b55f-8fd93f5ada7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Counterfactual Reasoning Evaluation ===\n",
      "\n",
      "1. Loading and preprocessing dataset...\n",
      "Loaded MenatQA dataset with 999 examples\n",
      "Dataset preprocessed: 999 questions\n",
      "1-hop questions: 0 (0.0%)\n",
      "2-hop questions: 135 (13.5%)\n",
      "3-hop questions: 698 (69.9%)\n",
      "4-hop questions: 166 (16.6%)\n",
      "\n",
      "2. Setting up model...\n",
      "Loading Qwen/Qwen3-0.6B on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Running counterfactual evaluation...\n",
      "Running counterfactual evaluation on 50 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [06:57<03:48, 12.70s/it]"
     ]
    }
   ],
   "source": [
    "# # Here's the completion of the `counterfactual_reasoning_evaluation.py` file:\n",
    "\n",
    "# # ```python\n",
    "# import json\n",
    "# import re\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import random\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Tuple, Any, Optional\n",
    "# import copy\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# # --- DATA LOADING ---\n",
    "# def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "#     file_path = Path(file_path)\n",
    "    \n",
    "#     if not file_path.exists():\n",
    "#         import urllib.request\n",
    "#         print(f\"Downloading MenatQA to {file_path}...\")\n",
    "#         urllib.request.urlretrieve(\n",
    "#             \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "#             str(file_path)\n",
    "#         )\n",
    "    \n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "#     return data\n",
    "\n",
    "# def extract_reasoning_hops(example):\n",
    "#     question = example.get('question', '')\n",
    "#     answer = example.get('answer', '')\n",
    "#     q_type = example.get('type', '')\n",
    "#     time_scope = example.get('time_scope', '')\n",
    "    \n",
    "#     sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "#     clauses = len([c for c in re.split(r'and|or|but|because|when|if', question.lower()) if c.strip()])\n",
    "#     capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "#     complexity_score = 1\n",
    "#     complexity_score += min(1, len(sentences) - 1)\n",
    "#     complexity_score += min(1, (clauses - 1) // 2)\n",
    "#     complexity_score += min(1, capitalized_words // 3)\n",
    "#     if time_scope:\n",
    "#         complexity_score += 1\n",
    "#     complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "#     return complexity_score\n",
    "\n",
    "# def preprocess_dataset(data):\n",
    "#     processed_data = []\n",
    "#     hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "#     for item in data:\n",
    "#         complexity_score = extract_reasoning_hops(item)\n",
    "#         hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "#         entry = {\n",
    "#             'ID': item.get('ID', ''),\n",
    "#             'question': item.get('question', ''),\n",
    "#             'answer': item.get('answer', ''),\n",
    "#             'type': item.get('type', ''),\n",
    "#             'time_scope': item.get('time_scope', ''),\n",
    "#             'hop_count': complexity_score,\n",
    "#         }\n",
    "#         processed_data.append(entry)\n",
    "    \n",
    "#     df = pd.DataFrame(processed_data)\n",
    "#     df['hop_category'] = pd.cut(\n",
    "#         df['hop_count'],\n",
    "#         bins=[-1, 1, 2, 3, float('inf')],\n",
    "#         labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "#     for i in range(1, 5):\n",
    "#         print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # --- COUNTERFACTUAL GENERATION ---\n",
    "# def extract_modifiable_elements(question: str) -> Dict[str, List[Tuple[str, str]]]:\n",
    "#     \"\"\"Extract elements that can be modified for counterfactual reasoning\"\"\"\n",
    "#     elements = {\n",
    "#         'years': [],\n",
    "#         'numbers': [],\n",
    "#         'entities': [],\n",
    "#         'locations': [],\n",
    "#         'time_expressions': []\n",
    "#     }\n",
    "    \n",
    "#     # Extract years (4-digit numbers that look like years)\n",
    "#     year_pattern = r'\\b(1\\d{3}|20\\d{2})\\b'\n",
    "#     years = re.findall(year_pattern, question)\n",
    "#     for year in years:\n",
    "#         elements['years'].append((year, f\"year_{year}\"))\n",
    "    \n",
    "#     # Extract other numbers\n",
    "#     number_pattern = r'\\b(\\d+(?:\\.\\d+)?)\\b'\n",
    "#     numbers = re.findall(number_pattern, question)\n",
    "#     for num in numbers:\n",
    "#         if num not in years:  # Don't duplicate years\n",
    "#             elements['numbers'].append((num, f\"number_{num}\"))\n",
    "    \n",
    "#     # Extract proper nouns/entities (capitalized words)\n",
    "#     entity_pattern = r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b'\n",
    "#     entities = re.findall(entity_pattern, question)\n",
    "#     for entity in entities:\n",
    "#         if not re.match(year_pattern, entity):  # Don't include years as entities\n",
    "#             elements['entities'].append((entity, f\"entity_{entity.replace(' ', '_')}\"))\n",
    "    \n",
    "#     # Extract time expressions\n",
    "#     time_patterns = [\n",
    "#         r'\\b(before|after|during|in|at|on)\\s+(\\d{4}|\\w+\\s+\\d{4})\\b',\n",
    "#         r'\\b(early|late|mid)[\\s-](\\d{4}s?)\\b',\n",
    "#         r'\\b(\\d{4})[\\s-](to|until|through)[\\s-](\\d{4})\\b'\n",
    "#     ]\n",
    "    \n",
    "#     for pattern in time_patterns:\n",
    "#         matches = re.findall(pattern, question, re.IGNORECASE)\n",
    "#         for match in matches:\n",
    "#             if isinstance(match, tuple):\n",
    "#                 time_expr = ' '.join(match)\n",
    "#             else:\n",
    "#                 time_expr = match\n",
    "#             elements['time_expressions'].append((time_expr, f\"time_{time_expr.replace(' ', '_')}\"))\n",
    "    \n",
    "#     # Extract locations (common location indicators)\n",
    "#     location_indicators = ['in', 'at', 'from', 'to', 'near', 'around']\n",
    "#     location_pattern = r'\\b(?:' + '|'.join(location_indicators) + r')\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b'\n",
    "#     locations = re.findall(location_pattern, question)\n",
    "#     for location in locations:\n",
    "#         elements['locations'].append((location, f\"location_{location.replace(' ', '_')}\"))\n",
    "    \n",
    "#     return elements\n",
    "\n",
    "# def generate_counterfactual_changes(elements: Dict[str, List[Tuple[str, str]]]) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"Generate different types of counterfactual changes\"\"\"\n",
    "#     changes = []\n",
    "    \n",
    "#     # Year modifications (+/- 1-10 years)\n",
    "#     for original_year, identifier in elements['years']:\n",
    "#         year_val = int(original_year)\n",
    "#         modifications = [\n",
    "#             (str(year_val + 1), f\"year_plus_1\"),\n",
    "#             (str(year_val - 1), f\"year_minus_1\"),\n",
    "#             (str(year_val + 5), f\"year_plus_5\"),\n",
    "#             (str(year_val - 5), f\"year_minus_5\"),\n",
    "#             (str(year_val + 10), f\"year_plus_10\"),\n",
    "#         ]\n",
    "        \n",
    "#         for new_year, change_type in modifications:\n",
    "#             if 1800 <= int(new_year) <= 2030:  # Reasonable year range\n",
    "#                 changes.append({\n",
    "#                     'type': 'year_change',\n",
    "#                     'original_value': original_year,\n",
    "#                     'new_value': new_year,\n",
    "#                     'change_description': change_type,\n",
    "#                     'element_type': 'year'\n",
    "#                 })\n",
    "    \n",
    "#     # Number modifications (+/- percentage)\n",
    "#     for original_num, identifier in elements['numbers']:\n",
    "#         try:\n",
    "#             num_val = float(original_num)\n",
    "#             modifications = [\n",
    "#                 (str(int(num_val * 1.1)), f\"number_plus_10_percent\"),\n",
    "#                 (str(int(num_val * 0.9)), f\"number_minus_10_percent\"),\n",
    "#                 (str(int(num_val * 1.2)), f\"number_plus_20_percent\"),\n",
    "#                 (str(int(num_val + 1)), f\"number_plus_1\"),\n",
    "#                 (str(max(1, int(num_val - 1))), f\"number_minus_1\"),\n",
    "#             ]\n",
    "            \n",
    "#             for new_num, change_type in modifications:\n",
    "#                 changes.append({\n",
    "#                     'type': 'number_change',\n",
    "#                     'original_value': original_num,\n",
    "#                     'new_value': new_num,\n",
    "#                     'change_description': change_type,\n",
    "#                     'element_type': 'number'\n",
    "#                 })\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "    \n",
    "#     # Entity substitutions (hypothetical alternatives)\n",
    "#     entity_substitutions = {\n",
    "#         'John': 'Peter', 'Mary': 'Sarah', 'Smith': 'Johnson', 'Brown': 'Wilson',\n",
    "#         'United States': 'Canada', 'England': 'France', 'Germany': 'Italy',\n",
    "#         'Company': 'Corporation', 'University': 'College', 'School': 'Academy'\n",
    "#     }\n",
    "    \n",
    "#     for original_entity, identifier in elements['entities']:\n",
    "#         # Direct substitutions\n",
    "#         if original_entity in entity_substitutions:\n",
    "#             new_entity = entity_substitutions[original_entity]\n",
    "#             changes.append({\n",
    "#                 'type': 'entity_substitution',\n",
    "#                 'original_value': original_entity,\n",
    "#                 'new_value': new_entity,\n",
    "#                 'change_description': f\"substitute_{original_entity}_with_{new_entity}\",\n",
    "#                 'element_type': 'entity'\n",
    "#             })\n",
    "        \n",
    "#         # Generic substitutions\n",
    "#         if any(word in original_entity.lower() for word in ['president', 'king', 'queen', 'leader']):\n",
    "#             changes.append({\n",
    "#                 'type': 'entity_substitution',\n",
    "#                 'original_value': original_entity,\n",
    "#                 'new_value': 'Prime Minister',\n",
    "#                 'change_description': f\"substitute_leader\",\n",
    "#                 'element_type': 'entity'\n",
    "#             })\n",
    "    \n",
    "#     return changes\n",
    "\n",
    "# def apply_counterfactual_change(question: str, change: Dict[str, Any]) -> str:\n",
    "#     \"\"\"Apply a counterfactual change to the question\"\"\"\n",
    "#     modified_question = question\n",
    "    \n",
    "#     # Replace the original value with the new value\n",
    "#     original_value = change['original_value']\n",
    "#     new_value = change['new_value']\n",
    "    \n",
    "#     # Use word boundaries for exact matches\n",
    "#     pattern = r'\\b' + re.escape(original_value) + r'\\b'\n",
    "#     modified_question = re.sub(pattern, new_value, modified_question)\n",
    "    \n",
    "#     return modified_question\n",
    "\n",
    "# # --- MODEL GENERATION ---\n",
    "# def setup_model(model_name=\"Qwen/Qwen3-0.6B\"):\n",
    "#     \"\"\"Initialize the model and tokenizer\"\"\"\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     print(f\"Loading {model_name} on {device}...\")\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    \n",
    "#     pipe = pipeline(\n",
    "#         \"text-generation\", \n",
    "#         model=model, \n",
    "#         tokenizer=tokenizer, \n",
    "#         device=0 if device == 'cuda' else -1,\n",
    "#         max_new_tokens=250,\n",
    "#         temperature=0.01,\n",
    "#         do_sample=True\n",
    "#     )\n",
    "    \n",
    "#     return pipe\n",
    "\n",
    "# def format_counterfactual_prompt(original_question: str, modified_question: str, change_description: str) -> str:\n",
    "#     \"\"\"Format prompt for counterfactual reasoning\"\"\"\n",
    "#     return (f\"Given this change in the question, please provide step-by-step reasoning to find the new answer.\\n\\n\"\n",
    "#             f\"Original Question: {original_question}\\n\\n\"\n",
    "#             f\"Modified Question (changed: {change_description}): {modified_question}\\n\\n\"\n",
    "#             f\"Let's think step by step about how this change affects the answer:\\n\"\n",
    "#             f\"Step 1:\")\n",
    "\n",
    "# def extract_reasoning_steps(generation: str, prompt: str) -> List[str]:\n",
    "#     \"\"\"Extract reasoning steps from model generation\"\"\"\n",
    "#     # Remove the original prompt\n",
    "#     output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "    \n",
    "#     # Split by step indicators\n",
    "#     steps = []\n",
    "#     step_pattern = r'Step \\d+:'\n",
    "#     step_splits = re.split(step_pattern, output)\n",
    "    \n",
    "#     for i, step_content in enumerate(step_splits[1:], 1):  # Skip first empty split\n",
    "#         step_text = step_content.strip().split('\\n')[0].strip()  # Get first line of each step\n",
    "#         if step_text:\n",
    "#             steps.append(f\"Step {i}: {step_text}\")\n",
    "    \n",
    "#     # If no steps found, try to extract logical flow\n",
    "#     if not steps:\n",
    "#         lines = [line.strip() for line in output.split('\\n') if line.strip()]\n",
    "#         steps = lines[:5]  # Take first 5 meaningful lines\n",
    "    \n",
    "#     return steps\n",
    "\n",
    "# def extract_final_answer(generation: str, prompt: str) -> str:\n",
    "#     \"\"\"Extract the final answer from generation\"\"\"\n",
    "#     output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "    \n",
    "#     # Look for answer patterns\n",
    "#     answer_patterns = [\n",
    "#         r'(?:final answer|answer|conclusion)[\\s:]*(.+?)(?:\\n|$)',\n",
    "#         r'(?:therefore|thus|so)[\\s,]*(.+?)(?:\\n|$)',\n",
    "#         r'(?:the answer is|answer is)[\\s:]*(.+?)(?:\\n|$)',\n",
    "#     ]\n",
    "    \n",
    "#     for pattern in answer_patterns:\n",
    "#         match = re.search(pattern, output, re.IGNORECASE)\n",
    "#         if match:\n",
    "#             answer = match.group(1).strip()\n",
    "#             # Clean answer\n",
    "#             answer = re.sub(r'[.!?]*$', '', answer).strip()\n",
    "#             if answer:\n",
    "#                 return answer\n",
    "    \n",
    "#     # Fallback: look for the last meaningful line\n",
    "#     lines = [line.strip() for line in output.split('\\n') if line.strip()]\n",
    "#     if lines:\n",
    "#         return lines[-1]\n",
    "    \n",
    "#     return \"No answer found\"\n",
    "\n",
    "# # --- COUNTERFACTUAL EVALUATION ---\n",
    "# def evaluate_change_incorporation(original_steps: List[str], modified_steps: List[str],\n",
    "#                                 change: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#     \"\"\"Evaluate how well the model incorporated the counterfactual change\"\"\"\n",
    "    \n",
    "#     # Check if the change is mentioned in the reasoning\n",
    "#     change_mentioned = False\n",
    "#     original_value = change['original_value'].lower()\n",
    "#     new_value = change['new_value'].lower()\n",
    "    \n",
    "#     modified_text = ' '.join(modified_steps).lower()\n",
    "    \n",
    "#     # Check if new value is mentioned\n",
    "#     if new_value in modified_text:\n",
    "#         change_mentioned = True\n",
    "    \n",
    "#     # Check if the model explicitly acknowledges the change\n",
    "#     change_indicators = ['changed', 'modified', 'different', 'instead', 'now', 'updated']\n",
    "#     change_acknowledged = any(indicator in modified_text for indicator in change_indicators)\n",
    "    \n",
    "#     # Check logical consistency - steps should be different if change is significant\n",
    "#     original_text = ' '.join(original_steps).lower()\n",
    "    \n",
    "#     # Calculate similarity between reasoning chains\n",
    "#     original_words = set(original_text.split())\n",
    "#     modified_words = set(modified_text.split())\n",
    "    \n",
    "#     if len(original_words) > 0:\n",
    "#         similarity = len(original_words & modified_words) / len(original_words | modified_words)\n",
    "#     else:\n",
    "#         similarity = 0\n",
    "    \n",
    "#     # Evaluate adaptation quality\n",
    "#     adaptation_score = 0\n",
    "#     if change_mentioned:\n",
    "#         adaptation_score += 2\n",
    "#     if change_acknowledged:\n",
    "#         adaptation_score += 2\n",
    "#     if similarity < 0.8:  # Reasoning changed significantly\n",
    "#         adaptation_score += 1\n",
    "    \n",
    "#     # Check for step-by-step logical flow\n",
    "#     logical_flow_score = 0\n",
    "#     if len(modified_steps) >= 2:\n",
    "#         for i in range(len(modified_steps) - 1):\n",
    "#             current_step = modified_steps[i].lower()\n",
    "#             next_step = modified_steps[i + 1].lower()\n",
    "#             # Check for logical connectors\n",
    "#             if any(conn in next_step for conn in ['because', 'therefore', 'since', 'so', 'thus']):\n",
    "#                 logical_flow_score += 1\n",
    "    \n",
    "#     return {\n",
    "#         'change_mentioned': change_mentioned,\n",
    "#         'change_acknowledged': change_acknowledged,\n",
    "#         'reasoning_similarity': similarity,\n",
    "#         'adaptation_score': min(5, adaptation_score),\n",
    "#         'logical_flow_score': logical_flow_score,\n",
    "#         'reasoning_steps_count': len(modified_steps)\n",
    "#     }\n",
    "\n",
    "# def evaluate_reasoning_consistency(steps: List[str]) -> Dict[str, Any]:\n",
    "#     \"\"\"Evaluate the consistency and quality of reasoning steps\"\"\"\n",
    "    \n",
    "#     if not steps:\n",
    "#         return {\n",
    "#             'has_steps': False,\n",
    "#             'step_count': 0,\n",
    "#             'logical_connectors': 0,\n",
    "#             'consistency_score': 0\n",
    "#         }\n",
    "    \n",
    "#     # Count logical connectors\n",
    "#     connector_words = ['because', 'therefore', 'since', 'so', 'thus', 'hence', 'consequently', 'as a result']\n",
    "#     logical_connectors = 0\n",
    "    \n",
    "#     for step in steps:\n",
    "#         step_lower = step.lower()\n",
    "#         for connector in connector_words:\n",
    "#             if connector in step_lower:\n",
    "#                 logical_connectors += 1\n",
    "#                 break\n",
    "    \n",
    "#     # Check for contradictions (simple heuristic)\n",
    "#     contradiction_indicators = ['but', 'however', 'although', 'despite', 'unless', 'except']\n",
    "#     contradictions = 0\n",
    "    \n",
    "#     for step in steps:\n",
    "#         step_lower = step.lower()\n",
    "#         for indicator in contradiction_indicators:\n",
    "#             if indicator in step_lower:\n",
    "#                 contradictions += 1\n",
    "#                 break\n",
    "    \n",
    "#     # Calculate consistency score (higher is better)\n",
    "#     consistency_score = len(steps)  # Base score for having steps\n",
    "#     consistency_score += logical_connectors * 2  # Bonus for logical flow\n",
    "#     consistency_score -= contradictions  # Penalty for contradictions\n",
    "#     consistency_score = max(0, consistency_score)  # Ensure non-negative\n",
    "    \n",
    "#     return {\n",
    "#         'has_steps': True,\n",
    "#         'step_count': len(steps),\n",
    "#         'logical_connectors': logical_connectors,\n",
    "#         'contradictions': contradictions,\n",
    "#         'consistency_score': consistency_score\n",
    "#     }\n",
    "\n",
    "# def calculate_counterfactual_score(evaluation_results: Dict[str, Any]) -> float:\n",
    "#     \"\"\"Calculate overall counterfactual reasoning score\"\"\"\n",
    "    \n",
    "#     # Weights for different aspects\n",
    "#     weights = {\n",
    "#         'adaptation': 0.4,      # How well the model adapted to the change\n",
    "#         'consistency': 0.3,     # Internal consistency of reasoning\n",
    "#         'logical_flow': 0.2,    # Logical flow between steps\n",
    "#         'completeness': 0.1     # Completeness of reasoning\n",
    "#     }\n",
    "    \n",
    "#     # Normalize scores to 0-1 range\n",
    "#     adaptation_norm = evaluation_results.get('adaptation_score', 0) / 5.0\n",
    "#     consistency_norm = min(1.0, evaluation_results.get('consistency_score', 0) / 10.0)\n",
    "#     logical_flow_norm = min(1.0, evaluation_results.get('logical_flow_score', 0) / 3.0)\n",
    "#     completeness_norm = min(1.0, evaluation_results.get('reasoning_steps_count', 0) / 5.0)\n",
    "    \n",
    "#     # Calculate weighted score\n",
    "#     total_score = (\n",
    "#         weights['adaptation'] * adaptation_norm +\n",
    "#         weights['consistency'] * consistency_norm +\n",
    "#         weights['logical_flow'] * logical_flow_norm +\n",
    "#         weights['completeness'] * completeness_norm\n",
    "#     )\n",
    "    \n",
    "#     return total_score\n",
    "\n",
    "# # --- EVALUATION PIPELINE ---\n",
    "# def run_counterfactual_evaluation(pipe, df: pd.DataFrame, num_samples: int = 100) -> Dict[str, Any]:\n",
    "#     \"\"\"Run full counterfactual reasoning evaluation\"\"\"\n",
    "    \n",
    "#     results = []\n",
    "#     sample_data = df.sample(n=min(num_samples, len(df)), random_state=42)\n",
    "    \n",
    "#     print(f\"Running counterfactual evaluation on {len(sample_data)} examples...\")\n",
    "    \n",
    "#     for idx, row in tqdm(sample_data.iterrows(), total=len(sample_data)):\n",
    "#         try:\n",
    "#             # Extract modifiable elements\n",
    "#             elements = extract_modifiable_elements(row['question'])\n",
    "            \n",
    "#             # Skip if no modifiable elements found\n",
    "#             if not any(elements.values()):\n",
    "#                 continue\n",
    "            \n",
    "#             # Generate counterfactual changes\n",
    "#             changes = generate_counterfactual_changes(elements)\n",
    "            \n",
    "#             if not changes:\n",
    "#                 continue\n",
    "            \n",
    "#             # Select one random change\n",
    "#             change = random.choice(changes)\n",
    "            \n",
    "#             # Apply counterfactual change\n",
    "#             modified_question = apply_counterfactual_change(row['question'], change)\n",
    "            \n",
    "#             # Generate reasoning for original question\n",
    "#             original_prompt = f\"Question: {row['question']}\\nLet's think step by step:\\nStep 1:\"\n",
    "#             original_generation = pipe(original_prompt, max_new_tokens=200)[0]['generated_text']\n",
    "#             original_steps = extract_reasoning_steps(original_generation, original_prompt)\n",
    "#             original_answer = extract_final_answer(original_generation, original_prompt)\n",
    "            \n",
    "#             # Generate reasoning for modified question\n",
    "#             counterfactual_prompt = format_counterfactual_prompt(\n",
    "#                 row['question'], modified_question, change['change_description']\n",
    "#             )\n",
    "#             modified_generation = pipe(counterfactual_prompt, max_new_tokens=200)[0]['generated_text']\n",
    "#             modified_steps = extract_reasoning_steps(modified_generation, counterfactual_prompt)\n",
    "#             modified_answer = extract_final_answer(modified_generation, counterfactual_prompt)\n",
    "            \n",
    "#             # Evaluate change incorporation\n",
    "#             change_eval = evaluate_change_incorporation(original_steps, modified_steps, change)\n",
    "            \n",
    "#             # Evaluate reasoning consistency\n",
    "#             consistency_eval = evaluate_reasoning_consistency(modified_steps)\n",
    "            \n",
    "#             # Combine evaluations\n",
    "#             combined_eval = {**change_eval, **consistency_eval}\n",
    "            \n",
    "#             # Calculate overall score\n",
    "#             overall_score = calculate_counterfactual_score(combined_eval)\n",
    "            \n",
    "#             result = {\n",
    "#                 'id': row['ID'],\n",
    "#                 'hop_category': row['hop_category'],\n",
    "#                 'original_question': row['question'],\n",
    "#                 'modified_question': modified_question,\n",
    "#                 'change_type': change['type'],\n",
    "#                 'change_description': change['change_description'],\n",
    "#                 'original_steps': original_steps,\n",
    "#                 'modified_steps': modified_steps,\n",
    "#                 'original_answer': original_answer,\n",
    "#                 'modified_answer': modified_answer,\n",
    "#                 'evaluation': combined_eval,\n",
    "#                 'overall_score': overall_score\n",
    "#             }\n",
    "            \n",
    "#             results.append(result)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing example {idx}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # --- ANALYSIS AND VISUALIZATION ---\n",
    "# def analyze_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "#     \"\"\"Analyze counterfactual reasoning results\"\"\"\n",
    "    \n",
    "#     if not results:\n",
    "#         return {\"error\": \"No results to analyze\"}\n",
    "    \n",
    "#     # Convert to DataFrame for easier analysis\n",
    "#     df_results = pd.DataFrame(results)\n",
    "    \n",
    "#     # Overall statistics\n",
    "#     overall_stats = {\n",
    "#         'total_examples': len(results),\n",
    "#         'average_score': df_results['overall_score'].mean(),\n",
    "#         'score_std': df_results['overall_score'].std(),\n",
    "#         'change_mentioned_rate': df_results['evaluation'].apply(lambda x: x['change_mentioned']).mean(),\n",
    "#         'change_acknowledged_rate': df_results['evaluation'].apply(lambda x: x['change_acknowledged']).mean(),\n",
    "#         'avg_adaptation_score': df_results['evaluation'].apply(lambda x: x['adaptation_score']).mean(),\n",
    "#         'avg_consistency_score': df_results['evaluation'].apply(lambda x: x['consistency_score']).mean()\n",
    "#     }\n",
    "    \n",
    "#     # Analysis by hop category\n",
    "#     hop_analysis = {}\n",
    "#     for hop_cat in df_results['hop_category'].unique():\n",
    "#         hop_data = df_results[df_results['hop_category'] == hop_cat]\n",
    "#         hop_analysis[hop_cat] = {\n",
    "#             'count': len(hop_data),\n",
    "#             'avg_score': hop_data['overall_score'].mean(),\n",
    "#             'change_mentioned_rate': hop_data['evaluation'].apply(lambda x: x['change_mentioned']).mean(),\n",
    "#             'avg_steps': hop_data['evaluation'].apply(lambda x: x['reasoning_steps_count']).mean()\n",
    "#         }\n",
    "    \n",
    "#     # Analysis by change type\n",
    "#     change_analysis = {}\n",
    "#     for change_type in df_results['change_type'].unique():\n",
    "#         change_data = df_results[df_results['change_type'] == change_type]\n",
    "#         change_analysis[change_type] = {\n",
    "#             'count': len(change_data),\n",
    "#             'avg_score': change_data['overall_score'].mean(),\n",
    "#             'success_rate': change_data['evaluation'].apply(lambda x: x['adaptation_score'] >= 3).mean()\n",
    "#         }\n",
    "    \n",
    "#     return {\n",
    "#         'overall_stats': overall_stats,\n",
    "#         'hop_analysis': hop_analysis,\n",
    "#         'change_analysis': change_analysis,\n",
    "#         'detailed_results': results\n",
    "#     }\n",
    "\n",
    "# def create_visualizations(analysis_results: Dict[str, Any], save_dir: str = \"./plots\"):\n",
    "#     \"\"\"Create visualizations for counterfactual reasoning results\"\"\"\n",
    "    \n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # Overall score distribution\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     scores = [r['overall_score'] for r in analysis_results['detailed_results']]\n",
    "#     plt.hist(scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "#     plt.xlabel('Overall Counterfactual Score')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.title('Distribution of Counterfactual Reasoning Scores')\n",
    "#     plt.axvline(np.mean(scores), color='red', linestyle='--', label=f'Mean: {np.mean(scores):.3f}')\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"{save_dir}/score_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Performance by hop category\n",
    "#     if 'hop_analysis' in analysis_results:\n",
    "#         hop_data = analysis_results['hop_analysis']\n",
    "#         hop_categories = list(hop_data.keys())\n",
    "#         hop_scores = [hop_data[cat]['avg_score'] for cat in hop_categories]\n",
    "        \n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.bar(hop_categories, hop_scores, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "#         plt.xlabel('Reasoning Complexity (Hop Category)')\n",
    "#         plt.ylabel('Average Counterfactual Score')\n",
    "#         plt.title('Counterfactual Reasoning Performance by Complexity')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f\"{save_dir}/performance_by_hops.png\", dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "    \n",
    "#     # Performance by change type\n",
    "#     if 'change_analysis' in analysis_results:\n",
    "#         change_data = analysis_results['change_analysis']\n",
    "#         change_types = list(change_data.keys())\n",
    "#         change_scores = [change_data[ct]['avg_score'] for ct in change_types]\n",
    "        \n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         plt.bar(change_types, change_scores, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "#         plt.xlabel('Change Type')\n",
    "#         plt.ylabel('Average Counterfactual Score')\n",
    "#         plt.title('Counterfactual Reasoning Performance by Change Type')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f\"{save_dir}/performance_by_change_type.png\", dpi=300, bbox_inches='tight')\n",
    "#         plt.close()\n",
    "\n",
    "# def save_detailed_results(results: List[Dict[str, Any]], analysis: Dict[str, Any], \n",
    "#                          filename: str = \"counterfactual_results.json\"):\n",
    "#     \"\"\"Save detailed results to JSON file\"\"\"\n",
    "    \n",
    "#     output_data = {\n",
    "#         'analysis_summary': {k: v for k, v in analysis.items() if k != 'detailed_results'},\n",
    "#         'detailed_results': results,\n",
    "#         'metadata': {\n",
    "#             'num_examples': len(results),\n",
    "#             'timestamp': pd.Timestamp.now().isoformat(),\n",
    "#             'evaluation_type': 'counterfactual_reasoning'\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     with open(filename, 'w') as f:\n",
    "#         json.dump(output_data, f, indent=2, default=str)\n",
    "    \n",
    "#     print(f\"Results saved to {filename}\")\n",
    "\n",
    "# # --- MAIN EXECUTION ---\n",
    "# def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "#     # Configuration\n",
    "#     MODEL_NAME = \"Qwen/Qwen3-0.6B\"  # You can change this to other models\n",
    "#     NUM_SAMPLES = 50  # Number of examples to evaluate\n",
    "#     SAVE_DIR = \"./counterfactual_evaluation_results\"\n",
    "    \n",
    "#     os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "#     print(\"=== Counterfactual Reasoning Evaluation ===\")\n",
    "    \n",
    "#     # Load and preprocess dataset\n",
    "#     print(\"\\n1. Loading and preprocessing dataset...\")\n",
    "#     data = load_menatqa_dataset()\n",
    "#     df = preprocess_dataset(data)\n",
    "    \n",
    "#     # Setup model\n",
    "#     print(\"\\n2. Setting up model...\")\n",
    "#     pipe = setup_model(MODEL_NAME)\n",
    "    \n",
    "#     # Run evaluation\n",
    "#     print(\"\\n3. Running counterfactual evaluation...\")\n",
    "#     results = run_counterfactual_evaluation(pipe, df, num_samples=NUM_SAMPLES)\n",
    "    \n",
    "#     if not results:\n",
    "#         print(\"No results generated. Please check your data and model setup.\")\n",
    "#         return\n",
    "    \n",
    "#     # Analyze results\n",
    "#     print(\"\\n4. Analyzing results...\")\n",
    "#     analysis = analyze_results(results)\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(\"\\n=== EVALUATION SUMMARY ===\")\n",
    "#     if 'overall_stats' in analysis:\n",
    "#         stats = analysis['overall_stats']\n",
    "#         print(f\"Total examples evaluated: {stats['total_examples']}\")\n",
    "#         print(f\"Average counterfactual score: {stats['average_score']:.3f} ± {stats['score_std']:.3f}\")\n",
    "#         print(f\"Change mentioned rate: {stats['change_mentioned_rate']:.1%}\")\n",
    "#         print(f\"Change acknowledged rate: {stats['change_acknowledged_rate']:.1%}\")\n",
    "#         print(f\"Average adaptation score: {stats['avg_adaptation_score']:.2f}/5\")\n",
    "#         print(f\"Average consistency score: {stats['avg_consistency_score']:.2f}\")\n",
    "    \n",
    "#     # Create visualizations\n",
    "#     print(\"\\n5. Creating visualizations...\")\n",
    "#     create_visualizations(analysis, f\"{SAVE_DIR}/plots\")\n",
    "    \n",
    "#     # Save results\n",
    "#     print(\"\\n6. Saving results...\")\n",
    "#     save_detailed_results(results, analysis, f\"{SAVE_DIR}/detailed_results.json\")\n",
    "    \n",
    "#     print(f\"\\nEvaluation complete! Results saved in {SAVE_DIR}/\")\n",
    "#     print(f\"Check the plots in {SAVE_DIR}/plots/ for visualizations.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131b808-61a3-4c66-9a91-06ca6c6d2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "import copy\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "def load_menatqa_dataset(file_path='./MenatQA.json'):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        import urllib.request\n",
    "        print(f\"Downloading MenatQA to {file_path}...\")\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://raw.githubusercontent.com/weiyifan1023/MenatQA/main/datasets/MenatQA.json\",\n",
    "            str(file_path)\n",
    "        )\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded MenatQA dataset with {len(data)} examples\")\n",
    "    return data\n",
    "\n",
    "def extract_reasoning_hops(example):\n",
    "    question = example.get('question', '')\n",
    "    answer = example.get('answer', '')\n",
    "    q_type = example.get('type', '')\n",
    "    time_scope = example.get('time_scope', '')\n",
    "    \n",
    "    sentences = [s.strip() for s in question.split('.') if s.strip()]\n",
    "    clauses = len([c for c in re.split(r'and|or|but|because|when|if', question.lower()) if c.strip()])\n",
    "    capitalized_words = len([w for w in question.split() if w and w[0].isupper()])\n",
    "    \n",
    "    complexity_score = 1\n",
    "    complexity_score += min(1, len(sentences) - 1)\n",
    "    complexity_score += min(1, (clauses - 1) // 2)\n",
    "    complexity_score += min(1, capitalized_words // 3)\n",
    "    if time_scope:\n",
    "        complexity_score += 1\n",
    "    complexity_score = min(4, max(1, complexity_score))\n",
    "    \n",
    "    return complexity_score\n",
    "\n",
    "def preprocess_dataset(data):\n",
    "    processed_data = []\n",
    "    hop_counts = {1: 0, 2: 0, 3: 0, 4: 0}\n",
    "    \n",
    "    for item in data:\n",
    "        complexity_score = extract_reasoning_hops(item)\n",
    "        hop_counts[min(4, complexity_score)] += 1\n",
    "        \n",
    "        entry = {\n",
    "            'ID': item.get('ID', ''),\n",
    "            'question': item.get('question', ''),\n",
    "            'answer': item.get('answer', ''),\n",
    "            'type': item.get('type', ''),\n",
    "            'time_scope': item.get('time_scope', ''),\n",
    "            'hop_count': complexity_score,\n",
    "        }\n",
    "        processed_data.append(entry)\n",
    "    \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df['hop_category'] = pd.cut(\n",
    "        df['hop_count'],\n",
    "        bins=[-1, 1, 2, 3, float('inf')],\n",
    "        labels=['1-hop', '2-hop', '3-hop', '4+-hop']\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset preprocessed: {len(df)} questions\")\n",
    "    for i in range(1, 5):\n",
    "        print(f\"{i}-hop questions: {hop_counts[i]} ({hop_counts[i]/len(df):.1%})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- COUNTERFACTUAL GENERATION ---\n",
    "def extract_modifiable_elements(question: str) -> Dict[str, List[Tuple[str, str]]]:\n",
    "    \"\"\"Extract elements that can be modified for counterfactual reasoning\"\"\"\n",
    "    elements = {\n",
    "        'years': [],\n",
    "        'numbers': [],\n",
    "        'entities': [],\n",
    "        'locations': [],\n",
    "        'time_expressions': []\n",
    "    }\n",
    "    \n",
    "    # Extract years (4-digit numbers that look like years)\n",
    "    year_pattern = r'\\b(1\\d{3}|20\\d{2})\\b'\n",
    "    years = re.findall(year_pattern, question)\n",
    "    for year in years:\n",
    "        elements['years'].append((year, f\"year_{year}\"))\n",
    "    \n",
    "    # Extract other numbers\n",
    "    number_pattern = r'\\b(\\d+(?:\\.\\d+)?)\\b'\n",
    "    numbers = re.findall(number_pattern, question)\n",
    "    for num in numbers:\n",
    "        if num not in years:  # Don't duplicate years\n",
    "            elements['numbers'].append((num, f\"number_{num}\"))\n",
    "    \n",
    "    # Extract proper nouns/entities (capitalized words)\n",
    "    entity_pattern = r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b'\n",
    "    entities = re.findall(entity_pattern, question)\n",
    "    for entity in entities:\n",
    "        if not re.match(year_pattern, entity):  # Don't include years as entities\n",
    "            elements['entities'].append((entity, f\"entity_{entity.replace(' ', '_')}\"))\n",
    "    \n",
    "    # Extract time expressions\n",
    "    time_patterns = [\n",
    "        r'\\b(before|after|during|in|at|on)\\s+(\\d{4}|\\w+\\s+\\d{4})\\b',\n",
    "        r'\\b(early|late|mid)[\\s-](\\d{4}s?)\\b',\n",
    "        r'\\b(\\d{4})[\\s-](to|until|through)[\\s-](\\d{4})\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in time_patterns:\n",
    "        matches = re.findall(pattern, question, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            if isinstance(match, tuple):\n",
    "                time_expr = ' '.join(match)\n",
    "            else:\n",
    "                time_expr = match\n",
    "            elements['time_expressions'].append((time_expr, f\"time_{time_expr.replace(' ', '_')}\"))\n",
    "    \n",
    "    # Extract locations (common location indicators)\n",
    "    location_indicators = ['in', 'at', 'from', 'to', 'near', 'around']\n",
    "    location_pattern = r'\\b(?:' + '|'.join(location_indicators) + r')\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b'\n",
    "    locations = re.findall(location_pattern, question)\n",
    "    for location in locations:\n",
    "        elements['locations'].append((location, f\"location_{location.replace(' ', '_')}\"))\n",
    "    \n",
    "    return elements\n",
    "\n",
    "def generate_counterfactual_changes(elements: Dict[str, List[Tuple[str, str]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate different types of counterfactual changes\"\"\"\n",
    "    changes = []\n",
    "    \n",
    "    # Year modifications (+/- 1-10 years)\n",
    "    for original_year, identifier in elements['years']:\n",
    "        year_val = int(original_year)\n",
    "        modifications = [\n",
    "            (str(year_val + 1), f\"year_plus_1\"),\n",
    "            (str(year_val - 1), f\"year_minus_1\"),\n",
    "            (str(year_val + 5), f\"year_plus_5\"),\n",
    "            (str(year_val - 5), f\"year_minus_5\"),\n",
    "            (str(year_val + 10), f\"year_plus_10\"),\n",
    "        ]\n",
    "        \n",
    "        for new_year, change_type in modifications:\n",
    "            if 1800 <= int(new_year) <= 2030:  # Reasonable year range\n",
    "                changes.append({\n",
    "                    'type': 'year_change',\n",
    "                    'original_value': original_year,\n",
    "                    'new_value': new_year,\n",
    "                    'change_description': change_type,\n",
    "                    'element_type': 'year'\n",
    "                })\n",
    "    \n",
    "    # Number modifications (+/- percentage)\n",
    "    for original_num, identifier in elements['numbers']:\n",
    "        try:\n",
    "            num_val = float(original_num)\n",
    "            modifications = [\n",
    "                (str(int(num_val * 1.1)), f\"number_plus_10_percent\"),\n",
    "                (str(int(num_val * 0.9)), f\"number_minus_10_percent\"),\n",
    "                (str(int(num_val * 1.2)), f\"number_plus_20_percent\"),\n",
    "                (str(int(num_val + 1)), f\"number_plus_1\"),\n",
    "                (str(max(1, int(num_val - 1))), f\"number_minus_1\"),\n",
    "            ]\n",
    "            \n",
    "            for new_num, change_type in modifications:\n",
    "                changes.append({\n",
    "                    'type': 'number_change',\n",
    "                    'original_value': original_num,\n",
    "                    'new_value': new_num,\n",
    "                    'change_description': change_type,\n",
    "                    'element_type': 'number'\n",
    "                })\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Entity substitutions (hypothetical alternatives)\n",
    "    entity_substitutions = {\n",
    "        'John': 'Peter', 'Mary': 'Sarah', 'Smith': 'Johnson', 'Brown': 'Wilson',\n",
    "        'United States': 'Canada', 'England': 'France', 'Germany': 'Italy',\n",
    "        'Company': 'Corporation', 'University': 'College', 'School': 'Academy'\n",
    "    }\n",
    "    \n",
    "    for original_entity, identifier in elements['entities']:\n",
    "        # Direct substitutions\n",
    "        if original_entity in entity_substitutions:\n",
    "            new_entity = entity_substitutions[original_entity]\n",
    "            changes.append({\n",
    "                'type': 'entity_substitution',\n",
    "                'original_value': original_entity,\n",
    "                'new_value': new_entity,\n",
    "                'change_description': f\"substitute_{original_entity}_with_{new_entity}\",\n",
    "                'element_type': 'entity'\n",
    "            })\n",
    "        \n",
    "        # Generic substitutions\n",
    "        if any(word in original_entity.lower() for word in ['president', 'king', 'queen', 'leader']):\n",
    "            changes.append({\n",
    "                'type': 'entity_substitution',\n",
    "                'original_value': original_entity,\n",
    "                'new_value': 'Prime Minister',\n",
    "                'change_description': f\"substitute_leader\",\n",
    "                'element_type': 'entity'\n",
    "            })\n",
    "    \n",
    "    return changes\n",
    "\n",
    "def apply_counterfactual_change(question: str, change: Dict[str, Any]) -> str:\n",
    "    \"\"\"Apply a counterfactual change to the question\"\"\"\n",
    "    modified_question = question\n",
    "    \n",
    "    # Replace the original value with the new value\n",
    "    original_value = change['original_value']\n",
    "    new_value = change['new_value']\n",
    "    \n",
    "    # Use word boundaries for exact matches\n",
    "    pattern = r'\\b' + re.escape(original_value) + r'\\b'\n",
    "    modified_question = re.sub(pattern, new_value, modified_question)\n",
    "    \n",
    "    return modified_question\n",
    "\n",
    "# --- MODEL GENERATION ---\n",
    "def setup_model(model_name=\"Qwen/Qwen3-0.6B\"):\n",
    "    \"\"\"Initialize the model and tokenizer\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading {model_name} on {device}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        device=0 if device == 'cuda' else -1,\n",
    "        max_new_tokens=250,\n",
    "        temperature=0.01,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "def format_counterfactual_prompt(original_question: str, modified_question: str, change_description: str) -> str:\n",
    "    \"\"\"Format prompt for counterfactual reasoning\"\"\"\n",
    "    return (f\"Given this change in the question, please provide step-by-step reasoning to find the new answer.\\n\\n\"\n",
    "            f\"Original Question: {original_question}\\n\\n\"\n",
    "            f\"Modified Question (changed: {change_description}): {modified_question}\\n\\n\"\n",
    "            f\"Let's think step by step about how this change affects the answer:\\n\"\n",
    "            f\"Step 1:\")\n",
    "\n",
    "def extract_reasoning_steps(generation: str, prompt: str) -> List[str]:\n",
    "    \"\"\"Extract reasoning steps from model generation\"\"\"\n",
    "    # Remove the original prompt\n",
    "    output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "    \n",
    "    # Split by step indicators\n",
    "    steps = []\n",
    "    step_pattern = r'Step \\d+:'\n",
    "    step_splits = re.split(step_pattern, output)\n",
    "    \n",
    "    for i, step_content in enumerate(step_splits[1:], 1):  # Skip first empty split\n",
    "        step_text = step_content.strip().split('\\n')[0].strip()  # Get first line of each step\n",
    "        if step_text:\n",
    "            steps.append(f\"Step {i}: {step_text}\")\n",
    "    \n",
    "    # If no steps found, try to extract logical flow\n",
    "    if not steps:\n",
    "        lines = [line.strip() for line in output.split('\\n') if line.strip()]\n",
    "        steps = lines[:5]  # Take first 5 meaningful lines\n",
    "    \n",
    "    return steps\n",
    "\n",
    "def extract_final_answer(generation: str, prompt: str) -> str:\n",
    "    \"\"\"Extract the final answer from generation\"\"\"\n",
    "    output = generation[len(prompt):] if generation.startswith(prompt) else generation\n",
    "    \n",
    "    # Look for answer patterns\n",
    "    answer_patterns = [\n",
    "        r'(?:final answer|answer|conclusion)[\\s:]*(.+?)(?:\\n|$)',\n",
    "        r'(?:therefore|thus|so)[\\s,]*(.+?)(?:\\n|$)',\n",
    "        r'(?:the answer is|answer is)[\\s:]*(.+?)(?:\\n|$)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in answer_patterns:\n",
    "        match = re.search(pattern, output, re.IGNORECASE)\n",
    "        if match:\n",
    "            answer = match.group(1).strip()\n",
    "            # Clean answer\n",
    "            answer = re.sub(r'[.!?]*$', '', answer).strip()\n",
    "            if answer:\n",
    "                return answer\n",
    "    \n",
    "    # Fallback: look for the last meaningful line\n",
    "    lines = [line.strip() for line in output.split('\\n') if line.strip()]\n",
    "    if lines:\n",
    "        return lines[-1]\n",
    "    \n",
    "    return \"No answer found\"\n",
    "\n",
    "# --- COUNTERFACTUAL EVALUATION ---\n",
    "def evaluate_change_incorporation(original_steps: List[str], modified_steps: List[str],\n",
    "                                change: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate how well the model incorporated the counterfactual change\"\"\"\n",
    "    \n",
    "    # Check if the change is mentioned in the reasoning\n",
    "    change_mentioned = False\n",
    "    original_value = change['original_value'].lower()\n",
    "    new_value = change['new_value'].lower()\n",
    "    \n",
    "    modified_text = ' '.join(modified_steps).lower()\n",
    "    \n",
    "    # Check if new value is mentioned\n",
    "    if new_value in modified_text:\n",
    "        change_mentioned = True\n",
    "    \n",
    "    # Check if the model explicitly acknowledges the change\n",
    "    change_indicators = ['changed', 'modified', 'different', 'instead', 'now', 'updated']\n",
    "    change_acknowledged = any(indicator in modified_text for indicator in change_indicators)\n",
    "    \n",
    "    # Check logical consistency - steps should be different if change is significant\n",
    "    original_text = ' '.join(original_steps).lower()\n",
    "    \n",
    "    # Calculate similarity between reasoning chains\n",
    "    original_words = set(original_text.split())\n",
    "    modified_words = set(modified_text.split())\n",
    "    \n",
    "    if len(original_words) > 0:\n",
    "        similarity = len(original_words & modified_words) / len(original_words | modified_words)\n",
    "    else:\n",
    "        similarity = 0\n",
    "    \n",
    "    # Evaluate adaptation quality\n",
    "    adaptation_score = 0\n",
    "    if change_mentioned:\n",
    "        adaptation_score += 2\n",
    "    if change_acknowledged:\n",
    "        adaptation_score += 2\n",
    "    if similarity < 0.8:  # Reasoning changed significantly\n",
    "        adaptation_score += 1\n",
    "    \n",
    "    # Check for step-by-step logical flow\n",
    "    logical_flow_score = 0\n",
    "    if len(modified_steps) >= 2:\n",
    "        for i in range(len(modified_steps) - 1):\n",
    "            current_step = modified_steps[i].lower()\n",
    "            next_step = modified_steps[i + 1].lower()\n",
    "            # Check for logical connectors\n",
    "            if any(conn in next_step for conn in ['because', 'therefore', 'since', 'so', 'thus']):\n",
    "                logical_flow_score += 1\n",
    "    \n",
    "    return {\n",
    "        'change_mentioned': change_mentioned,\n",
    "        'change_acknowledged': change_acknowledged,\n",
    "        'reasoning_similarity': similarity,\n",
    "        'adaptation_score': min(5, adaptation_score),\n",
    "        'logical_flow_score': logical_flow_score,\n",
    "        'reasoning_steps_count': len(modified_steps)\n",
    "    }\n",
    "\n",
    "def evaluate_reasoning_consistency(steps: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate the consistency and quality of reasoning steps\"\"\"\n",
    "    \n",
    "    if not steps:\n",
    "        return {\n",
    "            'has_steps': False,\n",
    "            'step_count': 0,\n",
    "            'logical_connectors': 0,\n",
    "            'consistency_score': 0\n",
    "        }\n",
    "    \n",
    "    # Count logical connectors\n",
    "    connector_words = ['because', 'therefore', 'since', 'so', 'thus', 'hence', 'consequently', 'as a result']\n",
    "    logical_connectors = 0\n",
    "    \n",
    "    for step in steps:\n",
    "        step_lower = step.lower()\n",
    "        for connector in connector_words:\n",
    "            if connector in step_lower:\n",
    "                logical_connectors += 1\n",
    "                break\n",
    "    \n",
    "    # Check for contradictions (simple heuristic)\n",
    "    contradiction_indicators = ['but', 'however', 'although', 'despite', 'unless', 'except']\n",
    "    contradictions = 0\n",
    "    \n",
    "    for step in steps:\n",
    "        step_lower = step.lower()\n",
    "        for indicator in contradiction_indicators:\n",
    "            if indicator in step_lower:\n",
    "                contradictions += 1\n",
    "                break\n",
    "    \n",
    "    # Calculate consistency score (higher is better)\n",
    "    consistency_score = len(steps)  # Base score for having steps\n",
    "    consistency_score += logical_connectors * 2  # Bonus for logical flow\n",
    "    consistency_score -= contradictions  # Penalty for contradictions\n",
    "    consistency_score = max(0, consistency_score)  # Ensure non-negative\n",
    "    \n",
    "    return {\n",
    "        'has_steps': True,\n",
    "        'step_count': len(steps),\n",
    "        'logical_connectors': logical_connectors,\n",
    "        'contradictions': contradictions,\n",
    "        'consistency_score': consistency_score\n",
    "    }\n",
    "\n",
    "def calculate_counterfactual_score(evaluation_results: Dict[str, Any]) -> float:\n",
    "    \"\"\"Calculate overall counterfactual reasoning score\"\"\"\n",
    "    \n",
    "    # Weights for different aspects\n",
    "    weights = {\n",
    "        'adaptation': 0.4,      # How well the model adapted to the change\n",
    "        'consistency': 0.3,     # Internal consistency of reasoning\n",
    "        'logical_flow': 0.2,    # Logical flow between steps\n",
    "        'completeness': 0.1     # Completeness of reasoning\n",
    "    }\n",
    "    \n",
    "    # Normalize scores to 0-1 range\n",
    "    adaptation_norm = evaluation_results.get('adaptation_score', 0) / 5.0\n",
    "    consistency_norm = min(1.0, evaluation_results.get('consistency_score', 0) / 10.0)\n",
    "    logical_flow_norm = min(1.0, evaluation_results.get('logical_flow_score', 0) / 3.0)\n",
    "    completeness_norm = min(1.0, evaluation_results.get('reasoning_steps_count', 0) / 5.0)\n",
    "    \n",
    "    # Calculate weighted score\n",
    "    total_score = (\n",
    "        weights['adaptation'] * adaptation_norm +\n",
    "        weights['consistency'] * consistency_norm +\n",
    "        weights['logical_flow'] * logical_flow_norm +\n",
    "        weights['completeness'] * completeness_norm\n",
    "    )\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "# --- EVALUATION PIPELINE ---\n",
    "def run_counterfactual_evaluation(pipe, df: pd.DataFrame, batch_size: int = 10) -> Dict[str, Any]:\n",
    "    \"\"\"Run full counterfactual reasoning evaluation on ALL data\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"Running counterfactual evaluation on ALL {len(df)} examples...\")\n",
    "    print(f\"Processing in batches of {batch_size} for memory efficiency...\")\n",
    "    \n",
    "    # Process all data in batches\n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_data = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        for idx, row in batch_data.iterrows():\n",
    "            try:\n",
    "                # Extract modifiable elements\n",
    "                elements = extract_modifiable_elements(row['question'])\n",
    "                \n",
    "                # Skip if no modifiable elements found\n",
    "                if not any(elements.values()):\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Generate counterfactual changes\n",
    "                changes = generate_counterfactual_changes(elements)\n",
    "                \n",
    "                if not changes:\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Select one random change (or you could process all changes)\n",
    "                change = random.choice(changes)\n",
    "                \n",
    "                # Apply counterfactual change\n",
    "                modified_question = apply_counterfactual_change(row['question'], change)\n",
    "                \n",
    "                # Generate reasoning for original question\n",
    "                original_prompt = f\"Question: {row['question']}\\nLet's think step by step:\\nStep 1:\"\n",
    "                original_generation = pipe(original_prompt, max_new_tokens=200)[0]['generated_text']\n",
    "                original_steps = extract_reasoning_steps(original_generation, original_prompt)\n",
    "                original_answer = extract_final_answer(original_generation, original_prompt)\n",
    "                \n",
    "                # Generate reasoning for modified question\n",
    "                counterfactual_prompt = format_counterfactual_prompt(\n",
    "                    row['question'], modified_question, change['change_description']\n",
    "                )\n",
    "                modified_generation = pipe(counterfactual_prompt, max_new_tokens=200)[0]['generated_text']\n",
    "                modified_steps = extract_reasoning_steps(modified_generation, counterfactual_prompt)\n",
    "                modified_answer = extract_final_answer(modified_generation, counterfactual_prompt)\n",
    "                \n",
    "                # Evaluate change incorporation\n",
    "                change_eval = evaluate_change_incorporation(original_steps, modified_steps, change)\n",
    "                \n",
    "                # Evaluate reasoning consistency\n",
    "                consistency_eval = evaluate_reasoning_consistency(modified_steps)\n",
    "                \n",
    "                # Combine evaluations\n",
    "                combined_eval = {**change_eval, **consistency_eval}\n",
    "                \n",
    "                # Calculate overall score\n",
    "                overall_score = calculate_counterfactual_score(combined_eval)\n",
    "                \n",
    "                result = {\n",
    "                    'id': row['ID'],\n",
    "                    'hop_category': row['hop_category'],\n",
    "                    'original_question': row['question'],\n",
    "                    'modified_question': modified_question,\n",
    "                    'change_type': change['type'],\n",
    "                    'change_description': change['change_description'],\n",
    "                    'original_steps': original_steps,\n",
    "                    'modified_steps': modified_steps,\n",
    "                    'original_answer': original_answer,\n",
    "                    'modified_answer': modified_answer,\n",
    "                    'evaluation': combined_eval,\n",
    "                    'overall_score': overall_score\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Progress update every 100 examples\n",
    "                if processed_count % 100 == 0:\n",
    "                    print(f\"Processed: {processed_count}, Failed: {failed_count}, Success rate: {processed_count/(processed_count+failed_count):.1%}\")\n",
    "                \n",
    "                # Optional: Clear CUDA cache periodically to prevent memory issues\n",
    "                if torch.cuda.is_available() and processed_count % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {idx}: {e}\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nEvaluation completed!\")\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Failed: {failed_count}\")\n",
    "    print(f\"Total success rate: {processed_count/(processed_count+failed_count):.1%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- ANALYSIS AND VISUALIZATION ---\n",
    "def analyze_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze counterfactual reasoning results\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        return {\"error\": \"No results to analyze\"}\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Overall statistics\n",
    "    overall_stats = {\n",
    "        'total_examples': len(results),\n",
    "        'average_score': df_results['overall_score'].mean(),\n",
    "        'score_std': df_results['overall_score'].std(),\n",
    "        'median_score': df_results['overall_score'].median(),\n",
    "        'min_score': df_results['overall_score'].min(),\n",
    "        'max_score': df_results['overall_score'].max(),\n",
    "        'change_mentioned_rate': df_results['evaluation'].apply(lambda x: x['change_mentioned']).mean(),\n",
    "        'change_acknowledged_rate': df_results['evaluation'].apply(lambda x: x['change_acknowledged']).mean(),\n",
    "        'avg_adaptation_score': df_results['evaluation'].apply(lambda x: x['adaptation_score']).mean(),\n",
    "        'avg_consistency_score': df_results['evaluation'].apply(lambda x: x['consistency_score']).mean(),\n",
    "        'avg_logical_flow_score': df_results['evaluation'].apply(lambda x: x['logical_flow_score']).mean(),\n",
    "        'avg_reasoning_steps': df_results['evaluation'].apply(lambda x: x['reasoning_steps_count']).mean()\n",
    "    }\n",
    "    \n",
    "    # Analysis by hop category\n",
    "    hop_analysis = {}\n",
    "    for hop_cat in df_results['hop_category'].unique():\n",
    "        hop_data = df_results[df_results['hop_category'] == hop_cat]\n",
    "        hop_analysis[hop_cat] = {\n",
    "            'count': len(hop_data),\n",
    "            'avg_score': hop_data['overall_score'].mean(),\n",
    "            'std_score': hop_data['overall_score'].std(),\n",
    "            'change_mentioned_rate': hop_data['evaluation'].apply(lambda x: x['change_mentioned']).mean(),\n",
    "            'change_acknowledged_rate': hop_data['evaluation'].apply(lambda x: x['change_acknowledged']).mean(),\n",
    "            'avg_adaptation_score': hop_data['evaluation'].apply(lambda x: x['adaptation_score']).mean(),\n",
    "            'avg_steps': hop_data['evaluation'].apply(lambda x: x['reasoning_steps_count']).mean()\n",
    "        }\n",
    "    \n",
    "    # Analysis by change type\n",
    "    change_analysis = {}\n",
    "    for change_type in df_results['change_type'].unique():\n",
    "        change_data = df_results[df_results['change_type'] == change_type]\n",
    "        change_analysis[change_type] = {\n",
    "            'count': len(change_data),\n",
    "            'avg_score': change_data['overall_score'].mean(),\n",
    "            'std_score': change_data['overall_score'].std(),\n",
    "            'success_rate': change_data['evaluation'].apply(lambda x: x['adaptation_score'] >= 3).mean(),\n",
    "            'change_mentioned_rate': change_data['evaluation'].apply(lambda x: x['change_mentioned']).mean()\n",
    "        }\n",
    "    \n",
    "    # Performance distribution analysis\n",
    "    score_ranges = {\n",
    "        'excellent': (0.8, 1.0),\n",
    "        'good': (0.6, 0.8),\n",
    "        'fair': (0.4, 0.6),\n",
    "        'poor': (0.0, 0.4)\n",
    "    }\n",
    "    \n",
    "    performance_distribution = {}\n",
    "    for range_name, (min_score, max_score) in score_ranges.items():\n",
    "        count = len(df_results[(df_results['overall_score'] >= min_score) & \n",
    "                              (df_results['overall_score'] < max_score)])\n",
    "        performance_distribution[range_name] = {\n",
    "            'count': count,\n",
    "            'percentage': count / len(df_results) * 100\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'overall_stats': overall_stats,\n",
    "        'hop_analysis': hop_analysis,\n",
    "        'change_analysis': change_analysis,\n",
    "        'performance_distribution': performance_distribution,\n",
    "        'detailed_results': results\n",
    "    }\n",
    "\n",
    "def create_comprehensive_visualizations(analysis_results: Dict[str, Any], save_dir: str = \"./plots\"):\n",
    "    \"\"\"Create comprehensive visualizations for counterfactual reasoning results\"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Overall score distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scores = [r['overall_score'] for r in analysis_results['detailed_results']]\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(scores, bins=30, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "    plt.xlabel('Overall Counterfactual Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Counterfactual Reasoning Scores')\n",
    "    plt.axvline(np.mean(scores), color='red', linestyle='--', label=f'Mean: {np.mean(scores):.3f}')\n",
    "    plt.axvline(np.median(scores), color='orange', linestyle='--', label=f'Median: {np.median(scores):.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Performance by hop category\n",
    "    if 'hop_analysis' in analysis_results:\n",
    "        hop_data = analysis_results['hop_analysis']\n",
    "        hop_categories = list(hop_data.keys())\n",
    "        hop_scores = [hop_data[cat]['avg_score'] for cat in hop_categories]\n",
    "        hop_stds = [hop_data[cat]['std_score'] for cat in hop_categories]\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        bars = plt.bar(hop_categories, hop_scores, alpha=0.7, color='lightcoral', \n",
    "                      edgecolor='black', yerr=hop_stds, capsize=5)\n",
    "        plt.xlabel('Reasoning Complexity (Hop Category)')\n",
    "        plt.ylabel('Average Counterfactual Score')\n",
    "        plt.title('Performance by Reasoning Complexity')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for i, (bar, cat) in enumerate(zip(bars, hop_categories)):\n",
    "            count = hop_data[cat]['count']\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 3. Performance by change type\n",
    "    if 'change_analysis' in analysis_results:\n",
    "        change_data = analysis_results['change_analysis']\n",
    "        change_types = list(change_data.keys())\n",
    "        change_scores = [change_data[ct]['avg_score'] for ct in change_types]\n",
    "        change_stds = [change_data[ct]['std_score'] for ct in change_types]\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        bars = plt.bar(change_types, change_scores, alpha=0.7, color='lightgreen', \n",
    "                      edgecolor='black', yerr=change_stds, capsize=5)\n",
    "        plt.xlabel('Change Type')\n",
    "        plt.ylabel('Average Counterfactual Score')\n",
    "        plt.title('Performance by Change Type')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for i, (bar, ct) in enumerate(zip(bars, change_types)):\n",
    "            count = change_data[ct]['count']\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Performance distribution pie chart\n",
    "    if 'performance_distribution' in analysis_results:\n",
    "        perf_data = analysis_results['performance_distribution']\n",
    "        labels = list(perf_data.keys())\n",
    "        sizes = [perf_data[label]['percentage'] for label in labels]\n",
    "        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('Performance Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/comprehensive_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Detailed adaptation metrics\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Change mentioned vs acknowledged rates by hop category\n",
    "    if 'hop_analysis' in analysis_results:\n",
    "        hop_data = analysis_results['hop_analysis']\n",
    "        hop_categories = list(hop_data.keys())\n",
    "        mentioned_rates = [hop_data[cat]['change_mentioned_rate'] for cat in hop_categories]\n",
    "        acknowledged_rates = [hop_data[cat]['change_acknowledged_rate'] for cat in hop_categories]\n",
    "        \n",
    "        x = np.arange(len(hop_categories))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.bar(x - width/2, mentioned_rates, width, label='Change Mentioned', alpha=0.7)\n",
    "        plt.bar(x + width/2, acknowledged_rates, width, label='Change Acknowledged', alpha=0.7)\n",
    "        plt.xlabel('Hop Category')\n",
    "        plt.ylabel('Rate')\n",
    "        plt.title('Change Recognition by Complexity')\n",
    "        plt.xticks(x, hop_categories, rotation=45)\n",
    "        plt.legend()\n",
    "        plt.ylim(0, 1)\n",
    "    \n",
    "    # Adaptation scores distribution\n",
    "    adaptation_scores = [r['evaluation']['adaptation_score'] for r in analysis_results['detailed_results']]\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(adaptation_scores, bins=6, alpha=0.7, edgecolor='black', color='purple')\n",
    "    plt.xlabel('Adaptation Score (0-5)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Adaptation Scores')\n",
    "    plt.xticks(range(6))\n",
    "    \n",
    "    # Reasoning steps vs performance correlation\n",
    "    reasoning_steps = [r['evaluation']['reasoning_steps_count'] for r in analysis_results['detailed_results']]\n",
    "    overall_scores = [r['overall_score'] for r in analysis_results['detailed_results']]\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(reasoning_steps, overall_scores, alpha=0.6, color='orange')\n",
    "    plt.xlabel('Number of Reasoning Steps')\n",
    "    plt.ylabel('Overall Score')\n",
    "    plt.title('Reasoning Steps vs Performance')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = np.corrcoef(reasoning_steps, overall_scores)[0, 1]\n",
    "    plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "             transform=plt.gca().transAxes, bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_dir}/detailed_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_comprehensive_results(results: List[Dict[str, Any]], analysis: Dict[str, Any], \n",
    "                              filename: str = \"counterfactual_results_complete.json\"):\n",
    "    \"\"\"Save comprehensive results to JSON file\"\"\"\n",
    "    \n",
    "    output_data = {\n",
    "        'analysis_summary': {k: v for k, v in analysis.items() if k != 'detailed_results'},\n",
    "        'detailed_results': results,\n",
    "        'metadata': {\n",
    "            'num_examples': len(results),\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'evaluation_type': 'counterfactual_reasoning_complete_dataset',\n",
    "            'total_dataset_size': 'all_available_data'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Complete results saved to {filename}\")\n",
    "    \n",
    "    # Also save a summary CSV for easy analysis\n",
    "    summary_filename = filename.replace('.json', '_summary.csv')\n",
    "    summary_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        summary_row = {\n",
    "            'id': result['id'],\n",
    "            'hop_category': result['hop_category'],\n",
    "            'change_type': result['change_type'],\n",
    "            'overall_score': result['overall_score'],\n",
    "            'change_mentioned': result['evaluation']['change_mentioned'],\n",
    "            'change_acknowledged': result['evaluation']['change_acknowledged'],\n",
    "            'adaptation_score': result['evaluation']['adaptation_score'],\n",
    "            'consistency_score': result['evaluation']['consistency_score'],\n",
    "            'logical_flow_score': result['evaluation']['logical_flow_score'],\n",
    "            'reasoning_steps_count': result['evaluation']['reasoning_steps_count'],\n",
    "            'reasoning_similarity': result['evaluation']['reasoning_similarity']\n",
    "        }\n",
    "        summary_data.append(summary_row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(summary_filename, index=False)\n",
    "    print(f\"Summary CSV saved to {summary_filename}\")\n",
    "\n",
    "def print_detailed_summary(analysis: Dict[str, Any]):\n",
    "    \"\"\"Print a detailed summary of the evaluation results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE COUNTERFACTUAL REASONING EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if 'overall_stats' in analysis:\n",
    "        stats = analysis['overall_stats']\n",
    "        print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "        print(f\"   Total examples evaluated: {stats['total_examples']:,}\")\n",
    "        print(f\"   Average counterfactual score: {stats['average_score']:.3f} ± {stats['score_std']:.3f}\")\n",
    "        print(f\"   Median score: {stats['median_score']:.3f}\")\n",
    "        print(f\"   Score range: {stats['min_score']:.3f} - {stats['max_score']:.3f}\")\n",
    "        print(f\"   Change mentioned rate: {stats['change_mentioned_rate']:.1%}\")\n",
    "        print(f\"   Change acknowledged rate: {stats['change_acknowledged_rate']:.1%}\")\n",
    "        print(f\"   Average adaptation score: {stats['avg_adaptation_score']:.2f}/5\")\n",
    "        print(f\"   Average consistency score: {stats['avg_consistency_score']:.2f}\")\n",
    "        print(f\"   Average logical flow score: {stats['avg_logical_flow_score']:.2f}\")\n",
    "        print(f\"   Average reasoning steps: {stats['avg_reasoning_steps']:.1f}\")\n",
    "    \n",
    "    if 'performance_distribution' in analysis:\n",
    "        print(f\"\\n🎯 PERFORMANCE DISTRIBUTION:\")\n",
    "        perf_dist = analysis['performance_distribution']\n",
    "        for level, data in perf_dist.items():\n",
    "            print(f\"   {level.capitalize()}: {data['count']:,} examples ({data['percentage']:.1f}%)\")\n",
    "    \n",
    "    if 'hop_analysis' in analysis:\n",
    "        print(f\"\\n🔢 PERFORMANCE BY REASONING COMPLEXITY:\")\n",
    "        hop_analysis = analysis['hop_analysis']\n",
    "        for hop_cat, data in hop_analysis.items():\n",
    "            print(f\"   {hop_cat}:\")\n",
    "            print(f\"     Count: {data['count']:,}\")\n",
    "            print(f\"     Avg Score: {data['avg_score']:.3f} ± {data['std_score']:.3f}\")\n",
    "            print(f\"     Change Recognition: {data['change_mentioned_rate']:.1%}\")\n",
    "            print(f\"     Change Acknowledgment: {data['change_acknowledged_rate']:.1%}\")\n",
    "            print(f\"     Avg Steps: {data['avg_steps']:.1f}\")\n",
    "    \n",
    "    if 'change_analysis' in analysis:\n",
    "        print(f\"\\n🔄 PERFORMANCE BY CHANGE TYPE:\")\n",
    "        change_analysis = analysis['change_analysis']\n",
    "        for change_type, data in change_analysis.items():\n",
    "            print(f\"   {change_type}:\")\n",
    "            print(f\"     Count: {data['count']:,}\")\n",
    "            print(f\"     Avg Score: {data['avg_score']:.3f} ± {data['std_score']:.3f}\")\n",
    "            print(f\"     Success Rate: {data['success_rate']:.1%}\")\n",
    "            print(f\"     Change Recognition: {data['change_mentioned_rate']:.1%}\")\n",
    "\n",
    "def create_final_report(analysis: Dict[str, Any], save_dir: str):\n",
    "    \"\"\"Create a final evaluation report\"\"\"\n",
    "    \n",
    "    report_filename = f\"{save_dir}/evaluation_report.txt\"\n",
    "    \n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(\"COUNTERFACTUAL REASONING EVALUATION REPORT\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        f.write(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        if 'overall_stats' in analysis:\n",
    "            stats = analysis['overall_stats']\n",
    "            f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Total examples evaluated: {stats['total_examples']:,}\\n\")\n",
    "            f.write(f\"Overall performance score: {stats['average_score']:.3f}/1.0\\n\")\n",
    "            f.write(f\"Change recognition rate: {stats['change_mentioned_rate']:.1%}\\n\")\n",
    "            f.write(f\"Change acknowledgment rate: {stats['change_acknowledged_rate']:.1%}\\n\\n\")\n",
    "            \n",
    "            f.write(\"KEY FINDINGS\\n\")\n",
    "            f.write(\"-\" * 12 + \"\\n\")\n",
    "            \n",
    "            # Performance assessment\n",
    "            if stats['average_score'] >= 0.7:\n",
    "                f.write(\"✅ STRONG: Model demonstrates strong counterfactual reasoning capabilities\\n\")\n",
    "            elif stats['average_score'] >= 0.5:\n",
    "                f.write(\"⚠️  MODERATE: Model shows moderate counterfactual reasoning capabilities\\n\")\n",
    "            else:\n",
    "                f.write(\"❌ WEAK: Model shows limited counterfactual reasoning capabilities\\n\")\n",
    "            \n",
    "            # Change recognition assessment\n",
    "            if stats['change_mentioned_rate'] >= 0.7:\n",
    "                f.write(\"✅ GOOD: Model effectively recognizes changes in questions\\n\")\n",
    "            elif stats['change_mentioned_rate'] >= 0.4:\n",
    "                f.write(\"⚠️  FAIR: Model moderately recognizes changes in questions\\n\")\n",
    "            else:\n",
    "                f.write(\"❌ POOR: Model struggles to recognize changes in questions\\n\")\n",
    "            \n",
    "            f.write(\"\\nDETAILED STATISTICS\\n\")\n",
    "            f.write(\"-\" * 19 + \"\\n\")\n",
    "            for key, value in stats.items():\n",
    "                if isinstance(value, float):\n",
    "                    f.write(f\"{key}: {value:.3f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Final report saved to {report_filename}\")\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "def main():\n",
    "    \"\"\"Main execution function - processes ALL data\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    MODEL_NAME = \"microsoft/DialoGPT-medium\"  # You can change this to other models\n",
    "    BATCH_SIZE = 20  # Process in batches for memory efficiency\n",
    "    SAVE_DIR = \"./counterfactual_evaluation_complete\"\n",
    "    \n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"=== COMPREHENSIVE COUNTERFACTUAL REASONING EVALUATION ===\")\n",
    "    print(\"Processing ENTIRE dataset - this may take several hours depending on dataset size and hardware\")\n",
    "    \n",
    "    # Load and preprocess dataset\n",
    "    print(\"\\n1. Loading and preprocessing dataset...\")\n",
    "    data = load_menatqa_dataset()\n",
    "    df = preprocess_dataset(data)\n",
    "    \n",
    "    print(f\"\\n📋 Dataset Summary:\")\n",
    "    print(f\"   Total questions: {len(df):,}\")\n",
    "    print(f\"   Hop distribution:\")\n",
    "    for hop_cat in df['hop_category'].value_counts().sort_index().items():\n",
    "        print(f\"     {hop_cat[0]}: {hop_cat[1]:,} ({hop_cat[1]/len(df):.1%})\")\n",
    "    \n",
    "    # Setup model\n",
    "    print(\"\\n2. Setting up model...\")\n",
    "    pipe = setup_model(MODEL_NAME)\n",
    "    \n",
    "    # Estimate processing time\n",
    "    estimated_time_hours = len(df) * 0.5 / 60  # Rough estimate: 30 seconds per example\n",
    "    print(f\"\\n⏱️  Estimated processing time: {estimated_time_hours:.1f} hours\")\n",
    "    \n",
    "    response = input(\"\\nDo you want to proceed with the full evaluation? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"Evaluation cancelled.\")\n",
    "        return\n",
    "    \n",
    "    # Run evaluation on ALL data\n",
    "    print(\"\\n3. Running comprehensive counterfactual evaluation...\")\n",
    "    print(\"💡 Tip: This will process the entire dataset. Consider running in screen/tmux for long sessions.\")\n",
    "    \n",
    "    results = run_counterfactual_evaluation(pipe, df, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ No results generated. Please check your data and model setup.\")\n",
    "        return\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\n4. Analyzing comprehensive results...\")\n",
    "    analysis = analyze_results(results)\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print_detailed_summary(analysis)\n",
    "    \n",
    "    # Create comprehensive visualizations\n",
    "    print(\"\\n5. Creating comprehensive visualizations...\")\n",
    "    create_comprehensive_visualizations(analysis, f\"{SAVE_DIR}/plots\")\n",
    "    \n",
    "    # Save complete results\n",
    "    print(\"\\n6. Saving comprehensive results...\")\n",
    "    save_comprehensive_results(results, analysis, f\"{SAVE_DIR}/complete_results.json\")\n",
    "    \n",
    "    # Create final report\n",
    "    print(\"\\n7. Generating final report...\")\n",
    "    create_final_report(analysis, SAVE_DIR)\n",
    "    \n",
    "    print(f\"\\n🎉 COMPREHENSIVE EVALUATION COMPLETE!\")\n",
    "    print(f\"📁 All results saved in: {SAVE_DIR}/\")\n",
    "    print(f\"📊 Visualizations: {SAVE_DIR}/plots/\")\n",
    "    print(f\"📄 Summary CSV: {SAVE_DIR}/complete_results_summary.csv\")\n",
    "    print(f\"📋 Final report: {SAVE_DIR}/evaluation_report.txt\")\n",
    "    \n",
    "    # Final statistics\n",
    "    if 'overall_stats' in analysis:\n",
    "        stats = analysis['overall_stats']\n",
    "        print(f\"\\n📈 FINAL PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   Evaluated: {stats['total_examples']:,} examples\")\n",
    "        print(f\"   Overall Score: {stats['average_score']:.3f}/1.0\")\n",
    "        print(f\"   Success Rate: {stats['change_mentioned_rate']:.1%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea5362-d141-4568-817c-dd13b6e07182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
